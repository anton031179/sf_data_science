{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Введение"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Рады приветствовать вас во втором модуле, посвящённом линейной алгебре!\n",
    "\n",
    "В предыдущем модуле мы познакомились с базовыми понятиями линейной алгебры. По сути, мы изучили основы матричного языка и теперь можем на нем говорить. \n",
    "\n",
    "В этом модуле мы перейдём к применению линейной алгебры в машинном обучении и рассмотрим алгоритмы анализа данных с разных сторон. В основном мы, конечно, будем говорить о модели линейной регрессии и её модификациях.\n",
    "\n",
    "Мы будем использовать весь математический аппарат, который изучили в прошлом модуле. Темы предстоят очень интересные, но в то же время непростые. Для того чтобы усвоить их, необходимо владеть всеми навыками, приобретёнными в предыдущем модуле — от базовых операций над векторами до знания принципов решения СЛАУ. Давайте проверим, насколько вы готовы ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.1**\n",
    "\n",
    "Найдите скалярное произведение векторов:\n",
    "\n",
    "$\\vec{v_{1}}  = (-1, 2, \\ -7, 9)^T$\n",
    "$\\vec{v_{2}}  = (2, 8, 2, \\ -1)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "v1 = np.array([-1, 2, -7, 9])  \n",
    "v2 = np.array([2, 8, 2, -1])  \n",
    "G = np.dot(v1.T, v2.T)\n",
    "G"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.4**\n",
    "\n",
    "Задана матрица\n",
    "\n",
    "<img src=\"data\\MATHML_md2_1_1.png\" alt=\"drawing\" width=\"100\"/>\n",
    "\n",
    "Найдите матрицу Грама $A^{T}A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2, 3],\n",
       "        [3, 5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.matrix (\"1, 1; 1, 2\")\n",
    "G = np.dot(A.T, A)\n",
    "G"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.6**\n",
    "\n",
    "Вычислите обратную матрицу $A^{-1}$, если:  \n",
    "<img src=\"data\\MATHML_md2_1_4.png\" alt=\"drawing\" width=\"100\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7. -2.]\n",
      " [-3.  1.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.matrix (\"1, 2; 3, 7\")\n",
    "print(np.linalg.inv(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.7**\n",
    "\n",
    "Найдите ранг матрицы системы, составленной из векторов:\n",
    "\n",
    "$\\vec{v_{1}}  = (2, 10, \\ -2)^T$  \n",
    "$\\vec{v_{2}}  = (3, 2, \\ -2)^T$  \n",
    "$\\vec{v_{3}}  = (8, 14, \\ -6)^T$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "v1 = np.array([2,10,-2])\n",
    "v2 = np.array([3,2,-2])\n",
    "v3 = np.array([8,14,-6])\n",
    "A = np.array([v1, v2, v3]).T\n",
    "print(np.linalg.matrix_rank(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, второй модуль, посвящённый линейной алгебре, будет состоять **из двух частей**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В первой части** мы будем говорить о **классической модели линейной регрессии**. Для этого мы вернёмся к неоднородным системам линейных алгебраических уравнений, которые мы затронули в прошлом модуле, и посмотрим, как они связаны с **методом наименьших квадратов** (МНК, или OLS, Ordinary Least Squares). Затем мы с математической точки зрения посмотрим на проблемы, которые возникают при его использовании, например мультиколлинеарность или чересчур большое количество факторов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Во второй части** мы перейдём к модификациям модели линейной регрессии и посмотрим, как линейная алгебра работает в **полиномиальной регрессии**, а также поговорим о том, как работают методы регуляризации на математическом уровне."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конце каждого из блоков нас ожидает небольшая **практическая задача** на применение регрессионных моделей.\n",
    "\n",
    "Сразу отметим, что **мы не будем рассматривать следующие вопросы**:\n",
    "\n",
    "- Вероятностные предпосылки использования модели линейной регрессии, необходимые для её валидности (данные предпосылки регламентирует теорема Маркова-Гаусса).\n",
    "- Оценки качества полученной регрессионной модели и оценки статистической значимости её коэффициентов.\n",
    "- Статистические методы предварительной обработки данных.\n",
    "Для изучения этих вопросов понадобятся знания в области теории вероятности и статистики. Мы обязательно обсудим их в следующих модулях. А пока нас интересуют подробности построения вычислительных алгоритмов с точки зрения линейной алгебры.\n",
    "\n",
    "- Также мы не будем затрагивать тему корректной валидации моделей и разделять выборку на тренировочную/тестовую/валидационную с целью экономии времени и сил. Надеемся, что вы помните, как правильно оценивать качество получаемых моделей, сможете проделать это самостоятельно.\n",
    "Таким образом, мы будем рассматривать **только математическую составляющую** линейных (и полиномиальных) моделей регрессии, не концентрируясь на сторонних аспектах машинного обучения."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ЦЕЛИ ДАННОГО МОДУЛЯ:**\n",
    "\n",
    "- познакомиться с неоднородными СЛАУ и случаями их решений;\n",
    "- изучить математическую формализацию метода наименьших квадратов;\n",
    "- научиться строить модель линейной регрессии с помощью МНК;\n",
    "- понять, какие проблемы возникают в МНК с математической точки зрения;\n",
    "- познакомиться с математической формализацией полиномиальной регрессии;\n",
    "- рассмотреть методы регуляризации и принципы их работы.  \n",
    "Готовы? Тогда начинаем →"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Неоднородные СЛАУ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Мы начнём с алгоритма классической линейной регрессии по методу наименьших квадратов (OLS, Ordinary Least Squares). Данный алгоритм является базовым, но, тем не менее, весьма непрост для восприятия, поэтому данная сложносочинённая задача будет разделена на две части:\n",
    "\n",
    "В этом юните мы обсудим случаи и алгоритм решения неоднородных СЛАУ.\n",
    "В следующем юните подведём под эту задачу контекст задачи регрессии.\n",
    "Для начала давайте вспомним, что такое неоднородные СЛАУ.\n",
    "\n",
    "**Примечание**. Совокупность уравнений первой степени, в которых каждая переменная и коэффициенты в ней являются вещественными числами, называется **системой линейных алгебраических уравнений (СЛАУ)** и в общем случае записывается как:\n",
    "\n",
    "$\\left\\{ \\begin{array}{c} a_{11}x_1+a_{12}x_2+\\dots +a_{1m}x_m=b_1 \\\\ a_{21}x_1+a_{22}x_2+\\dots +a_{2m}x_m=b_2 \\\\ \\dots \\\\ a_{n1}x_1+a_{n2}x_2+\\dots +a_{nm}x_m=b_n \\end{array} \\right.\\ (1)$,   \n",
    "где\n",
    "\n",
    "$n$ — количество уравнений;  \n",
    "$m$ — количество переменных;  \n",
    "$x_i$ — неизвестные переменные системы;  \n",
    "$a_{ij}$ — коэффициенты системы;  \n",
    "$b_i$ — свободные члены системы.    \n",
    "СЛАУ (1) называется **однородной**, если все свободные члены системы равны $0 b_1=b_2=⋯=b_n=0:$  \n",
    "\n",
    "$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\forall b_i=0  $\n",
    "СЛАУ (1) называется **неоднородной**, если хотя бы один из свободных членов системы отличен от 0:\n",
    "\n",
    "$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{н}\\textrm{е}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\exists b_i\\neq 0 $  \n",
    "Пример неоднородной СЛАУ:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$    \n",
    "\n",
    "Вспомним, что СЛАУ можно записать в матричном виде:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_3.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $A$ — матрица системы, $w$ — вектор неизвестных коэффициентов, а $b$ — вектор свободных коэффициентов. \n",
    "\n",
    "Давайте введём новое для нас определение.\n",
    "\n",
    "**Расширенной матрицей системы (A|b) неоднородных СЛАУ** называется матрица, составленная из исходной матрицы и вектора свободных коэффициентов (записывается через вертикальную черту):\n",
    "\n",
    "$(A \\mid \\vec{b})=\\left(\\begin{array}{cccc|c} a_{11} & a_{12} & \\ldots & a_{1 m} & b_{1} \\\\ a_{21} & a_{22} & \\ldots & a_{2 m} & b_{2} \\\\ \\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n m} & b_{n} \\end{array}\\right)$  \n",
    "Расширенная матрица системы — это обычная матрица. Черта, отделяющая коэффициенты $a_{ij}$ от свободных членов $b_{i}$ — чисто символическая. \n",
    "\n",
    "Над расширенной матрицей неоднородной СЛАУ можно производить те же самые действия, что и над обычной, а именно:\n",
    "\n",
    "складывать/вычитать между собой строки/столбцы матрицы;\n",
    "умножать строки/столбцы на константу;\n",
    "менять строки/столбцы местами.\n",
    "Приведём пример расширенной матрицы системы. Пусть исходная система будет следующей:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$  \n",
    "Запишем её в матричном виде:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_6.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда расширенная матрица системы будет иметь вид:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_7.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы вспомнили все определения и познакомились с термином расширенной матрицы, мы можем переходить к решению неоднородных СЛАУ.\n",
    "\n",
    "Существует три случая при решении неоднородных СЛАУ:\n",
    "\n",
    "- **«Идеальная пара»**\n",
    "\n",
    "Это так называемые определённые системы линейных уравнений, имеющие единственные решения.\n",
    "\n",
    "- **«В активном поиске»**\n",
    "\n",
    "Неопределённые системы, имеющие бесконечно много решений.\n",
    "\n",
    "- **«Всё сложно»**\n",
    "\n",
    "Это самый интересный для нас случай — переопределённые системы, которые не имеют точных решений.\n",
    "\n",
    "**Примечание**. В данной классификации неоднородных СЛАУ допущено упрощение в терминологии. На самом деле неопределённые системы — это те, в которых независимых уравнений меньше, чем неизвестных. Они могут иметь бесконечно много решений (быть совместными) или ни одного решения (быть несовместными, если уравнения противоречат друг другу).\n",
    "\n",
    "На практике, например в обучении регрессий, этот случай практически не встречается.\n",
    "\n",
    "Что касается переопределённых систем, то в них, помимо несовместности (отсутствия решений), количество независимых уравнений превышает количество неизвестных — это тот самый случай, что мы видим в регрессионном анализе.\n",
    "\n",
    "Далее мы рассмотрим каждый из случаев на примере."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### СЛУЧАЙ «ИДЕАЛЬНАЯ ПАРА»"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой случай решения неоднородной СЛАУ — когда система имеет единственное решение. Такие системы называются **совместными**.\n",
    "\n",
    "На вопрос о том, когда СЛАУ является совместной, отвечает главная теорема СЛАУ — теорема Кронекера — Капелли (также её называют **критерием совместности системы**).\n",
    "\n",
    "**Теорема Кронекера — Капелли**:\n",
    "\n",
    "Неоднородная система линейный алгебраических уравнений $A \\vec{w} = \\vec{b}$ является совместной тогда и только тогда, когда ранг матрицы системы A равен рангу расширенной матрицы системы $(A|\\vec{b})$ и равен количеству независимых переменных $m$:\n",
    "\n",
    "$rk(A) = rk(A|\\vec{b}) = m \\leftrightarrow \\exists ! \\vec{w} = (w_{1}, w_{2}, \\ldots w_m)^T$\n",
    "Причём решение системы будет равно:\n",
    "\n",
    "$\\vec{w} = A^{-1} \\vec{b}$\n",
    "\n",
    "**Примечание**. Здесь значок $\\exists !$ переводится как «существует и причём единственное».\n",
    "\n",
    "Сложно и непонятно? Давайте разберёмся, как работает эта теорема, на примерах ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Дана СЛАУ:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_2_8.png\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "где w_1 и w_2 — неизвестные переменные.\n",
    "\n",
    "При решении системы «в лоб» получим:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_9.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интерпретация:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_10.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На языке линейной алгебры это означает что вектор $(1, 2)^T$ линейно выражается через векторы коэффициентов системы (1,1)^T и (1,2)^T.\n",
    "\n",
    "В матричном виде система запишется, как:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_11.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование уравнений будем таким же, как и при преобразовании расширенной матрицы системы (A|\\vec{b}), вычитая сначала первую строку из второй, а затем — результат из первой, получим то же решение, что и решение «в лоб»."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_12.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Других решений у системы нет. \n",
    "\n",
    "Посмотрим на ранги матрицы А и расширенной матрицы $(A|\\vec{b})$ (количество ступеней в ступенчатых матрицах):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_13.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Они совпадают и равны количеству неизвестных, а это и гарантирует существование и единственность решения. То есть в общем случае, чтобы узнать, сколько решений существует у системы, её необязательно было бы решать — достаточно было бы найти ранги матриц A и (A | b)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Тут возникает вопрос: «Можно ли найти решение одной формулой?»\n",
    "\n",
    "Для удобства перепишем систему без стрелок:\n",
    "\n",
    "$Aw = b$\n",
    "Так как матрица квадратная и невырожденная, у неё обязательно есть обратная матрица.\n",
    "\n",
    "Умножим на $A^{-1}$ слева обе части уравнения. Стоит напомнить, что произведение матриц не перестановочно, поэтому есть разница, с какой стороны умножать.\n",
    "\n",
    "$A^{-1} Aw = A^{-1}b$  \n",
    "$w = A^{-1} b$  \n",
    "**Важно**! Отсюда явно видны **ограничения** этого метода: его можно применять только для квадратных невырожденных матриц (тех, у которых определитель не равен 0).\n",
    "\n",
    "Убедимся в правильности формулы. Найдём произведение матрицы $A^{-1}$ и вектора-столбца $b$:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_14.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем**↓\n",
    "\n",
    "У нас есть квадратная система с m неизвестных. Если ранг матрицы коэффициентов $A$ **равен** рангу расширенной матрицы $(A | b)$ и **равен** количеству переменных $(rk(A)=rk(\\vec{b})=m)$, то в системе будет ровно столько независимых уравнений, сколько и неизвестных $m$, а значит будет единственное решение.\n",
    "\n",
    "Вектор свободных коэффициентов $b$ при этом линейно независим со столбцами матрицы $A$, его разложение по столбцам $A$ единственно."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.4**\n",
    "\n",
    "Решите систему линейных уравнений:  \n",
    "\n",
    "<img src=\"data\\MATHML_md2_2_23.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-2.,  4.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.matrix (\"4, 7; 5, 10\")\n",
    "b = np.array([20, 30])\n",
    "w = np.dot(np.linalg.inv(A), b.T)\n",
    "w\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### СЛУЧАЙ «В АКТИВНОМ ПОИСКЕ»"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? А что, если система не удовлетворяет теореме Кронекера — Капелли? То есть ранг матрицы системы равен расширенному рангу матрицы, но не равен количеству неизвестных. Неужели тогда система нерешаема?\n",
    "\n",
    "На этот вопрос отвечает первое следствие из теоремы ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Следствие №1 из теоремы Кронекера — Капелли**:\n",
    "\n",
    "Если ранг матрицы системы A равен рангу расширенной матрицы системы $(A|\\vec{b})$, но меньше, чем количество неизвестных m, то система имеет бесконечное множество решений:\n",
    "\n",
    "$rk(A) = rk(A | \\vec{b}) < m  \\leftrightarrow  \\infty \\ решений $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Решим систему уравнений:\n",
    "\n",
    "$w_1 + w_2 + w_3 = 10$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, уравнение одно, но формально оно является неоднородной СЛАУ.\n",
    "\n",
    "Итак, мы имеем одно уравнение на три неизвестных, значит две координаты из трёх вектора w мы можем задать как угодно. Например, зададим вторую и третью как $\\alpha$ и $\\beta$. Тогда первая будет равна $10 - \\alpha - \\beta.$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_30.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо переменных $\\alpha$ и $\\beta$ мы можем подставлять любые числа и всегда будем получать равенство. \n",
    "\n",
    "Составим расширенную матрицу:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_31.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Её ранг, как и ранг A, равен 1, что меньше числа неизвестных $m=3$:\n",
    "\n",
    "$rk(A) = rk(A | \\vec{b}) = 1 < 3$\n",
    "Такая ситуация, по следствию из теоремы Кронекера — Капелли, говорит о существовании и не единственности решения, то есть решений бесконечно много.\n",
    "\n",
    "**Резюмируем** ↓\n",
    "\n",
    "Если ранги матриц $A$ и $(A|\\vec{b})$ всё ещё совпадают, но уже меньше количества неизвестных $(rk(A) = rk(A | \\vec{b}) < m)$, значит, уравнений не хватает для того, чтобы определить систему полностью, и решений будет бесконечно много.\n",
    "\n",
    "На языке линейной алгебры это значит, что вектор $\\vec{b}$ линейно зависим со столбцами матрицы A, но также и сами столбцы зависимы между собой, поэтому равнозначного разложения не получится, т. е. таких разложений может быть сколько угодно."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.6**\n",
    "\n",
    "Сколько решений имеет представленная ниже система уравнений?\n",
    "\n",
    "<img src=\"data\\MATHML_md2_2_37.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "A = np.matrix (\"4, 7, -1; -4, 2, 5; 0, 9, 4\")\n",
    "Ab = np.matrix (\"4, 7, -1, 7; -4, 2, 5, 3; 0, 9, 4, 10\")\n",
    "print(np.linalg.matrix_rank(A))\n",
    "print(np.linalg.matrix_rank(Ab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### СЛУЧАЙ «ВСЁ СЛОЖНО»"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь посмотрим на самый интересный для нас случай. Его формально регламентирует второе следствие из теоремы Кронекера — Капелли.\n",
    "\n",
    "**Следствие №2 из теоремы Кронекера — Капелли**:\n",
    "\n",
    "Если ранг матрицы системы $A$ меньше, чем ранг расширенной матрицы системы $(A|\\vec{b})$, то система несовместна, то есть не имеет точных решений:\n",
    "\n",
    "$rk(A)  < rk(A | \\vec{b})  \\leftrightarrow  \\nexists \\ решений$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Решим систему уравнений:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_2_42.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на первое и третье уравнение — очевидно, что такая система не имеет решений, так как данные уравнения противоречат друг другу.\n",
    "\n",
    "Но давайте обоснуем это математически. Для этого запишем расширенную матрицу системы:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_2_43.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем ранги матриц $A$ и $(A|\\vec{b})$:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_2_44.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, $rk(A)=2$, в то время как $rk(A|\\vec{b})=3$. Это и есть **критерий переопределённости** системы уравнений: ранг матрицы системы меньше ранга расширенной матрицы системы.\n",
    "\n",
    "→ Получается, что идеальное решение найти нельзя, но чуть позже мы увидим, что такие системы возникают в задачах регрессии практически всегда, а значит нам всё-таки хотелось бы каким-то образом её решать. Можно попробовать найти приблизительное решение — вопрос лишь в том, какое из всех этих решений лучшее."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Найдем наилучшее приближение для $w_1$, $w_2$, если:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_2_45.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим приближённое решение как $\\hat{w}.$ Приближением для вектора b будет $\\hat{b} = A \\hat{w}.$ Также введём некоторый вектор ошибок $e = b - \\hat{b} = b - A \\hat{w}.$\n",
    "\n",
    "**Примечание.** Здесь мы снова опустили стрелки у векторов $b$, $\\hat{b}$ и $\\hat{w}$ для наглядности.\n",
    "\n",
    "Например, если мы возьмём в качестве вектора $\\hat{w}$ вектор $\\hat{w}_1=(1, 1)^T$, то получим:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_46.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возьмём в качестве вектора $\\hat{w}_2 = (4, -1)^T$, получим:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_47.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Конечно, нам хотелось бы, чтобы ошибка была поменьше. Но какая из них поменьше? Векторы сами по себе сравнить нельзя, но зато можно сравнить их длины.\n",
    "\n",
    "Для первого случая будем иметь:\n",
    "\n",
    "$\\left\\|e_1 \\right\\| = \\sqrt{(-1)^2 + (-1)^2 + (10)^2} = \\sqrt{102}$\n",
    "Для второго случая:\n",
    "\n",
    "$\\left\\|e_2 \\right\\| = \\sqrt{(-2)^2 + 0^2 + 9^2} = \\sqrt{85}$\n",
    "Видно, что вторая ошибка всё-таки меньше, соответственно, приближение лучше. Но в таком случае из всех приближений нам нужно выбрать то, у которого длина вектора ошибок минимальна, если, конечно, это возможно.\n",
    "\n",
    "$\\left\\|e \\right\\| \\rightarrow min$\n",
    "Мы вернёмся к этой задаче чуть позже, а пока выполните несколько заданий на проверку понимания второго следствия теоремы Кронекера — Капелли ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернёмся к задаче поиска оптимальных приближений вектора $\\hat{w}$.\n",
    "\n",
    "**Примечание**. Проблема поиска оптимальных приближённых решений неоднородных переопределённых СЛАУ стояла у математиков вплоть до XIX века. До этого времени математики использовали частные решения, зависящие от вида уравнений и размерности. Впервые данную задачу для общего случая решил Гаусс, опубликовав метод решения этой задачи, который впоследствии будет назван **методом наименьших квадратов (МНК)**. В дальнейшем Лаплас прибавил к данному методу теорию вероятности и доказал оптимальность МНК-оценок с точки зрения статистики.\n",
    "\n",
    "Сейчас мы почувствуем себя настоящими математиками и попробуем решить эту задачу самостоятельно с помощью простой геометрии и знакомых нам операций над матрицами.\n",
    "\n",
    "Вспомним, что на языке линейной алгебры неразрешимость системы"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_48.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "означает, что попытка выразить вектор $(1, 2, 12)^T$ через $(1, 1, 1)^T$ и $(1,2,1)^T$ не будет успешной, так как они линейно независимы.\n",
    "\n",
    "Геометрически это означает, что вектор свободных коэффициентов $b$ (коричневый) не лежит в одной плоскости со столбцами матрицы $A$ (синие векторы)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_49.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Напомним, что подобную задачу мы решали в предыдущем модуле по линейной алгебре, в юните «Практика: векторы». Вы можете вернуться в предыдущий модуль и освежить в памяти решение задачи.\n",
    "\n",
    "Идея состояла в том, что наилучшим приближением для коричневого вектора будет ортогональная проекция на синюю плоскость — голубой вектор. Так происходит потому, что наименьший по длине вектор ошибок — красный — должен быть перпендикулярен к синей плоскости:\n",
    "\n",
    "$e = b - \\hat{b}$\n",
    "\n",
    "В прошлом модуле мы производили расчёты интуитивно, а теперь настала пора вывести формулу.\n",
    "\n",
    "Давайте умножим наши уравнения слева на $A^T$:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_50.png\" alt=\"drawing\" width=\"350\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея заключается в следующем: справа мы найдём скалярное произведение столбцов матрицы A на вектор b, а слева — произведение столбцов A на приближённый вектор $\\hat{b}$ (по сути, на голубую проекцию)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_71.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упростим уравнение, перемножив всё, что содержит только числа. В левой части умножим $A^T$ на $A$, в правой — умножим $A^T$ на $b$. Тогда слева получим матрицу 2×2 — это не что иное, как матрица Грама столбцов A.\n",
    "\n",
    "Столбцы $A$ линейно независимы, а это значит, что, по свойству матрицы Грама, $A^{T} \\cdot A$  — невырожденная квадратная матрица (её определитель не равен нулю, и для неё существует обратная матрица). Получившаяся система — один в один случай «идеальная пара» (ранг матрицы, как и ранг расширенной матрицы, равен 2, в чём несложно убедиться), а это значит, что теперь мы можем её решить."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_51.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Но ведь мы не могли решить изначальную задачу, так как она была переопределена, а эту — можем. Как так получилось?\n",
    "\n",
    "Мы потребовали, чтобы у приближения $\\hat{b}$ были с векторами $(1, 1, 1)^T$ и $(1, 2, 1)^T$ такие же скалярные произведения, как у $b$. Это и означает что $\\hat{b}$ — ортогональная проекция на нашу синюю плоскость, в которой лежат столбцы матрицы A, и в этой плоскости мы можем найти коэффициенты.\n",
    "\n",
    "Мы с вами отлично умеем решать системы типа «Идеальная пара». Для этого нам нужно найти обратную матрицу $(A^{T}A)^{-1}$ и умножить на неё слева всё уравнение. Так мы и получим наше приближение:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_52.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим определитель матрицы $(A^{T} A)$:\n",
    "\n",
    "$det(A^T A) = 3 \\cdot 6 - 4 \\cdot 4 = 2$\n",
    "\n",
    "Находим обратную матрицу $(A^{T}A)^{-1}$:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_53.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Умножаем всё уравнение на обратную матрицу слева:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_54.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И, наконец, вот он — долгожданный приближённый вектор $\\hat{w}$:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_55.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё раз посмотрим на финальную формулу:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_56.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⭐ Пришло время открытий!\n",
    "\n",
    "Только что мы геометрическим образом вывели формулу оценки решения методом наименьших квадратов (МНК или OLS, Ordinary Least Squares).\n",
    "\n",
    "**Примечание**. Стоит отметить, что полученная матричная формула не зависит от размерностей и конкретных значений, а значит применима не только в нашем локальном случае, но и в общем.\n",
    "\n",
    "Нам осталось выполнить проверку полученных результатов, чтобы убедиться в верности решения.\n",
    "\n",
    "Вычислим голубой вектор $\\hat{b}$. Для этого возьмём линейную комбинацию столбцов матрицы А с найденными нами коэффициентами $\\hat{w}_1$ и $\\hat{w}_2$:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_57.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим вектор ошибок e:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_58.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что данный вектор действительно ортогонален столбцам матрицы А. Для этого найдём их скалярные произведения:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_59.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_2_60.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скалярные произведения равны 0, а это означает, что вектор ошибок e действительно ортогонален всей синей плоскости, а голубой вектор $\\hat{b}$ приближённого значения является ортогональной проекцией коричневого вектора $b$.\n",
    "\n",
    "**Примечание**. Прежде чем перейти к выводам, стоит отметить, что обычно OLS-оценку выводят немного иначе, а именно минимизируя в явном виде длину вектора ошибок по коэффициентам $\\hat{w}$, вернее, даже квадрат длины для удобства вычисления производных.\n",
    "\n",
    "$\\left\\|\\vec{e} \\right\\| \\rightarrow min\n",
    "\\left\\|\\vec{e} \\right\\|^2 \\rightarrow min\n",
    "\\left\\|\\vec{b} - A \\vec{w} \\right\\|^2 \\rightarrow min$\n",
    "Формула получится точно такой же, какая есть у нас, просто способ вычислений будет не геометрический, а аналитический. Мы вернёмся к этому способу, когда будем обсуждать оптимизацию функции многих переменных в разделе по математическому анализу.\n",
    "\n",
    "Наконец, мы может подвести итоги для случая «Всё сложно».\n",
    "\n",
    "**Резюмируем** ↓\n",
    "\n",
    "Если ранг матрицы A меньше ранга расширенной системы $(A|\\vec{b})$, то независимых уравнений больше, чем переменных $(rkA<(A|\\vec{b})<m)$, а значит некоторые из них будут противоречить друг другу, то есть решений у системы нет.\n",
    "\n",
    "Говоря на языке линейной алгебры, вектор $b$ линейно независим со столбцами матрицы $A$, а значит его нельзя выразить в качестве их линейной комбинации.  \n",
    "\n",
    "Однако можно получить приближённые решения по методу наименьших квадратов (OLS-оценка - $\\hat{b} = (A^{T}A)^{-1}\\cdot A^{T} b$), идеей которого является ортогональная проекция вектора $b$ на столбцы матрицы $A$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.9**\n",
    "\n",
    "Вычислите вектор ошибок для приближённого решения системы e, если:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_2_61.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Введите координаты полученного вектора через запятую, без пробелов. Пример ввода ответа: 1,1,1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "A = np.matrix (\"1, -5; 2, 1; 1, 1\")\n",
    "Ab = np.matrix (\"1, -5, 1; 2, 1, 2; 1, 1, 2\")\n",
    "print(np.linalg.matrix_rank(A))\n",
    "print(np.linalg.matrix_rank(Ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 5, -1,  0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array([1, 2, 2])\n",
    "w = np.array([1, 1])\n",
    "e = b - np.dot(A, w.T)\n",
    "e"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.11**\n",
    "\n",
    "Найдите OLS-оценку для коэффициентов $w_1$, $w_2$ СЛАУ:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_2_63.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "Для этого выполните задания под цифрами 1-4 ниже.\n",
    "\n",
    "1. Вычислите матрицу Грама столбцов $A:A^{T}A=$\n",
    "Заполните текстовые поля слева направо, сверху вниз.\n",
    "\n",
    "2. Вычислите матрицу $(A^{T}A)^{-1}$. \n",
    "\n",
    "3. Вычислите $A^{T} \\vec{b}$. Введите координаты полученного вектора через запятую, без пробелов. Пример ввода ответа: 1,1.\n",
    "  нет ответа \n",
    "\n",
    "4. Вычислите вектор оценок коэффициентов $\\vec{w}$.\n",
    "\n",
    "Примечание: для корректного ответа не округляйте $(A^{T}A)^{-1}$.\n",
    "\n",
    "Чему равен полученный вектор коэффициентов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[12,  0],\n",
       "        [ 0, 10]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.matrix (\"1, 2; -3, 1; 1, 2; 1, -1\")\n",
    "b = np.array([1, 4, 5, 0])\n",
    "G = np.dot(A.T, A)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.08333333, 0.        ],\n",
       "        [0.        , 0.1       ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ainv = np.linalg.inv(G)\n",
    "Ainv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-6, 16]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.dot(A.T, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.5],\n",
       "        [ 1.6]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.dot(Ainv, c.T)\n",
    "w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Линейная регрессия по методу наименьших квадратов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ С алгоритмом МНК мы познакомились. Теперь можем перейти к задаче регрессии. Начнём с её постановки.\n",
    "\n",
    "В задаче регрессии обычно есть **целевая переменная**, которую мы хотим предсказать. Её, как правило, обозначают буквой $y$. Помимо целевой переменной, есть признаки (их также называют **факторами** или **регрессорами**). Пусть их будет $k$ штук:\n",
    "\n",
    "$y$ — целевая переменная\n",
    "\n",
    "$x_1,x_2, … ,x_k$ — признаки/факторы/регрессоры\n",
    "\n",
    "Поставить задачу — значит ответить на два вопроса:\n",
    "\n",
    "Что у нас есть?\n",
    "Что мы хотим получить?\n",
    "Ответим на них ↓\n",
    "\n",
    "В задаче регрессии есть $N$ (как правило, их действительно много) наблюдений. Это наша обучающая выборка или датасет, представленный в виде таблицы. В столбцах таблицы располагаются векторы признаков $\\vec{x_i}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"data\\MATHML_md2_3_1.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть и целевая переменная, и признаки представлены векторами из векторного пространства $\\mathbb{R}^N$ — каждого вектора $N$ координат.\n",
    "\n",
    "В качестве регрессионной модели мы будем использовать модель линейной регрессии. Мы предполагаем, что связь между целевой переменной и признаками линейная. Это означает, что:\n",
    "\n",
    "$y=w_0+w_1x_1+w_2x_2+…+w_kx_k$,\n",
    "или\n",
    "\n",
    "$y=(\\vec{w}, \\vec{x})$\n",
    "Здесь $\\vec{w}=(w_0,w_1,…,w_k)^T$ обозначают веса (коэффициенты уравнения линейной регрессии), а $\\vec{x}=(1,x_1, x_2,…, x_k)^T$.\n",
    "\n",
    "→ Наличие коэффициента w_0 говорит о том, что мы строим регрессию с константой, или, как ещё иногда говорят, с интерсептом."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Пока что коэффициенты w нам неизвестны. Как же их найти?\n",
    "\n",
    "Для этого у нас есть N наблюдений — обучающий набор данных.\n",
    "\n",
    "Давайте попробуем подобрать такие веса w, чтобы для каждого наблюдения наше равенство было выполнено. Таким образом, получается N уравнений на k+1 неизвестную."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_3_2.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или в привычном виде систем уравнений:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_3_3.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Говоря на языке машинного обучения, мы хотим обучить такую модель, которая описывала бы зависимость целевой переменной от факторов на обучающей выборке.\n",
    "\n",
    "Как правило, N гораздо больше k (количество строк с данными в таблице намного больше количества столбцов) и система переопределена, значит точного решения нет. Поэтому можно найти только приближённое.\n",
    "\n",
    "**Примечание**. Полученной СЛАУ можно дать геометрическую интерпретацию. Если представить каждое наблюдение в виде точки на графике (см. ниже), то уравнение линейной регрессии будет задавать прямую (если фактор один) или гиперплоскость (если факторов k штук). Приравняв уравнение прямой к целевому признаку, мы потребовали, чтобы эта прямая проходила через все точки в нашем наборе данных. Конечно же, это условие не может быть выполнено полностью, так как в данных всегда присутствует какой-то шум, и идеальной прямой (гиперплоскости) не получится, но зато можно построить приближённое решение."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_3_4.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обратите внимание**, что у нас появился новый вектор из единиц. Он здесь из-за того, что мы взяли модель с интерсептом. Можно считать, что это новый регрессор-константа. Данная константа тянется из уравнения прямой, которое мы разбирали в модуле «ML-2. Обучение с учителем: регрессия».\n",
    "\n",
    "Мы уже умеем решать переопределённые системы, для этого мы должны составить матрицу системы A, записав в столбцы все наши регрессоры, включая регрессор константу:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_3_5.png\" alt=\"drawing\" width=\"350\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. В контексте задач машинного обучения матрица A называется **матрицей наблюдений**: по строкам отложены наблюдения (объекты), а по столбцам — характеризующие их признаки. В модулях по машинному обучению мы в основном обозначали её за X. Здесь же мы будем придерживаться традиций линейной алгебры и обозначать матрицу за A.\n",
    "\n",
    "**Примечание**. Обратите внимание, что индексация матрицы A отличается от привычной нам индексации матрицы. Например, здесь x_{12} — второе наблюдение первого регрессора. Это чистая формальность. Если обозначать за первый индекс номер наблюдения, а за второй индекс — номер регрессора, мы получим привычную нам нумерацию элементов матрицы (строка-столбец).\n",
    "\n",
    "Осталось записать финальную формулу OLS-оценки для коэффициентов:\n",
    "\n",
    "$\\hat{\\vec{w}} = (A^T A)^{-1} A^T \\vec{y}$\n",
    "→ Казалось бы, задача решена, однако это совсем не так, ведь мы искали коэффициенты не просто так, а чтобы сделать прогноз — предсказание на новых данных.\n",
    "\n",
    "Допустим, у нас есть новое наблюдение по регрессорам, которое характеризуется признаками $\\vec{x}_{NEW} = (x_{1, NEW}, x_{2, NEW}, ..., x_{k, NEW})^T$.\n",
    "  \n",
    "Тогда, предсказание будет строиться следующим образом:\n",
    "\n",
    "$\\vec{y}_{NEW} = \\vec{w}_0 + \\vec{w}_1 x_{1, NEW} + ... + \\vec{w}_k x_{k, NEW}$  \n",
    "\n",
    "или\n",
    "\n",
    "$\\vec{y}_{NEW} = (\\hat{\\vec{w}}, \\vec{x}_{NEW})$  \n",
    "\n",
    "Теперь перейдём от формул к практике и решим задачу в контексте."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Рассмотрим классический датасет для обучения линейной регрессии — Boston Housing. В нём собраны усреднённые данные по стоимости недвижимости в 506 районах Бостона. Ниже вы видите фрагмент датасета.\n",
    "\n",
    "Примечание. С данным датасетом мы знакомились, когда говорили о модели линейной регрессии в модуле «ML-2. Обучение с учителем: регрессия».\n",
    "\n",
    "Целевой переменной будет PRICE — это, в некотором смысле, типичная (медианная) стоимость дома в районе.\n",
    "\n",
    "Для примера возьмём в качестве регрессоров уровень преступности (CRIM) и среднее количество комнат в доме (RM)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_3_6.png\" alt=\"drawing\" width=\"750\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем нашу модель:\n",
    "\n",
    "$y=w_0+w_1 \\cdot x_1+w_2 \\cdot x_2$  \n",
    "Для наглядности обозначим:\n",
    "\n",
    "$y=w_0+w_1 \\cdot CRIM+w_2 \\cdot RM$  \n",
    "Составим матрицу регрессоров:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_3_7.png\" alt=\"drawing\" width=\"350\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем случае N=506, а k=2. Размерность матрицы A будет равна $dim A =(506, 3)$. Далее мы применяем формулу для вычисления оценок коэффициентов:\n",
    "\n",
    "$\\hat{\\vec{w}} = (A^T A)^{-1} A^T \\vec{y}$  \n",
    "Вычисления к этой задаче мы сделаем в Python ниже, а пока приведём конечный результат. Если сократить запись до двух знаков после точки, получим следующие коэффициенты:\n",
    "\n",
    "$\\hat{\\vec{w}} = (-29.3, \\ -0.26, \\ 8.4)^T$  \n",
    "То есть:\n",
    "\n",
    "$\\hat{w}_0 = -29.3\n",
    "\\hat{w}_1 = -0.26\n",
    "\\hat{w}_2 = 8.4$\n",
    "Мы можем переписать нашу модель для прогноза:\n",
    "\n",
    "$\\hat{y} = -29.3 - 0.26 \\cdot CRIM + 8.4 \\cdot RM$\n",
    "Теперь, если у нас появятся новые наблюдения, то есть ещё один небольшой район с уровнем преступности 0.1 на душу населения и средним количеством комнат в доме, равным 8, мы сможем сделать прогноз на типичную стоимость дома в этом районе — 37 тысяч долларов:\n",
    "\n",
    "$CRIM_{NEW} = 0.1$  \n",
    "$RM_{NEW} = 8$  \n",
    "$\\hat{y}_{NEW} = -29.3 -0.26 \\cdot 0.1 + 8.4 \\cdot 8 \\approx 37$  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**→ Решение на Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка библиотек\n",
    "import numpy as np # для работы с массивами\n",
    "import pandas as pd # для работы с DataFrame \n",
    "from sklearn import datasets # для импорта данных\n",
    "import seaborn as sns # для визуализации статистических данных\n",
    "import matplotlib.pyplot as plt # для построения графиков\n",
    "\n",
    "# # загружаем датасет\n",
    "# boston = datasets.load_boston()\n",
    "# boston_data = pd.DataFrame(\n",
    "#     data=boston.data, #данные\n",
    "#     columns=boston.feature_names #наименования столбцов\n",
    "# )\n",
    "# boston_data['PRICE'] = boston.target\n",
    "# boston_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. В scikit-learn 1.2 датасет о домах в Бостоне был удалён из репозитория библиотеки, поэтому, начиная с этой версии, в результате выполнения кода выше вы можете получить следующую ошибку:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_3-extra-01.png\" alt=\"drawing\" width=\"850\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком случае, для прохождения материалов модуля вам необходимо скачать файл с данными здесь.\n",
    "\n",
    "Код для чтения данных в этом случае будет выглядеть следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE']\n",
    "boston_data = pd.read_csv('data/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "boston_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем матрицу A из столбца единиц и факторов CRIM и RM, а также вектор целевой переменной y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000e+00 6.3200e-03 6.5750e+00]\n",
      " [1.0000e+00 2.7310e-02 6.4210e+00]\n",
      " [1.0000e+00 2.7290e-02 7.1850e+00]\n",
      " ...\n",
      " [1.0000e+00 6.0760e-02 6.9760e+00]\n",
      " [1.0000e+00 1.0959e-01 6.7940e+00]\n",
      " [1.0000e+00 4.7410e-02 6.0300e+00]]\n"
     ]
    }
   ],
   "source": [
    "# составляем матрицу А и вектор целевой переменной\n",
    "CRIM = boston_data['CRIM']\n",
    "RM = boston_data['RM']\n",
    "A = np.column_stack((np.ones(506), CRIM, RM))\n",
    "y = boston_data[['PRICE']]\n",
    "print(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на размерность матрицы A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 3)\n"
     ]
    }
   ],
   "source": [
    "# проверим размерность\n",
    "print(A.shape)\n",
    "## (506, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам ничего не мешает вычислить оценку вектора коэффициентов w по выведенной нами формуле МНК:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-29.24471945]\n",
      " [ -0.26491325]\n",
      " [  8.39106825]]\n"
     ]
    }
   ],
   "source": [
    "# вычислим OLS-оценку для коэффициентов\n",
    "w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь составим прогноз нашей модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37.85733519]\n"
     ]
    }
   ],
   "source": [
    "# добавились новые данные:\n",
    "CRIM_new = 0.1\n",
    "RM_new = 8\n",
    "# делаем прогноз типичной стоимости дома\n",
    "PRICE_new = w_hat.iloc[0]+w_hat.iloc[1]*CRIM_new+w_hat.iloc[2]*RM_new\n",
    "print(PRICE_new.values)\n",
    "## [37.85733519]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Согласитесь, такая запись вычисления оценки стоимости слишком длинная и неудобная, особенно если факторов не два, как у нас, а 200. Более короткий способ сделать прогноз — вычислить скалярное произведение вектора признаков и коэффициентов регрессии."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства дальнейшего использования оформим характеристики нового наблюдения в виде матрицы размером (1,3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [[37.85733519]]\n"
     ]
    }
   ],
   "source": [
    "# короткий способ сделать прогноз\n",
    "new=np.array([[1,CRIM_new,RM_new]])\n",
    "print('prediction:', (new@w_hat).values)\n",
    "## prediction: [[37.85733519]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. Обратите внимание, что, решая задачу с помощью Python, мы получили немного другой результат прогноза стоимости. Это связано с тем, что при выполнении ручного расчёта мы округлили значения коэффициентов и получили менее точный результат.\n",
    "\n",
    "Мы уже знаем, что алгоритм построения модели линейной регрессии по МНК реализован в классе LinearRegression, находящемся в модуле sklearn.linear_model. Для вычисления коэффициентов (обучения модели) нам достаточно передать в метод fit() нашу матрицу с наблюдениями и вектор целевой переменной, а для построения прогноза — вызвать метод predict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat: [[-29.24471945  -0.26491325   8.39106825]]\n",
      "prediction: [[37.85733519]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# создаём модель линейной регрессии\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# вычисляем коэффициенты регрессии\n",
    "model.fit(A, y)\n",
    "print('w_hat:', model.coef_)\n",
    "new_prediction = model.predict(new)\n",
    "print('prediction:', new_prediction)\n",
    "## w_hat: [[-29.24471945  -0.26491325   8.39106825]]\n",
    "## prediction: [[37.85733519]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание.** Здесь при создании объекта класса LinearRegression мы указали fit_itercept=False, так как в нашей матрице наблюдений A уже присутствует столбец с единицами для умножения на свободный член w_0. Его повторное добавление не имеет смысла.\n",
    "\n",
    "Получили те же результаты, что и ранее."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.5**\n",
    "\n",
    "Сделайте прогноз типичной стоимости (в тыс. долларов) дома в городе с уровнем преступности CRIM = 0.2 и средним количеством комнат в доме RM = 6. В качестве модели используйте линейную регрессию, оценка вектора коэффициентов которой равна:\n",
    "\n",
    "$\\hat{\\vec{w}} = (-29.3, \\ -0.26, \\ 8.4)$\n",
    "Ответ округлите до целого числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-29.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0 -29.30\n",
       "1  -0.26\n",
       "2   8.40"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([-29.3, -0.26, 8.4])\n",
    "w_hat = pd.DataFrame(w)\n",
    "w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.048]\n"
     ]
    }
   ],
   "source": [
    "# добавились новые данные:\n",
    "CRIM_new = 0.2\n",
    "RM_new = 6\n",
    "# делаем прогноз типичной стоимости дома\n",
    "PRICE_new = w_hat.iloc[0]+w_hat.iloc[1]*CRIM_new+w_hat.iloc[2]*RM_new\n",
    "print(PRICE_new.values)\n",
    "## [37.85733519]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ПРОБЛЕМЫ В КЛАССИЧЕСКОЙ МНК-МОДЕЛИ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что в уравнении классической OLS-регрессии присутствует очень важный множитель $A^T A$:\n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T}\\vec{y}$  \n",
    "Вы могли заметить, что это матрица Грама значений наших признаков, включая признак-константу.\n",
    "\n",
    "Вспомним свойства этой матрицы: \n",
    "\n",
    "квадратная (размерности $k+1$ на $k+1$, где $k$ — количество факторов);\n",
    "симметричная.\n",
    "Как и у любого метода, у классической OLS-регрессии есть свои *ограничения*. Если матрица $A^T A$ вырождена или близка к вырожденной, то хорошего решения у классической модели не получится. Такие данные называют **плохо обусловленными**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Корректна ли модель классической OLS-регрессии, если\n",
    "\n",
    "<img src=\"data\\MATHML_md2_3_11.png\" alt=\"drawing\" width=\"350\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем матрицу $A$ и вычислим $A^T A$:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_3_12.png\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видите, две последние строки матрицы $A^T A$ являются пропорциональными. Это говорит о том, что матрица вырождена ($det A^T A =0$) или её ранг ($rkA=2$) меньше количества неизвестных (3), а значит обратной матрицы $(A^T A)^{-1}$ к ней не существует. Отсюда следует, что классическая OLS-модель **неприменима для этих данных**.\n",
    "\n",
    "Борьба с вырожденностью матрицы $A^T A$ часто сводится к устранению «плохих» (зависимых) признаков. Для этого анализируют корреляционную матрицу признаков или матрицу их значений. Но иногда проблема может заключаться, например, в том, что один признак измерен в тысячных долях, а другой — в тысячах единиц. Тогда коэффициенты при них могут отличаться в миллион раз, что потенциально может привести к вырожденности матрицы $A^T A$.\n",
    "\n",
    "В устранении этой проблемы может помочь знакомая нам **нормализация/стандартизация данных**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ОСОБЕННОСТИ КЛАССА LINEAR REGRESSION БИБЛИОТЕКИ SKLEARN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что «скажет» Python, если мы попробуем построить модель линейной регрессии на вырожденной матрице наблюдений, используя классическую формулу линейной регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # создадим вырожденную матрицу А\n",
    "# A = np.array([\n",
    "#     [1, 1, 1, 1], \n",
    "#     [2, 1, 1, 2], \n",
    "#     [-2, -1, -1, -2]]\n",
    "# ).T\n",
    "# y = np.array([1, 2, 5, 1])\n",
    "# # вычислим OLS-оценку для коэффициентов\n",
    "# w_hat=np.linalg.inv(A.T@A)@A.T@y\n",
    "# print(w_hat) \n",
    "# ## LinAlgError: Singular matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, мы получили ошибку, говорящую о том, что матрица $A^T A$ — сингулярная (вырожденная), а значит обратить её не получится. Что и требовалось доказать — с математикой всё сходится.\n",
    "\n",
    "⭐ Настало время фокусов!\n",
    "\n",
    "Попробуем обучить модель линейной регрессии LinearRegression из модуля sklearn, используя нашу вырожденную матрицу $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat: [[-29.24471945  -0.26491325   8.39106825]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# создаём модель линейной регрессии\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# вычисляем коэффициенты регрессии\n",
    "model.fit(A, y)\n",
    "print('w_hat:', model.coef_)\n",
    "## w_hat: [ 6.   -1.25  1.25]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Никакой ошибки не возникло! Более того, у нас даже получились вполне адекватные оценки коэффициентов линейной регрессии $\\hat{\\vec{w}}$.\n",
    "\n",
    "? Но ведь мы только что использовали формулу для вычисления коэффициентов при расчётах вручную и получали ошибку. Как мы могли получить результат, если матрица $A^T A$ вырожденная? Существование обратной матрицы для неё противоречит законам линейной алгебры. Неужели это очередной случай, когда «мнения» математики и Python расходятся?\n",
    "\n",
    "На самом деле, не совсем. Здесь нет никакой магии, ошибки округления или бага. Просто в реализации линейной регрессии в sklearn предусмотрена борьба с **плохо определёнными (близкими к вырожденным и вырожденными) матрицами**.\n",
    "\n",
    "Для этого используется метод под названием **сингулярное разложение (SVD)**. О нём мы будем говорить отдельно, однако уже сейчас отметим тот факт, что данный метод позволяет всегда получать корректные значения при обращении матриц.\n",
    "\n",
    "Если вы хотите понять, почему так происходит, ознакомьтесь с этой статьёй. https://towardsdatascience.com/understanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33  \n",
    "\n",
    "Суть метода заключается в том, что в OLS-формуле мы на самом деле используем не саму матрицу $A$, а её диагональное представление из сингулярного разложения, которое гарантированно является невырожденным. Вот и весь секрет."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Правда, открытым остаётся вопрос: можно ли доверять коэффициентам, полученным таким способом, и интерпретировать их? \n",
    "\n",
    "В дальнейшем мы увидим, что делать этого лучше не стоит: возможна такая ситуация, при которой коэффициенты при линейно зависимых факторах, которые получаются в результате применения линейной регрессии через сингулярное разложение, могут получиться слишком большими по модулю. Они могут измеряться миллионами, миллиардами и более высокими порядками, что не будет иметь отношения к действительности. Такие коэффициенты не подлежат интерпретации.\n",
    "\n",
    "Заметим, что в случае использования решения через сингулярное разложение для линейно зависимых столбцов коэффициенты будут всегда получаться одинаковыми по модулю, но различными по знаку: $w_1= -1.25$ и $w_2=1.25$. Неудивительно, ведь второй и третий столбцы матрицы $A$ линейно зависимы с коэффициентом — 1.\n",
    "\n",
    "Запишем итоговое уравнение линейной регрессии:\n",
    "\n",
    "$y=w_{0} \\overrightarrow{1}+w_{1} \\vec{x}_{1}+w_{2} \\vec{x}_{2}=6-1.25 \\cdot \\vec{x}_{1}+1.25 \\cdot \\vec{x}_{2}$,\n",
    "поставим столбцы матрицы $A$ в данное уравнение, чтобы получить прогноз:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_3_13.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. На самом деле сингулярное разложение зашито в функцию np.linalg.lstsq(), которая позволяет в одну строку построить модель линейной регрессии по МНК:\n",
    "\n",
    "**классическая OLS-регрессия в numpy с возможностью получения решения даже для вырожденных матриц**  \n",
    "np.linalg.lstsq(A, y, rcond=None)\n",
    "\n",
    "<img src=\"data\\MATHML_md2_3_14.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "Функция возвращает четыре значения:\n",
    "\n",
    "- вектор рассчитанных коэффициентов линейной регрессии;\n",
    "- сумму квадратов ошибок, MSE (она не считается, если ранг матрицы A меньше числа неизвестных, как в нашем случае);\n",
    "- ранг матрицы A;\n",
    "- вектор из сингулярных значений, которые как раз и оберегают нас от ошибки (о них мы поговорим позже).\n",
    "  \n",
    "Обратите внимание, что мы получили те же коэффициенты, что и с помощью sklearn. При этом ранг матрицы A равен 2, что меньше количества неизвестных коэффициентов. Это ожидаемо говорит о вырожденности матрицы $A$ и, как следствие, матрицы $A^T A$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резюмируем ↓\n",
    "\n",
    "Для поиска коэффициентов модели линейной регрессии используется МНК-оценка: \n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$  \n",
    "\n",
    "- Полученная матричная формула не зависит от размерности матрицы наблюдений A и работает при любом количестве объектов/признаков в данных.\n",
    "\n",
    "- Для реализации обучения модели линейной регрессии по МНК в sklearn используется класс LinearRegression.\n",
    "\n",
    "- Для предотвращения обращения вырожденной матрицы A в LinearRegression вместо самой матрицы используется её сингулярное разложение. Поэтому на практике при построении модели линейной регрессии вместо ручного вычисления обратной матрицы с помощью np.inv() приоритетнее пользоваться именно LinearRegression из sklearn (или np.linalg.lstsq()).\n",
    "\n",
    "Данный метод оберегает от ошибки только при обращении плохо обусловленных и вырожденных матриц и не гарантирует получение корректных коэффициентов линейной регрессии."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Стандартизация векторов и матрица корреляций"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В этом юните мы продолжим обсуждать проблемы классической МНК-модели линейной регрессии и способы их решения. Мы поговорим о **стандартизации векторов** и плавно перейдём к разбору **корреляционной матрицы**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### СТАНДАРТИЗАЦИЯ ВЕКТОРОВ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модулях по разведывательному анализу данных и машинному обучению мы не раз говорили о преобразованиях признаков путём нормализации и стандартизации. Вспомним, что это такое ↓\n",
    "\n",
    "**Нормализация** — это процесс приведения признаков к единому масштабу, например от 0 до 1. Пример — min-max-нормализация:\n",
    "\n",
    "\\[$x_{scaled} =  \\frac{x - x_{min}}{x_{max} - x_{min}}$\\]\n",
    "\n",
    "**Стандартизация** — это процесс приведения признаков к единому масштабу характеристик распределения — нулевому среднему и единичному стандартному отклонению:\n",
    "\n",
    "\\[$x_{scaled} =  \\frac{x - x_{mean}}{x_{std}}$\\]\n",
    "\n",
    "В линейной алгебре под стандартизацией вектора \\($\\vec{x} \\in R^n$\\) понимается несколько другая операция, которая проходит в два этапа:\n",
    "\n",
    "**Центрирование вектора** — это операция приведения среднего к 0:\n",
    "\n",
    "\\[$\\vec{x}_{cent} = \\vec{x} - \\vec{x}_{mean}$\\]\n",
    "\n",
    "**Нормирование вектора** — это операция приведения диапазона вектора к масштабу от -1 до 1 путём деления центрированного вектора на его длину:\n",
    "\n",
    "\\[$vec{x}_{st} =  \\frac{\\vec{x}_{cent}}{ \\| \\vec{x}_{cent} \\| },$\\]\n",
    "\n",
    "где \\($\\vec{x}_{mean}$\\) — вектор, составленный из среднего значения вектора \\($\\vec{x}$\\), а \\($\\| \\vec{x}_{cent} \\|$\\) — длина вектора  \\($\\vec{x}_{cent}$\\).\n",
    "\n",
    "В результате стандартизации вектора всегда получается новый вектор, длина которого равна 1:\n",
    "\n",
    "\\[$\\| \\vec{x}_{st} \\|  = 1$\\]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 1**\n",
    "\n",
    "Необходимо стандартизировать векторы:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_4_1.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Центрируем:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_4_2.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_3.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормируем:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_4.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видите, теперь оба признака имеют значения от -1 до 1 и равный порядок, в отличие от исходных признаков.\n",
    "\n",
    "Давайте посмотрим, что произойдёт с матрицей Грама после стандартизации векторов \\($\\vec{x}_1$\\) и \\($\\vec{x}_2$\\):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 2**\n",
    "\n",
    "Найти матрицу для стандартизированных признаков для\n",
    "\n",
    "<img src=\"data\\MATHML_md2_4_5.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим попарные скалярные произведения новых признаков:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_4_6.png\" alt=\"drawing\" width=\"750\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видите, все числа — в диапазоне от -1 до 1. \n",
    "\n",
    "Забегая вперёд, скажем, что это так называемые **выборочные корреляции признаков**, а сама матрица является **матрицей корреляций** или **корреляционной матрицей**. Пока просто запомните, как выглядит эта матрица.\n",
    "\n",
    "Вот **ещё одна особенность стандартизации** ↓\n",
    "\n",
    "До стандартизации мы прогоняли регрессию \\($y$\\) на регрессоры \\($x_1, x_2, …, x_k$\\) и константу. Всего получалось \\($k+1$\\) коэффициентов:\n",
    "\n",
    "\\[$\\vec{y}=w_{0}+w_{1} \\vec{x}_{1}+w_{2} \\vec{x}_{2}+\\ldots+w_{k} \\vec{x}_{k}$\\]\n",
    "\n",
    "После стандартизации мы прогоняем регрессию стандартизованного \\($y$\\) на стандартизованные регрессоры **без константы**:\n",
    "\n",
    "\\[$\\vec{y}=w_{1_{st}} \\vec{x}_{1_{st}}+w_{2_{st}} \\vec{x}_{2_{st}}+\\ldots+w_{k_{st}} \\vec{x}_{k_{st}}$\\]\n",
    "\n",
    "Математически мы получим одну и ту же регрессию в том смысле, что если пересчитать стандартизированные коэффициенты, мы получим исходные. То же и с прогнозом (пересчёт здесь опустим).\n",
    "\n",
    "**В ЧЁМ БОНУСЫ?**\n",
    "\n",
    "Математика говорит, что регрессия исходного \\($y$\\) на исходные («сырые») признаки c константой точно такая же, как регрессия стандартизированного на стандартизированные признаки без константы. В чём же разница? Математически — ни в чём.\n",
    "\n",
    "На прогноз модели линейной регрессии, построенной по МНК, и её качество стандартизация практически не влияет. Масштабы признаков будут иметь значение только в том случае, если для поиска коэффициентов вы используете численные методы, такие как градиентный спуск (SGDRegressor из sklearn). О нём мы поговорим, когда будем знакомиться с алгоритмом градиентного спуска в модуле по оптимизации.\n",
    "\n",
    "Однако **с точки зрения интерпретации важности коэффициентов разница есть**. Если вы занимаетесь отбором наиболее важных признаков по значению коэффициентов линейной регрессии на нестандартизированных данных, это будет не совсем корректно: один признак может изменяться от 0 до 1, а второй — от -1000 до 1000. Коэффициенты при них также будут различного масштаба. Если же вы посмотрите оценки коэффициентов регрессии после стандартизации, то они будут в едином масштабе, что даст более цельную и объективную картину.\n",
    "\n",
    "Более важный бонус заключается в том, что **после стандартизации матрица Грама признаков** как по волшебству **превращается в корреляционную матрицу**, о которой пойдёт речь далее. Почему это хорошо? На свойства корреляционной матрицы опираются такие алгоритмы, как метод главных компонент и сингулярное разложение, а так как «сырая» и стандартизированная регрессия математически эквивалентны, то имеет смысл исследовать стандартизированную, а результаты обобщить на «сырую».\n",
    "\n",
    "Рассмотрим всё это на **примере** ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 3**\n",
    "\n",
    "Вновь рассмотрим данные о стоимости жилья в районах Бостона.\n",
    "\n",
    "На этот раз возьмём четыре признака: CHAS, LSTAT, CRIM и RM.\n",
    "\n",
    "Для начала посмотрим на статистические характеристики с помощью метода describe():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAS</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.069170</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>3.613524</td>\n",
       "      <td>6.284634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.253994</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>8.601545</td>\n",
       "      <td>0.702617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>3.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>0.082045</td>\n",
       "      <td>5.885500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>0.256510</td>\n",
       "      <td>6.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>3.677083</td>\n",
       "      <td>6.623500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>88.976200</td>\n",
       "      <td>8.780000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CHAS       LSTAT        CRIM          RM\n",
       "count  506.000000  506.000000  506.000000  506.000000\n",
       "mean     0.069170   12.653063    3.613524    6.284634\n",
       "std      0.253994    7.141062    8.601545    0.702617\n",
       "min      0.000000    1.730000    0.006320    3.561000\n",
       "25%      0.000000    6.950000    0.082045    5.885500\n",
       "50%      0.000000   11.360000    0.256510    6.208500\n",
       "75%      0.000000   16.955000    3.677083    6.623500\n",
       "max      1.000000   37.970000   88.976200    8.780000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data[['CHAS', 'LSTAT', 'CRIM','RM']].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что каждый из признаков измеряется в различных единицах и изменяется в различных диапазонах: например, CHAS лежит в диапазоне от 0 до 1, а вот CRIM — в диапазоне от 0.006 до 88.976.\n",
    "\n",
    "Рассмотрим модель линейной регрессии по МНК без стандартизации. Помним, что необходимо добавить столбец из единиц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.92052548]\n",
      " [ 3.9975594 ]\n",
      " [-0.58240212]\n",
      " [-0.09739445]\n",
      " [ 5.07554248]]\n"
     ]
    }
   ],
   "source": [
    "# составляем матрицу наблюдений и вектор целевой переменной\n",
    "A = np.column_stack((np.ones(506), boston_data[['CHAS', 'LSTAT', 'CRIM','RM']]))\n",
    "y = boston_data[['PRICE']]\n",
    "# вычисляем OLS-оценку для коэффициентов без стандартизации\n",
    "w_hat=np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте вспомним интерпретацию коэффициентов построенной модели линейной регрессии, которую мы изучали в модуле «ML-2. Обучение с учителем: регрессия». Значение коэффициента \\($\\hat{w}_i$\\) означает, на сколько в среднем изменится медианная цена (в тысячах долларов) при увеличении \\($x_i$\\) на 1.\n",
    "\n",
    "Например, если количество низкостатусного населения (LSTAT) увеличится на 1 %, то медианная цена домов в районе (в среднем) упадёт на 0.1 тысяч долларов. А если среднее количество комнат (RM) в районе станет больше на 1, то медианная стоимость домов в районе (в среднем) увеличится на 5 тысяч долларов. \n",
    "\n",
    "Тут в голову может прийти мысль: судя по значению коэффициентов, количество комнат (RM) оказывает на стоимость жилья большее влияние, чем процент низкостатусного населения (LSTAT). Однако **такой вывод будет ошибочным**. Мы не учитываем, что признаки, а значит и коэффициенты линейной регрессии, лежат в разных масштабах. Чтобы говорить о важности влияния признаков на модель, нужно строить её на стандартизированных данных.\n",
    "\n",
    "Помним, что для построения стандартизированной линейной регрессии нам не нужен вектор свободных коэффициентов, а значит и столбец из единиц тоже не понадобится.\n",
    "\n",
    "Сначала центрируем векторы, которые находятся в столбцах матрицы \\($A$\\). Для этого вычтем среднее, вычисленное по строкам матрицы \\($A$\\) в каждом столбце, с помощью метода mean(). Затем разделим результат на длины центрированных векторов, вычисленных с помощью функции linalg.norm().\n",
    "\n",
    "**Примечание**. Обратите внимание, что для функции linalg.norm() обязательно необходимо указать параметр axis=0, так как по умолчанию норма считается для всей матрицы, а не для каждого столбца в отдельности. С определением нормы матрицы и тем, как она считается, вы можете ознакомиться в документации к функции norm(). https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAS</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CHAS   LSTAT    CRIM      RM\n",
       "count  506.00  506.00  506.00  506.00\n",
       "mean    -0.00   -0.00   -0.00   -0.00\n",
       "std      0.04    0.04    0.04    0.04\n",
       "min     -0.01   -0.07   -0.02   -0.17\n",
       "25%     -0.01   -0.04   -0.02   -0.03\n",
       "50%     -0.01   -0.01   -0.02   -0.00\n",
       "75%     -0.01    0.03    0.00    0.02\n",
       "max      0.16    0.16    0.44    0.16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# составляем матрицу наблюдений без дополнительного столбца из единиц\n",
    "A = boston_data[['CHAS', 'LSTAT', 'CRIM','RM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# стандартизируем векторы в столбцах матрицы A\n",
    "A_cent = A - A.mean()\n",
    "A_st = A_cent/np.linalg.norm(A_cent, axis=0)\n",
    "A_st.describe().round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь векторы имеют одинаковые средние значения и стандартные отклонения. Если вычислить длину каждого из векторов, мы увидим, что они будут равны 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(A_st, axis=0))\n",
    "## [1. 1. 1. 1.]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения стандартизированных коэффициентов нам также понадобится стандартизация целевой переменной \\($y$\\) по тому же принципу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стандартизируем вектор целевой переменной\n",
    "y_cent = y - y.mean()\n",
    "y_st = y_cent/np.linalg.norm(y_cent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формула для вычисления коэффициента та же, что и раньше, только матрица \\($A$\\) теперь заменяется на \\($A_{st}$\\), а \\($y$\\) — на \\($y_{st}$\\):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11039956]\n",
      " [-0.45220423]\n",
      " [-0.09108766]\n",
      " [ 0.38774848]]\n"
     ]
    }
   ],
   "source": [
    "# вычислим OLS-оценку для стандартизированных коэффициентов\n",
    "w_hat_st=np.linalg.inv(A_st.T@A_st)@A_st.T@y_st\n",
    "print(w_hat_st.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вновь смотрим на коэффициенты. Помним, что коэффициента \\($\\hat{w}_0$\\) у нас больше нет:\n",
    "\n",
    "\\[$\\hat{w}_{CHAS} = 0.11$\\]\n",
    "\n",
    "\\[$\\hat{w}_{LSTAT, \\ st} = -0.45$\\]\n",
    "\n",
    "\\[$\\hat{w}_{CRIM, \\ st} = -0.09$\\]\n",
    "\n",
    "\\[$\\hat{w}_{RM, \\ st} = 0.38$\\]\n",
    "\n",
    "Итак, мы видим картину, прямо противоположную той, что видели ранее. Теперь модуль коэффициента  \\($\\left|\\hat{w}_{LSTAT, \\ st} \\right| = 0.45$\\) будет выше, чем модуль коэффициента \\($\\left|\\hat{w}_{RM, \\ st} \\right| = 0.38$\\). Значит, процент низкостатусного населения оказывает большее влияние на значение стоимости жилья, чем количество комнат.\n",
    "\n",
    "Однако теперь интерпретировать сами коэффициенты в тех же измерениях у нас не получится."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделаем важный вывод ↓**\n",
    "\n",
    "Для того чтобы проинтерпретировать оценки коэффициентов линейной регрессии (понять, каков будет прирост целевой переменной при изменении фактора на 1 условную единицу), нам достаточно построить линейную регрессию в обычном виде без стандартизации и получить обычный вектор \\($\\hat{\\vec{w}}$\\).\n",
    "\n",
    "Однако, чтобы корректно говорить о том, какой фактор оказывает на прогноз большее влияние, необходимо рассматривать стандартизированную оценку вектора коэффициентов \\($\\hat{\\vec{w}}_{st}$\\).\n",
    "\n",
    "Давайте поближе взглянем на матрицу Грама для стандартизированных факторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAS</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.091251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>-0.053929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.613808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.219247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CHAS     LSTAT      CRIM        RM\n",
       "CHAS   1.000000 -0.053929 -0.055892  0.091251\n",
       "LSTAT -0.053929  1.000000  0.455621 -0.613808\n",
       "CRIM  -0.055892  0.455621  1.000000 -0.219247\n",
       "RM     0.091251 -0.613808 -0.219247  1.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# матрица Грама\n",
    "A_st.T @ A_st"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле мы с вами только что вычислили **матрицу выборочных корреляций** наших исходных факторов. Мы уже сталкивались с ней много раз в разделах по разведывательному анализу данных и машинному обучению, правда, вычисляли её мы с помощью функции Pandas, а теперь научились делать это вручную.\n",
    "\n",
    "**Примечание**. Матрицу корреляций можно получить только в том случае, если производить стандартизацию признаков как векторы (делить на длину центрированного вектора \\($\\vec{x}_{st}$\\)). Другие способы стандартизации/нормализации признаков не превращают матрицу Грама в матрицу корреляций."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.3**\n",
    "\n",
    "Стандартизируйте вектор \\($\\vec{x}=(12, 8)^T$\\), приведя его к единичной длине. В качестве ответа введите координаты полученного вектора. Ответ округлите до третьего знака после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70710678, -0.70710678])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# стандартизируем вектор \n",
    "\n",
    "x = np.array([12, 8])\n",
    "x_cent = x - x.mean()\n",
    "x_st = x_cent/np.linalg.norm(x_cent)\n",
    "x_st"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте разберёмся с математическими особенностями корреляционной матрицы."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КОРРЕЛЯЦИОННАЯ МАТРИЦА"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что **корреляционная матрица** \\($C$\\) — это матрица выборочных корреляций между факторами регрессий.\n",
    "\n",
    "\\[$C=corr(X)$\\]\n",
    "\n",
    "Корреляция является одной из важнейших статистических характеристик выборки. Как мы уже знаем из модуля «EDA-2. Математическая статистика в контексте EDA», корреляцию можно измерять различным способами:\n",
    "\n",
    "- корреляцией Пирсона;\n",
    "- корреляцией Спирмена;\n",
    "- корреляцией Кендалла.  \n",
    "\n",
    "В этом модуле мы будем говорить именно о **корреляции Пирсона**. Она измеряет тесноту линейных связей между непрерывными числовыми факторами и может принимать значения от -1 до +1.\n",
    "\n",
    "\\[$c_{ij} = corr(\\vec{x}_{i}, \\vec{x}_{j})$\\]\n",
    "\n",
    "Как и любая статистическая величина, корреляция бывает **генеральной** и **выборочной**. Разница очень тонкая, и мы подробнее разберём её в модуле по теории вероятностей.\n",
    "\n",
    "**Генеральная (истинная) корреляция** — это теоретическая величина, которая отражает общую линейную зависимость между случайными величинами \\($X_i$\\) и \\($X_j$\\). Забегая вперёд скажем, что данная характеристика является абстрактной и вычисляется для генеральных совокупностей — всех возможных реализаций \\($X_i$\\) и \\($X_j$\\). В природе такой величины не существует, она есть только в теории вероятностей.\n",
    "\n",
    "**Выборочная корреляция** — это корреляция, вычисленная на ограниченной выборке. Это уже ближе к нашей теме. Выборочная корреляция отражает линейную взаимосвязь между факторами \\($\\vec{x}_{i}$\\) и \\($\\vec{x}_{j}$\\), реализации которых представлены в выборке.\n",
    "\n",
    "Выборочная корреляция между факторами высчитывается по громоздкой (на первый взгляд) формуле:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_14.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из вычисленных \\($c_{ij}$\\) как раз и составляется матрица корреляций \\($C$\\). Если факторов \\($k$\\) штук, то матрица \\($C$\\) будет квадратной размера \\($dim C =(k,k)$\\):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_15.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте разберём представленную выше формулу на простом примере. Но сначала нас вновь будут ждать довольно сложные формулы — не пугайтесь."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 1**\n",
    "\n",
    "Найти выборочную корреляцию факторов:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_4_16.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим на формулу для выборочной корреляции. Чтобы вычислить коэффициент корреляции \\($c_{12}$\\), необходимо предварительно вычислить \\($\\vec{x}_{1_{mean}}$\\) и \\($\\vec{x}_{2_{mean}}$\\) — средние значения координат векторов.\n",
    "\n",
    "Мы уже вычисляли их ранее:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_17.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нужно вычислить числитель:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_18.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если присмотреться, можно заметить не что иное, как скалярное произведение векторов $(\\vec{x}_{1} -\\vec{x}_{1_{mean}})$ и $(\\vec{x}_{2} -\\vec{x}_{2_{mean}})$. Считаем:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_19.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, мы это уже где-то видели. Да — это векторы $\\vec{x}_{1_{cent}}$ и $\\vec{x}_{2_{cent}}$, которые мы получили, когда стандартизировали векторы. Посчитаем их скалярное произведение:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_20.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А что в знаменателе?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_21.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это произведение длин векторов $\\vec{x}_{1_{cent}}$ и $\\vec{x}_{2_{cent}}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_22.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также уже считали их в примере по стандартизации:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_23.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем коэффициент корреляции:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_24.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова знакомые числа. Да — это элемент на побочной диагонали матрицы Грама, вычисленной для стандартизированных векторов $\\vec{x}_{1_{cent}} и \\vec{x}_{2_{cent}}$, а значит:\n",
    "\n",
    "$c_{12} = (\\vec{x}_{1_{st}}, \\vec{x}_{2_{st}})$\n",
    "Если посчитать корреляцию в обратном порядке между факторами $c_{21}=(\\vec{x}_{2_{st}}, \\vec{x}_{1_{st}})$, получим то же самое число, ведь скалярное произведение перестановочно: $c_{12}=c_{21}$.\n",
    "\n",
    "Ещё один очевидный факт → Корреляция фактора с самим собой всегда равна 1: $c_{ii}=1$, то есть $c_{11}=c_{22}=1$. Так происходит потому, что скалярное произведение вектора с самим собой всегда даёт 1 по свойствам скалярного произведения.\n",
    "\n",
    "Вот мы и нашли нашу матрицу корреляций:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_25.png\" alt=\"drawing\" width=\"350\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Она в точности совпадает с матрицей Грама, вычисленной для стандартизированных векторов $\\vec{x}_{1_{st}}$ и $\\vec{x}_{2_{st}}$:\n",
    "\n",
    "$C = G(\\vec{x}_{1_{st}}, \\vec{x}_{2_{st}})$\n",
    "\n",
    "? Но «магия» ещё не закончилась. Давайте подумаем: какова геометрическая интерпретация корреляции?\n",
    "\n",
    "Присмотритесь к формуле, вспомните свойства скалярного произведения, а затем загляните в ответ:\n",
    "\n",
    "$c_{i j}=\\frac{\\left(\\vec{x}_{i_{\\text {cent }}} \\vec{x}_{j_{\\text {cent }}}\\right)}{\\left\\|\\vec{x}_{i_{\\text {cent }}}\\right\\| \\cdot\\left\\|\\vec{x}_{j_{\\text {cent }}}\\right\\|}$\n",
    "\n",
    "**Ответ ↓**\n",
    "Это косинус угла между центрированными векторами $\\vec{x}_{i_{cent}}$ и $\\vec{x}_{j_{cent}}$. \n",
    "По свойству скалярного произведения:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_26.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примечание. В NumPy матрица корреляций вычисляется функцией np.corrcoef():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.18898224],\n",
       "       [-0.18898224,  1.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = np.array([1, 2, 6])\n",
    "x_2 = np.array([3000, 1000, 2000])\n",
    "np.corrcoef(x_1, x_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили тот же результат, что и раньше.\n",
    "\n",
    "В Pandas матрица корреляций вычисляется методом corr(), вызванным от имени DataFrame.\n",
    "\n",
    "На практике корреляция с точки зрения линейной алгебры означает следующее:\n",
    "\n",
    "- Если корреляция $c_{ij} =1$, значит векторы $\\vec{x}_i$ и $\\vec{x}_j$ пропорциональны и сонаправлены.\n",
    "- Если корреляция $c_{ij} =-1$, значит векторы $\\vec{x}_i$ и $\\vec{x}_j$ пропорциональны и противонаправлены.\n",
    "- Если корреляция $c_{ij} =0$, значит векторы $\\vec{x}_i$ и $\\vec{x}_j$ ортогональны друг другу и, таким образом, являются линейно независимыми.\n",
    "\n",
    " Во всех остальных случаях между факторами $\\vec{x}_i$ и $\\vec{x}_j$ существует какая-то линейная взаимосвязь, причём чем ближе модуль \n",
    " коэффициента корреляции к 1, тем сильнее эта взаимосвязь. Вспомним классификацию связей факторов, которую мы рассматривали в модуле «EDA-2. Математическая статистика в контексте EDA»:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\pic-1.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод ↓**\n",
    "\n",
    "Таким образом, матрица корреляций — это матрица Грама, составленная для стандартизированных столбцов исходной матрицы наблюдений $A$. Она всегда (в теории) симметричная. На главной диагонали этой матрицы стоят 1, а на местах всех остальных элементов — коэффициенты корреляции между факторами $\\vec{x}_i$ и $\\vec{x}_j$.\n",
    "\n",
    "Если коэффициент корреляции больше 0, то взаимосвязь между факторами прямая (растёт один — растёт второй), в противном случае — обратная (растёт один — падает второй)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Рассмотрим пример ↓**\n",
    "\n",
    "**? Пример № 2**\n",
    "\n",
    "Проинтерпретировать выборочные коэффициенты корреляции:\n",
    "\n",
    "$corr(\\vec{x}, \\vec{u}) = 1,   \n",
    "corr(\\vec{x}, \\vec{v}) = -1,    \n",
    "corr(\\vec{x}, \\vec{w}) = 0$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Даны коэффициенты корреляции трёх пар факторов, причём это краевые значения. Что они означают?\n",
    "\n",
    "- $corr(\\vec{x}, \\vec{u}) = 1$ означает что $\\vec{x}$ и $\\vec{u}$ линейно выражаются друг через друга и имеют прямую зависимость (когда растёт один фактор, растёт и другой).  \n",
    "- $corr(\\vec{x}, \\vec{v}) = -1$ говорит о точно такой же линейной, но обратной взаимосвязи.  \n",
    "- $corr(\\vec{x}, \\vec{w}) = 0$ означает, что факторы не связаны, то есть один фактор не чувствителен к изменениям другого.  \n",
    "\n",
    "Теперь коэффициенты принимают уже не экстремальные значения:\n",
    "$corr(\\vec{x}, \\vec{u}) = 0.73, corr(\\vec{x}, \\vec{v}) = -0.72, corr(\\vec{x}, \\vec{w}) = 0.12$  \n",
    "\n",
    "- $corr(\\vec{x}, \\vec{u}) =0.73$ говорит о сильной прямой взаимосвязи. Угол между векторами — острый.  \n",
    "- $corr(\\vec{x}, \\vec{v}) = -0.72$ говорит о сильной обратной взаимосвязи. Угол между векторами — тупой.  \n",
    "- $corr(\\vec{x}, \\vec{w}) = 0.12$ говорит о слабой прямой взаимосвязи. Угол между векторами острый, но близок к 90 градусам."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 3**\n",
    "\n",
    "Давайте посмотрим на корреляционную матрицу в задаче прогнозирования количества показов квартир агентства недвижимости «Рай в шалаше» в зависимости от разных параметров.\n",
    "\n",
    "Здесь:\n",
    "\n",
    "- Demo 2 w — количество показов квартир за две недели;\n",
    "- Rub — стоимость аренды в рублях;\n",
    "- Area — площадь квартиры;\n",
    "- Liv.Area — жилая площадь квартиры;\n",
    "- Floor — этаж;\n",
    "- Euro — стоимость аренды в евро;\n",
    "- NLiv.Area — нежилая площадь квартиры."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_28.png\" alt=\"drawing\" width=\"950\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица получилась размером 7x7, однако её ранг равен 5, а определитель — и вовсе 0. Что это значит? Для начала заметим, что по главной диагонали матрицы стоят единицы — это корреляция каждого фактора с самим собой. Разумеется, матрица симметрична: в первой строке и первом столбце расположены корреляции целевого параметра, то есть количества показов со всеми остальными факторами. Чем эти корреляции больше, тем сильнее взаимосвязь факторов.\n",
    "\n",
    "Подозрительно одинаковыми выглядят **корреляции со стоимостью** аренды в рублях и евро. Корреляция между ними равна 1. Это логично так как факторы пропорциональны с каким-то коэффициентом. Кроме того, также **странно велика корреляция между жилой и общей площадью**. Чистой пропорциональности здесь нет, но из предыдущего модуля мы помним, что жилая, нежилая и общая площади линейно зависимы.\n",
    "\n",
    "Обратите внимание, что **корреляции с нежилой площадью** не так велики. Итого мы нашли два избыточных набора факторов: один набор пропорционален, другой просто линейно зависим. Это случай чистой коллинеарности. Уберём по одному фактору из каждого, и ранг станет максимальным. \n",
    "\n",
    "Нежилая площадь имеет самую маленькую корреляцию с целевым параметром, поэтому мы избавимся от неё.\n",
    "\n",
    "Между рублями и евро нет разницы — оставим рубли, так как они нам привычнее."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_29.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Итак, мы избавились от нежилой площади и аренды в евро. Ранг стал максимальным (то есть равным 5), чистой коллинеарности больше нет, но определитель всё равно маловат. В чём же дело?\n",
    "\n",
    "Стоимость аренды жилой площади и общей площади сильно коррелируют между собой. Обратите внимание на значения коэффициентов корреляции — они практически равны 1, хотя формально эти факторы линейно независимы. Такие корреляции ощутимо портят картину, что и отражается на определителе.\n",
    "\n",
    "Давайте оставим только жилую площадь, её корреляция с показами максимальна.\n",
    "\n",
    "Корреляции между жилой площадью и этажом уже не такие сильные."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_4_30.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранг матрицы теперь равен 3 (как ему и положено), а определитель не так близок к нулю.\n",
    "\n",
    "**Резюмируем ↓**\n",
    "\n",
    "Корреляция — это мера линейной зависимости между признаками.\n",
    "\n",
    "Чем больше по модулю корреляция между каким-нибудь фактором и целевым признаком, тем лучше:\n",
    "\n",
    "$\\left|corr(\\vec{x}_{i}, \\vec{y}) \\right| \\rightarrow 1$ - хорошо\n",
    "\n",
    "Чем больше по модулю корреляция между факторами, тем хуже:\n",
    "\n",
    "$\\left|corr(\\vec{x}_{i}, \\vec{x}_{j}) \\right| \\rightarrow 1$ - плохо  \n",
    "\n",
    "Чем больше линейно зависимых факторов, тем меньше ранг."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Можно выделить два неприятных случая:**\n",
    "\n",
    "**Чистая коллинеарность**\n",
    "\n",
    "Некоторые факторы являются линейно зависимыми между собой. Это влечёт к уменьшению ранга матрицы факторов. Корреляции между зависимыми факторами близки к +1 или -1. Матрица корреляции вырождена.\n",
    "\n",
    "Такие случаи очень редко встречаются на практике, но если вы таковые заметите, можете смело избавиться от одного из факторов.\n",
    "\n",
    "**Мультиколлинеарность**\n",
    "\n",
    "Формально линейной зависимости между факторами нет, и матрица факторов имеет максимальный ранг. Однако корреляции между мультиколлинеарными факторами по-прежнему близки к +1 или -1, и матрица корреляции практически вырождена, несмотря на то что имеет максимальный ранг."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, чистая коллинеарность провоцирует больше проблем, но её легче заметить. Мультиколлинеарность же может быть скрытой, и заметить её не так просто."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КАК ОБНАРУЖИТЬ МУЛЬТИКОЛЛИНЕАРНОСТЬ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Иногда видно сразу или заметно по контексту, что некоторые факторы будут коррелировать между собой.\n",
    "- Также можно посмотреть на определитель матрицы корреляции: если он близок к нулю, значит дела обстоят не очень хорошо.\n",
    "- Важным маркером будут странные результаты стандартной регрессионной формулы, например слишком большие по модулю коэффициенты (вспомните модуль «ML-2. Обучение с учителем: регрессия», где у нас получились запредельные коэффициенты при решении задач) или взаимно обратные коэффициенты (как мы видели в примере в предыдущем юните). \n",
    "- И, наконец, исследование спектра матрицы корреляций и числа обусловленности не только позволяет обнаружить мультиколлинераность, но и помогает избавиться от неё.\n",
    "Есть много способов борьбы с мультиколлинеарностью. Мы с вами применили самый наивный — **удаление взаимных факторов «на глаз»**. Увы, это получается не всегда.\n",
    "\n",
    "Два других метода называются по-разному, но по сути делают одно и тоже: это **метод главных компонент** для корреляционной матрицы и **сингулярное разложение** матрицы факторов. О них мы поговорим в следующем модуле. \n",
    "\n",
    "Кроме того, можно воспользоваться знакомыми нам **методами регуляризации**, о которых поговорим уже в этом модуле."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В ЧЁМ ПРОБЛЕМА МУЛЬТИКОЛЛИНЕАРНОСТИ ДЛЯ LINEAR REGRESSION?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на то что мультиколлинеарность делает матрицу корреляций более вырожденной, она не оказывает прямого влияния на точность модели сама по себе. Проблема полной вырожденности матрицы ($A^T A$), как мы уже обсуждали ранее, в sklearn вполне решается с помощью сингулярного разложения. То есть решение можно получить всегда даже при полной коллинеарности и сильной мультиколлинеарности, несмотря на противоречие с теорией линейной алгебры.\n",
    "\n",
    "? Однако сможем ли мы доверять такому решению?\n",
    "\n",
    "Бывают задачи, где важно не просто построить модель, но и проинтерпретировать её результат — коэффициенты линейной регрессии. Типичный пример — задача кредитного скоринга: в ней важно понять, что влияет на вероятность дефолта заёмщика.\n",
    "\n",
    "Проблема заключается в том, что в случае мультиколлинеарности коэффициенты линейной регрессии становятся неустойчивыми. Например, признак «остаток долга/сумма выдачи» вроде бы должен приводить к уменьшению вероятности дефолта, так как клиенту остаётся выплачивать всё меньшую сумму. Однако мультиколлинеарность приводит к тому, что подобранный в ходе обучения модели коэффициент может сменить знак на противоположный, а признак, с точки зрения модели, может начать говорить об обратном: чем меньше остаётся платить, тем больше вероятность дефолта. Подобный кейс хорошо описан в этой статье https://habr.com/ru/company/akbarsdigital/blog/592493/ — рекомендуем с ней ознакомиться.\n",
    "\n",
    "К тому же, чем больше в данных мультиколлинеарных факторов, тем сильнее увеличивается разброс коэффициентов регрессии. Полная коллинеарность означает, что существует бесконечное количество способов выразить один фактор через линейную комбинацию других. В свою очередь это значит, что есть бесконечное число возможных коэффициентов регрессии $\\vec{w}$, таких, которые дают одни и те же результаты. \n",
    "\n",
    "Чем больше высококоррелированных факторов в данных, тем больше таких линейных комбинаций и тем больше коэффициенты становятся по модулю, что приводит к проблеме под названием «взрывной рост весов», когда коэффициенты регрессии начинают стремиться к бесконечности, что приводит к «поломке» даже устойчивой к вырожденным матрицам модели."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.7**\n",
    "\n",
    "Вычислите коэффициент корреляции между векторами $\\vec{v}=(5, 1, 2)^T$ и $\\vec{u}=(4, 2, 8)^T$.\n",
    "\n",
    "Ответ округлите до двух знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.05241424],\n",
       "       [0.05241424, 1.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([5, 1, 2])\n",
    "u = np.array([4, 2, 8])\n",
    "np.corrcoef(v.T, u.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.8**\n",
    "\n",
    "Составьте корреляционную матрицу для системы векторов:\n",
    "\n",
    "$\\vec{x}_1=(5.1, 1.8, 2.1, 10.3, 12.1, 12.6)^T$  \n",
    "$\\vec{x}_2=(10.2, 3.7, 4.1, 20.5, 24.2, 24.1)^T$  \n",
    "$\\vec{x}_3=(2.5, 0.9, 1.1, 5.1, 6.1, 6.3)^T$  \n",
    "Для расчёта используйте библиотеку NumPy или Pandas.\n",
    "\n",
    "1. Чему равен ранг полученной корреляционной матрицы?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([5.1, 1.8, 2.1, 10.3, 12.1, 12.6])\n",
    "x2 = np.array([10.2, 3.7, 4.1, 20.5, 24.2, 24.1])\n",
    "x3 = np.array([2.5, 0.9, 1.1, 5.1, 6.1, 6.3])\n",
    "A = np.corrcoef(np.array([x1.T, x2.T, x3.T]))\n",
    "#A = np.array([x1, x2, x3]).T\n",
    "print(np.linalg.matrix_rank(A))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.99925473, 0.99983661],\n",
       "       [0.99925473, 1.        , 0.99906626],\n",
       "       [0.99983661, 0.99906626, 1.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Чему равен определитель полученной корреляционной матрицы? Ответ округлите до седьмого знака после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.862298229241645e-07\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.det(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Практика: линейная регрессия и метод наименьших квадратов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Настало время попрактиковаться в построении модели линейной регрессии и закрепить знания математического аппарата для работы с ней.\n",
    "\n",
    "Сразу импортируем необходимые библиотеки для работы с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У Василия, основателя компании «Газ-Таз-Ваз-Нефть», дела идут в гору: в этом году он открывает 100 новых скважин по добыче газа. Однако в целях оптимизации расходов и для потенциального повышения дохода Василию необходимо оценить, сколько денег будет приносить ему каждая из скважин, а также понять, какие факторы потенциально сильнейшим образом влияют на объём добычи газа. Для этого Василий решил нанять вас как специалиста по построению моделей машинного обучения.\n",
    "\n",
    "Василий представляет вам набор данных о добыче газа на своих скважинах. Файл с данными вы можете скачать здесь.\n",
    "\n",
    "Признаки:\n",
    "\n",
    "- Well — идентификатор скважины;\n",
    "- Por — пористость скважины (%);\n",
    "- Perm — проницаемость скважины;\n",
    "- AI — акустический импеданс (\\($кг/м^2*10^6$\\));\n",
    "- Brittle — коэффициент хрупкости скважины (%);\n",
    "- TOC — общий органический углерод (%);\n",
    "- VR — коэффициент отражения витринита (%);\n",
    "- Prod — добыча газа в сутки (млн. кубических футов).\n",
    "\n",
    "**Ваша задача** — построить регрессионную модель, которая прогнозирует выработку газа на скважине (**целевой признак** — Prod) на основе остальных характеристик скважины, и проинтерпретировать результаты вашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12.08</td>\n",
       "      <td>2.92</td>\n",
       "      <td>2.80</td>\n",
       "      <td>81.40</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4165.196191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.22</td>\n",
       "      <td>46.17</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3561.146205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>14.02</td>\n",
       "      <td>2.59</td>\n",
       "      <td>4.01</td>\n",
       "      <td>72.80</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.72</td>\n",
       "      <td>4284.348574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17.67</td>\n",
       "      <td>6.75</td>\n",
       "      <td>2.63</td>\n",
       "      <td>39.81</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.88</td>\n",
       "      <td>5098.680869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17.52</td>\n",
       "      <td>4.57</td>\n",
       "      <td>3.18</td>\n",
       "      <td>10.94</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3406.132832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Well    Por  Perm    AI  Brittle   TOC    VR         Prod\n",
       "0     1  12.08  2.92  2.80    81.40  1.16  2.31  4165.196191\n",
       "1     2  12.38  3.53  3.22    46.17  0.89  1.88  3561.146205\n",
       "2     3  14.02  2.59  4.01    72.80  0.89  2.72  4284.348574\n",
       "3     4  17.67  6.75  2.63    39.81  1.08  1.88  5098.680869\n",
       "4     5  17.52  4.57  3.18    10.94  1.51  1.90  3406.132832"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/unconv.csv')\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала в качестве модели будем использовать **простую линейную регрессию.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 5.1**\n",
    "\n",
    "Постройте корреляционную матрицу факторов, включив в неё целевой признак. Ответьте на следующие вопросы:\n",
    "\n",
    "1. Выберите топ-3 факторов, наиболее коррелированных с целевой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.121864</td>\n",
       "      <td>-0.069456</td>\n",
       "      <td>-0.057773</td>\n",
       "      <td>-0.021114</td>\n",
       "      <td>0.166757</td>\n",
       "      <td>0.024957</td>\n",
       "      <td>0.081462</td>\n",
       "      <td>-0.010434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.120639</td>\n",
       "      <td>-0.062298</td>\n",
       "      <td>-0.032792</td>\n",
       "      <td>0.031406</td>\n",
       "      <td>-0.009994</td>\n",
       "      <td>-0.014786</td>\n",
       "      <td>-0.019865</td>\n",
       "      <td>-0.053598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.119414</td>\n",
       "      <td>-0.023170</td>\n",
       "      <td>-0.071287</td>\n",
       "      <td>0.130194</td>\n",
       "      <td>0.123610</td>\n",
       "      <td>-0.014786</td>\n",
       "      <td>0.178076</td>\n",
       "      <td>-0.001920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.118189</td>\n",
       "      <td>0.063914</td>\n",
       "      <td>0.099073</td>\n",
       "      <td>-0.042373</td>\n",
       "      <td>-0.041902</td>\n",
       "      <td>0.013181</td>\n",
       "      <td>-0.019865</td>\n",
       "      <td>0.056270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.116965</td>\n",
       "      <td>0.060335</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>0.026404</td>\n",
       "      <td>-0.186744</td>\n",
       "      <td>0.076476</td>\n",
       "      <td>-0.015152</td>\n",
       "      <td>-0.064675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.116965</td>\n",
       "      <td>-0.072558</td>\n",
       "      <td>-0.049173</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.095414</td>\n",
       "      <td>-0.028034</td>\n",
       "      <td>0.022551</td>\n",
       "      <td>-0.033131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.118189</td>\n",
       "      <td>0.071548</td>\n",
       "      <td>0.226842</td>\n",
       "      <td>0.051414</td>\n",
       "      <td>-0.019275</td>\n",
       "      <td>-0.001538</td>\n",
       "      <td>0.027264</td>\n",
       "      <td>0.092180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.119414</td>\n",
       "      <td>-0.068502</td>\n",
       "      <td>-0.084391</td>\n",
       "      <td>0.068920</td>\n",
       "      <td>0.044692</td>\n",
       "      <td>-0.151679</td>\n",
       "      <td>-0.055211</td>\n",
       "      <td>-0.064429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.120639</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>-0.061130</td>\n",
       "      <td>0.050612</td>\n",
       "      <td>0.132411</td>\n",
       "      <td>0.090888</td>\n",
       "      <td>0.055477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.121864</td>\n",
       "      <td>0.140738</td>\n",
       "      <td>0.131425</td>\n",
       "      <td>0.032656</td>\n",
       "      <td>-0.009994</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.072036</td>\n",
       "      <td>0.168024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Well       Por      Perm        AI   Brittle       TOC        VR  \\\n",
       "0   -0.121864 -0.069456 -0.057773 -0.021114  0.166757  0.024957  0.081462   \n",
       "1   -0.120639 -0.062298 -0.032792  0.031406 -0.009994 -0.014786 -0.019865   \n",
       "2   -0.119414 -0.023170 -0.071287  0.130194  0.123610 -0.014786  0.178076   \n",
       "3   -0.118189  0.063914  0.099073 -0.042373 -0.041902  0.013181 -0.019865   \n",
       "4   -0.116965  0.060335  0.009798  0.026404 -0.186744  0.076476 -0.015152   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  0.116965 -0.072558 -0.049173  0.000144  0.095414 -0.028034  0.022551   \n",
       "196  0.118189  0.071548  0.226842  0.051414 -0.019275 -0.001538  0.027264   \n",
       "197  0.119414 -0.068502 -0.084391  0.068920  0.044692 -0.151679 -0.055211   \n",
       "198  0.120639  0.013333  0.006112 -0.061130  0.050612  0.132411  0.090888   \n",
       "199  0.121864  0.140738  0.131425  0.032656 -0.009994  0.105915  0.072036   \n",
       "\n",
       "         Prod  \n",
       "0   -0.010434  \n",
       "1   -0.053598  \n",
       "2   -0.001920  \n",
       "3    0.056270  \n",
       "4   -0.064675  \n",
       "..        ...  \n",
       "195 -0.033131  \n",
       "196  0.092180  \n",
       "197 -0.064429  \n",
       "198  0.055477  \n",
       "199  0.168024  \n",
       "\n",
       "[200 rows x 8 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# составляем матрицу наблюдений без дополнительного столбца из единиц\n",
    "A = data[['Well', 'Por', 'Perm','AI','Brittle','TOC','VR', 'Prod']]\n",
    "#y = data[['Prod']]\n",
    "# стандартизируем векторы в столбцах матрицы A\n",
    "A_cent = A - A.mean()\n",
    "A_st = A_cent/np.linalg.norm(A_cent, axis=0)\n",
    "# A_st.describe().round(2)\n",
    "A_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Well</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068927</td>\n",
       "      <td>0.077928</td>\n",
       "      <td>0.041483</td>\n",
       "      <td>-0.079252</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>-0.007279</td>\n",
       "      <td>0.026817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Por</th>\n",
       "      <td>0.068927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.760546</td>\n",
       "      <td>-0.461549</td>\n",
       "      <td>-0.218570</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.861910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perm</th>\n",
       "      <td>0.077928</td>\n",
       "      <td>0.760546</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.239636</td>\n",
       "      <td>-0.124017</td>\n",
       "      <td>0.471746</td>\n",
       "      <td>0.051023</td>\n",
       "      <td>0.727426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI</th>\n",
       "      <td>0.041483</td>\n",
       "      <td>-0.461549</td>\n",
       "      <td>-0.239636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>-0.531864</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>-0.390835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brittle</th>\n",
       "      <td>-0.079252</td>\n",
       "      <td>-0.218570</td>\n",
       "      <td>-0.124017</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.214282</td>\n",
       "      <td>0.317929</td>\n",
       "      <td>0.237155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOC</th>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>0.471746</td>\n",
       "      <td>-0.531864</td>\n",
       "      <td>-0.214282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.299483</td>\n",
       "      <td>0.654445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VR</th>\n",
       "      <td>-0.007279</td>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.051023</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>0.317929</td>\n",
       "      <td>0.299483</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.323182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prod</th>\n",
       "      <td>0.026817</td>\n",
       "      <td>0.861910</td>\n",
       "      <td>0.727426</td>\n",
       "      <td>-0.390835</td>\n",
       "      <td>0.237155</td>\n",
       "      <td>0.654445</td>\n",
       "      <td>0.323182</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Well       Por      Perm        AI   Brittle       TOC        VR  \\\n",
       "Well     1.000000  0.068927  0.077928  0.041483 -0.079252  0.022624 -0.007279   \n",
       "Por      0.068927  1.000000  0.760546 -0.461549 -0.218570  0.711831  0.111860   \n",
       "Perm     0.077928  0.760546  1.000000 -0.239636 -0.124017  0.471746  0.051023   \n",
       "AI       0.041483 -0.461549 -0.239636  1.000000  0.127599 -0.531864  0.499143   \n",
       "Brittle -0.079252 -0.218570 -0.124017  0.127599  1.000000 -0.214282  0.317929   \n",
       "TOC      0.022624  0.711831  0.471746 -0.531864 -0.214282  1.000000  0.299483   \n",
       "VR      -0.007279  0.111860  0.051023  0.499143  0.317929  0.299483  1.000000   \n",
       "Prod     0.026817  0.861910  0.727426 -0.390835  0.237155  0.654445  0.323182   \n",
       "\n",
       "             Prod  \n",
       "Well     0.026817  \n",
       "Por      0.861910  \n",
       "Perm     0.727426  \n",
       "AI      -0.390835  \n",
       "Brittle  0.237155  \n",
       "TOC      0.654445  \n",
       "VR       0.323182  \n",
       "Prod     1.000000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(A_st, columns=['Well', 'Por', 'Perm','AI','Brittle','TOC','VR', 'Prod'])\n",
    "A_corr = df.corr()\n",
    "A_corr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Вычислите ранг полученной матрицы корреляций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.matrix_rank(A_corr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычислите определитель матрицы корреляций. Ответ округлите до четвёртого знака после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007\n"
     ]
    }
   ],
   "source": [
    "print(round(np.linalg.det(A_corr), 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 5.2**\n",
    "\n",
    "Создайте матрицу наблюдений. Обозначьте её за X, а вектор правильных ответов — за y.\n",
    "\n",
    "1. Постройте модель линейной регрессии по методу наименьших квадратов. Для этого используйте матричную формулу NumPy. В качестве ответа укажите полученные оценки коэффициентов модели. Ответ округлите до целого числа.\n",
    "\n",
    "Коэффициент $\\hat{w}_0=$\n",
    " \n",
    "Коэффициент $\\hat{w}_{Well}=$\n",
    " \n",
    "Коэффициент $\\hat{w}_{Por}=$\n",
    "\n",
    "Коэффициент $\\hat{w}_{Perm}=$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = data[['Well', 'Por', 'Perm','AI','Brittle','TOC','VR']]\n",
    "y = data[['Prod']]\n",
    "\n",
    "# # стандартизируем векторы в столбцах матрицы A\n",
    "# A_cent = A - A.mean()\n",
    "# A_st = A_cent/np.linalg.norm(A_cent, axis=0)\n",
    "\n",
    "A = np.column_stack((np.ones(200), A))\n",
    "\n",
    "# # стандартизируем вектор целевой переменной\n",
    "# y_cent = y - y.mean()\n",
    "# y_st = y_cent/np.linalg.norm(y_cent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.23230803e+03]\n",
      " [ 5.07003631e-02]\n",
      " [ 2.30179140e+02]\n",
      " [ 1.16239006e+02]\n",
      " [-3.65202301e+02]\n",
      " [ 2.49943700e+01]\n",
      " [-7.84009294e+01]\n",
      " [ 7.85259815e+02]]\n"
     ]
    }
   ],
   "source": [
    "# вычислим OLS-оценку для стандартизированных коэффициентов\n",
    "#w_hat_st=np.linalg.inv(A_st.T@A_st)@A_st.T@y_st\n",
    "w_hat_st=np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat_st.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 5.3**\n",
    "\n",
    "Далее потренируемся строить предсказание для наблюдений целевой переменной.\n",
    "\n",
    "1. Постройте прогноз выработки газа для скважины с параметрами, указанными ниже. Чему равна абсолютная ошибка построенного вами прогноза для предложенной скважины (в миллионах кубических футов в день). Ответ округлите до целого числа.\n",
    "\n",
    "<img src=\"data\\MATHML_md2_5_4.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "mcf/day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.]\n"
     ]
    }
   ],
   "source": [
    "x_new = np.array([1, 106, 15.32, 3.71, 3.29, 55.99, 1.35, 2.42])\n",
    "y_new = 4748.315024\n",
    "#y_new_pred = x_new @ w_hat\n",
    "y_new_pred = np.dot(x_new, w_hat_st)\n",
    "print(np.round(np.abs(y_new_pred - y_new), 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Постройте прогноз выработки газа для всех скважин из обучающего набора данных. Чему равно значение метрики MAPE вашей модели? Ответ приведите в процентах (не указывайте знак процента), округлив его до первого знака после точки-разделителя.\n",
    "\n",
    "Примечание. Если вы забыли интерпретацию данной метрики и формулу (функцию из sklearn) для её расчёта, вернитесь в модуль «ML-2. Обучение с учителем: регрессия»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result MAPE 3.6:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "y_pred = A @ w_hat_st\n",
    "print('Result MAPE {:.1f}:'.format(mean_absolute_percentage_error(y, y_pred)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 5.4**\n",
    "\n",
    "Настало время анализа построенной модели. Посмотрите на коэффициенты и сравните их знаки со значениями выборочных корреляций между целевым признаком и факторами, которые вы нашли ранее.\n",
    "\n",
    "1. Есть ли в вашей модели фактор, при котором коэффициент в модели линейной регрессии противоречит соответствующему коэффициенту корреляции? Например, корреляция говорит, что зависимость между фактором и целью прямая, а модель говорит обратное."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ответ**:\n",
    "F Действительно, есть такой коэффициент \\($\\hat{w}_{TOC} \\approx -78$\\). Согласно построенной модели, зависимость между процентом органического углерода и производительностью скважины обратная. Однако, согласно положительному коэффициенту корреляции между этим факторым и целевым признаком, равным 0.65 (а также согласно реальным фактам из сферы добычи газа), зависимость должна быть прямой. Чтобы убедиться в этом, можно построить диаграмму рассеяния, отражающую зависимость между TOC и Prod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x='TOC', y='Prod', data=data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.23230803e+03],\n",
       "        [ 5.07003631e-02],\n",
       "        [ 2.30179140e+02],\n",
       "        [ 1.16239006e+02],\n",
       "        [-3.65202301e+02],\n",
       "        [ 2.49943700e+01],\n",
       "        [-7.84009294e+01],\n",
       "        [ 7.85259815e+02]]),\n",
       " array([7863008.02805328]),\n",
       " 8,\n",
       " array([1.75726792e+03, 3.87998844e+02, 7.41712919e+01, 1.61441542e+01,\n",
       "        1.13261139e+01, 5.07204769e+00, 1.60361699e+00, 9.73609868e-01]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.lstsq(A, y, rcond=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat: [[-1.23230803e+03  5.07003631e-02  2.30179140e+02  1.16239006e+02\n",
      "  -3.65202301e+02  2.49943700e+01 -7.84009294e+01  7.85259815e+02]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# создаём модель линейной регрессии\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# вычисляем коэффициенты регрессии\n",
    "model.fit(A, y)\n",
    "print('w_hat:', model.coef_)\n",
    "## w_hat: [ 6.   -1.25  1.25]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 5.5**\n",
    "\n",
    "Исключите из данных сильно коррелированные между собой факторы. Под сильной корреляцией в данной задаче будем понимать значения, выше \\($0.7$\\). Выбирая, какой из коррелированных факторов оставить, руководствуйтесь коэффициентом корреляции с целевой переменной: оставляйте тот фактор, который больше всего коррелирует с объёмом добычи газа.\n",
    "\n",
    "Также исключите из данных факторы, для которых корреляция с целевой переменной меньше \\($0.05$\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "x": [
          "Well",
          "Por",
          "Perm",
          "AI",
          "Brittle",
          "TOC",
          "VR",
          "Prod"
         ],
         "xaxis": "x",
         "y": [
          "Well",
          "Por",
          "Perm",
          "AI",
          "Brittle",
          "TOC",
          "VR",
          "Prod"
         ],
         "yaxis": "y",
         "z": [
          [
           1,
           0.0689265071693086,
           0.0779278089192913,
           0.041482618473639245,
           -0.07925236389007495,
           0.022624310437959868,
           -0.00727867826811346,
           0.026816776816352923
          ],
          [
           0.0689265071693086,
           1,
           0.7605459242862609,
           -0.46154937413830927,
           -0.21857029663949584,
           0.7118306758116918,
           0.11185977924568788,
           0.8619095390498698
          ],
          [
           0.0779278089192913,
           0.7605459242862609,
           1,
           -0.23963581449371407,
           -0.12401730308963656,
           0.47174631684108215,
           0.05102320050943362,
           0.7274261409379987
          ],
          [
           0.041482618473639245,
           -0.46154937413830927,
           -0.23963581449371407,
           1,
           0.1275994318104479,
           -0.531863596712834,
           0.4991426487391124,
           -0.3908347668172225
          ],
          [
           -0.07925236389007495,
           -0.21857029663949584,
           -0.12401730308963656,
           0.1275994318104479,
           1,
           -0.21428177043139066,
           0.31792865708154283,
           0.23715533470634584
          ],
          [
           0.022624310437959868,
           0.7118306758116918,
           0.47174631684108215,
           -0.531863596712834,
           -0.21428177043139066,
           1,
           0.2994828662620069,
           0.6544451995719833
          ],
          [
           -0.00727867826811346,
           0.11185977924568788,
           0.05102320050943362,
           0.4991426487391124,
           0.31792865708154283,
           0.2994828662620069,
           1,
           0.32318154748131245
          ],
          [
           0.026816776816352923,
           0.8619095390498698,
           0.7274261409379987,
           -0.3908347668172225,
           0.23715533470634584,
           0.6544451995719833,
           0.32318154748131245,
           1
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "height": 600,
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 600,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y"
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# импортируем библиотеки для визуализации\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.DataFrame(A_st, columns=['Well', 'Por', 'Perm','AI','Brittle','TOC','VR', 'Prod'])\n",
    "A_corr = df.corr()\n",
    "fig = px.imshow(A_corr, width=600, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Какие факторы вы будете исключать?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ответ**\n",
    "Верно:  \n",
    "A Верно.  Well     \n",
    "C Верно.  Perm  \n",
    "F Верно.  TOC  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Постройте линейную регрессию на обновлённых после удаления факторов данных по методу наименьших квадратов. Для этого используйте матричную формулу NumPy.\n",
    "\n",
    "В качестве ответа укажите полученные оценки коэффициентов модели. Ответ округлите до целого числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept   -1835.0\n",
      "Por           293.0\n",
      "AI           -200.0\n",
      "Brittle        28.0\n",
      "VR            517.0\n",
      "dtype: float64\n",
      "MAPE: 4.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "X = data.drop(['Prod', 'Perm', 'TOC', 'Well'], axis=1)\n",
    "y = data['Prod'].values\n",
    "index = ['intercept']+list(X.columns)\n",
    "n = X.shape[0]\n",
    "X = np.column_stack((np.ones(n), X))\n",
    "w_hat = np.linalg.inv(X.T@X)@X.T@y\n",
    "y_pred = X @ w_hat\n",
    "print(pd.Series(np.round(w_hat, 0), index=index))\n",
    "print(f'MAPE: {mean_absolute_percentage_error(y, y_pred)*100:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1835.44646069,   293.03624565,  -200.03091206,    27.64098209,\n",
       "          517.40272597]),\n",
       " array([10732170.91584706]),\n",
       " 5,\n",
       " array([739.64279965,  77.09118009,  11.80071094,   2.84278073,\n",
       "          1.2720029 ]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.lstsq(X, y, rcond=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Полиномиальная регрессия"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Когда мы знакомились с моделью линейной регрессии в модуле по машинному обучению, мы также кратко затронули и её модификации. Теперь настало время вновь обратиться к ним и посмотреть на них с математической точки зрения.\n",
    "\n",
    "Начнём с **модели полиномиальной регрессии**.\n",
    "\n",
    "**Полином (многочлен)** от $k$ переменных $x_1, \\ x_2, \\ ..., \\ x_k$ — это выражение (функция) вида:\n",
    "\n",
    "$P\\left(x_{1}, x_{2}, \\ldots, x_{k}\\right)=\\sum_{I} w_{i} x_{1}^{i_{1}} x_{2}{ }^{i_{2}} \\ldots x_{k}^{i_{k}}$,\n",
    "где\n",
    "\n",
    "$I$=(i_1, i_2, \\ ...., \\ i_k)$ — набор из $k$ целых неотрицательных чисел — степеней полинома;\n",
    " $w_I$ — числа, называемые **коэффициентами полинома**.\n",
    "Пока эта форма записи нам ничего не даёт — она слишком сложная. Давайте рассмотрим пример попроще. Когда переменная всего одна, полином будет записываться как:\n",
    "\n",
    "$P(x) = \\sum_{I} w_i x^i = w_0 + w_1 x^1 + w_2 x^2 + ... + w_k x^k$\n",
    "Выражение для полинома первой степени уже можно прочитать без особого труда. Видно, что на самом деле полином — это линейная комбинация из различных степеней переменной $x$, взятой с какими-то коэффициентами, причём некоторые из коэффициентов могут быть нулевыми.\n",
    "\n",
    "Максимальная степень при переменной $x$ называется **степенью полинома**.\n",
    "\n",
    "Самый простой пример полинома от одной переменной — парабола. Это полином второй степени. Вспомним её уравнение:\n",
    "\n",
    "$y = ax^2 + bx + c$,\n",
    "где $x$ — это некоторая неизвестная, а коэффициенты $a, b$ и $c$ определяют различные параметры этой параболы (направление её ветвей, начало параболы, её растяжение и т. д.).\n",
    "\n",
    "Ниже представлены возможные варианты расположения параболы в зависимости от коэффициентов $a, b$ и $c$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_1.png\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что уравнение $y = ax^2 + bx + c = 0$ определяет точки пересечения параболы с осью абсцисс — осью $X$. Чтобы найти точки пересечения, необходимо решить это квадратное уравнение. Вы наверняка делали это в школе: найти дискриминант, затем квадратные корни и т. д. Сейчас мы не будем этим заниматься, но понимание сути процедуры полезно для общего осознания принципа работы полиномиальной регрессии.\n",
    "\n",
    "Несколько простых примеров различных полиномов с числовыми коэффициентами для наглядности:\n",
    "\n",
    "$y=8+5x+2x^2$ — парабола\n",
    "\n",
    "$y=5x+x^3$ — кубическая парабола\n",
    "\n",
    "$y=8x^5+3x^4+x^3+x^2+5x$ — полином пятой степени\n",
    "\n",
    "Кстати, отметим важный факт: уравнение прямой также является частным случае полинома первой степени:\n",
    "\n",
    "$y=w_0+w_1 x$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Чем нам так интересны полиномы (особенно степени > 1)?\n",
    "\n",
    "На самом деле всё очень просто: полином степени $k$ способен описать абсолютно любую зависимость. Для этого ему достаточно задать набор наблюдений — точек, через которые он должен пройти (или пройти приблизительно). Вопрос стоит только в степени этого полинома — $k$. Например, ниже представлено три полинома: первой степени — линейная регрессия, второй степени — квадратичная регрессия и третьей степени — кубическая регрессия."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_2.png\" alt=\"drawing\" width=\"550\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что для полинома первой степени (линейной регрессии) представленная в данных нелинейная зависимость целевой переменной $y$ от фактора $x$, даётся тяжело: ошибка прогноза довольно велика. Два других полинома хорошо описывают поведение точек в пространстве.\n",
    "\n",
    "**Цель обучения** модели полиномиальной регрессии степени та же, что и для линейной регрессии: найти такие коэффициенты $w_i$, при которых ошибка между построенной функцией и обучающей выборкой была бы наименьшей из возможных.\n",
    "\n",
    "На самом деле для поиска этих коэффициентов мы можем использовать те же самые методы, что и для линейной регрессии, а именно **метод наименьших квадратов**. Мы можем взять уравнение полинома и потребовать, чтобы кривая проходила через точки в обучающей выборке (на графике выше они обозначены синим). Значения точек можно обозначить за $y_1, \\ y_2, \\ ..., \\ y_N$. Тогда мы хотим, чтобы для полинома степени $k$ (от одной переменной) выполнялась система уравнений:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_3.png\" alt=\"drawing\" width=\"350\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно количество точек в обучающей выборке $N$значительно больше, чем степень полинома $k$, а значит перед нами переопределённая СЛАУ относительно с $k+1$ неизвестной — $w_i$. Точных решений у системы практически никогда не будет, но мы умеем решать её приближённо. Мы даже вывели формулу для приближённого решения:\n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$\n",
    "→ Итак, мы вкратце обсудили, как должно выглядеть решение для полинома от одной переменной (от одного фактора). Теперь давайте более подробно остановимся на нюансах решения этой задачи и плавно перейдём к полиномам от нескольких переменных.\n",
    "\n",
    "Начнём с **квадратичной регрессии от одной переменной**.\n",
    "\n",
    "Пусть у нас некоторый вектор-фактор $\\vec{x}=(x_1, x_2, ..., x_N)^T$, от которого зависит целевая переменная $\\vec{y}=(y_1, y_2, ..., y_N)^T$. Будем предполагать, что зависимость нелинейная — допустим, квадратичная, то есть в качестве модели используется уравнение параболы. Тогда мы хотим выразить вектор $\\vec{y}$ как линейную комбинацию из векторов $\\vec{x}$ и $\\vec{x}^2$:\n",
    "\n",
    "$\\vec{y}=w_0 +w_1 \\vec{x}+w_2 \\vec{x}^2$\n",
    "\n",
    "или\n",
    "\n",
    "<img src=\"data\\MATHML_md2_6_4.png\" alt=\"drawing\" width=\"350\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важное лирическое отступление ↓**\n",
    "\n",
    "У внимательного студента должен был возникнуть вопрос: а что значит возвести вектор в квадрат? На первый взгляд, может показаться, что мы ищем скалярное произведение вектора с самим собой, ведь:\n",
    "\n",
    "$\\vec{x}^2=\\vec{x} \\cdot \\vec{x}=(\\vec{x} \\cdot \\vec{x})$\n",
    "Однако такой вариант нам не подходит, так как скалярное произведение — это число, а нам нужен именно вектор, иначе у нас не получится составить линейную комбинацию. Поэтому здесь под $\\vec{x}^2$ понимается вектор из квадратов координат вектора $\\vec{x}$. Тогда:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_6_5.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ещё одно важное замечание**: обратите внимание, что, несмотря на то что в нашем уравнении появились квадраты, оно всё равно продолжает быть линейным, так как неизвестным является не вектор $\\vec{x}$, а коэффициенты разложения $w_0, w_1$ и $w_2$. \n",
    "\n",
    "Итак, у нас получилась СЛАУ, состоящая из N уравнений на трёх неизвестных:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_6_6.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте для удобства и привычной нумерации переменных обозначим вектор $\\vec{z}_1=\\vec{x}$, а $\\vec{z}_2=\\vec{x}^2$. Тогда:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_7.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда получим уже знакомую нам неоднородную переопределённую СЛАУ:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_8.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или в матричном виде:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_9.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что мы делаем с такими СЛАУ? Верно — решаем. Правда, только приближённо. Раз система линейная и переопределённая, то МНК — наш лучший выбор. Тогда решение такой системы будет полностью аналогичным формуле для поиска коэффициентов простой линейной регрессии, разве что коэффициентов будет немного побольше:\n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$\n",
    "\n",
    "**? Пример № 1**\n",
    "\n",
    "Построить квадратичную регрессию на целевую переменную $\\vec{y}$ из одного фактора $\\vec{x}$, если:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_10.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, вот наша полиномиальная модель второй степени (квадратичная регрессионная модель):\n",
    "\n",
    "$\\vec{y}=w_0 +w_1 \\vec{x}+w_2 \\vec{x}^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_11.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужно найти такую линейную комбинацию из векторов $\\vec{1}, \\vec{x}$ и $\\vec{x}^2$, которая в сумме давала бы наилучшее приближение для $y$. Записываем систему в матричном виде:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_12.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем ранг матрицы системы и ранг расширенной матрицы системы на случай, если система определённая и имеет конкретное решение или вовсе имеет бесконечное количество решений:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_13.png\" alt=\"drawing\" width=\"750\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что ранг матрицы системы $(rk(A)=3)$ всё-таки меньше, чем ранг расширенной матрицы $(rk(A|\\vec{y})=4)$, а значит система не имеет конкретных решений — только приближённые. Найдём их:\n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$\n",
    "\n",
    "Для оптимизации процесса считать будем на Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4        0.46666667 0.13333333]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 3, -2, 1],\n",
    "    [1, 9, 4, 1]\n",
    "]).T\n",
    "y = np.array([4, 5, 2, 2])\n",
    "w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat) \n",
    "# [2.4        0.46666667 0.13333333]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, наш вектор оценок коэффициентов:\n",
    "\n",
    "$\\hat{\\vec{w}} = (\\hat{w}_0, \\hat{w}_1, \\hat{w}_2)^T = (2.4, 0.47, 0.13)^T$\n",
    "Чтобы сделать прогноз для нового наблюдения x_{new}, нам нужно поставить его в уравнение полинома с найденными коэффициентами:\n",
    "\n",
    "$\\vec{y}=2.4 +0.47 x_{new} + 0.13 x^{2}_{new}$\n",
    "Как вы понимаете, один фактор — это слишком тривиальная, далёкая от реальности ситуация. Давайте посмотрим, как выглядит уравнение квадратичной регрессии для случая двух переменных.\n",
    "\n",
    "Пусть у нас есть два фактора $\\vec{x}_1$ и $\\vec{x}_2$, от которых зависит целевая переменная $\\vec{y}$:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_14.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё то же самое: будем предполагать, что зависимость нелинейная, а точнее, квадратичная, то есть в качестве модели используется полином второй степени, задающий сложную трёхмерную поверхность, форма которой напрямую зависит от коэффициентов. Заметим, что в функцию уже будут включены попарные произведения факторов $\\vec{x}_1$ и $\\vec{x}_2$, а коэффициентов будет уже шесть:\n",
    "\n",
    "$\\vec{y}=w_{0}+w_{1} \\vec{x}_{1}+w_{2} \\vec{x}_{2}+w_{3} \\vec{x}_{1}^{2}+w_{4} \\vec{x}_{1} \\vec{x}_{2}+w_{5} \\vec{x}_{2}^{2}$\n",
    "\n",
    "**Примечание**. Здесь, как и в предыдущем случае, запись $\\vec{x}_1 \\vec{x}_2$ означает покоординатное (не скалярное!) произведение векторов $\\vec{x}_1$ и $\\vec{x}_2$.\n",
    "\n",
    "или"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_15.png\" alt=\"drawing\" width=\"750\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В матричном виде:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_16.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит громоздко, но на деле ничего серьёзного. Если воспринимать все полиномиальные столбцы как обычные столбцы, состоящие из чисел, мы просто снова получим обычную переопределённую неоднородную СЛАУ (если N значительно больше количества признаков k):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_17.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Сразу обратим внимание на то, что для того, чтобы система имела точное (была совместной) или хотя бы приближённое решение, нам необходимо, чтобы строк в матрице было как минимум шесть, причём все уравнения должны быть линейно независимыми. Иначе количество строк будет меньше количества столбцов, и тогда решений будет бесконечное множество (по первому следствию теоремы Кронекера — Капелли), а такой случай нам не подходит.\n",
    "\n",
    "**Что это значит на языке геометрии?**\n",
    "\n",
    "Это значит, что нам нужно как минимум шесть точек в трёхмерном пространстве с осями $\\vec{x}_1, \\vec{x}_2$ и $\\vec{y}$, чтобы мы смогли провести через них нашу поверхность, которую задаёт уравнение:\n",
    "\n",
    "$\\vec{y}=w_{0}+w_{1} x_{1}+w_{2} x_{2}+w_{3} x_{1}^{2}+w_{4} x_{1} x_{2}+w_{5} x_{2}^{2}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 2**\n",
    "\n",
    "Построить квадратичную регрессию на целевую переменную $\\vec{y}$ из двух факторов $\\vec{x}_1$ и $\\vec{x}_2$, если:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_18.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записываем нашу модель:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_19.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записываем систему в матричном виде:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_20.png\" alt=\"drawing\" width=\"750\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что первое и последнее уравнение системы противоречат друг другу. Можно не считать ранг —сразу понятно, что система будет переопределённой и нужно искать приблизительные решения по МНК:\n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$\n",
    "\n",
    "Чтобы решить такую задачу, без Python и матричных вычислений точно не обойтись. Переведём наши условия в программную реализацию. С точки зрения программы самое сложное в этой задаче — правильно записать матрицу A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.25799015  2.37672337 -0.1322068  -0.10208147 -0.26501791  0.29722471]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 3, -2, 1, 5, 13, 1],\n",
    "    [3, 4, 5, -2, 4, 11, 3],\n",
    "    [1, 9, 4, 1, 25, 169, 1],\n",
    "    [3, 12, -10, -2, 20, 143, 3],\n",
    "    [9, 16, 25, 4, 16, 121, 9]\n",
    "    \n",
    "]).T\n",
    "y = np.array([4, 5, 2, 2, 6, 8, -1])\n",
    "w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat)\n",
    "## [-2.25799015  2.37672337 -0.1322068  -0.10208147 -0.26501791  0.29722471]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы понимаете, вручную считать коэффициенты полиномиальной регрессии — неблагодарное дело. А ведь мы с вами рассмотрели только случай полинома второй степени с двумя факторами. Расти может как степень полинома, так и количество факторов. Можно представить, какую размерность может приобрести система уравнений.\n",
    "\n",
    "Например, уравнение модели полинома третьей степени для случая двух факторов будет иметь следующий вид:\n",
    "\n",
    "$\\vec{y}=w_{0}+w_{1} \\vec{x}_{1}+w_{2} \\vec{x}_{2}+w_{3} \\vec{x}_{1}^{2}+w_{4} \\vec{x}_{1} \\vec{x}_{2}+w_{5} \\vec{x}_{2}^{2}+w_{6} \\vec{x}_{1}^{3}+w_{7} \\vec{x}_{1}^{2} \\vec{x}_{2}+w_{8} \\vec{x}_{2}^{3}+w_{9} \\vec{x}_{1} \\vec{x}_{2}^{2}$\n",
    "Количество неизвестных уже равно 10, а факторов пока ещё два. Как говорится, то ли ещё будет...\n",
    "\n",
    "**Примечание**. Кстати, для того чтобы определить количество коэффициентов в регрессии, есть формула:\n",
    "\n",
    "$c = \\frac{n!}{(n-d)!d!}$,\n",
    "$n = k + d$,\n",
    "где $k$ — количество факторов, $d$ — степень полинома, а $!$ — символ факториала. Например, для двух факторов и пятой степени полинома будем иметь:\n",
    "\n",
    "$n=2+5=7$  \n",
    "$c = \\frac{n!}{(n-d)!d!} = \\frac{7!}{(7-5)!5!} = \\frac{7!}{2!5!} = \\frac{1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdot 5 \\cdot 6 \\cdot 7}{(1 \\cdot 2) \\cdot (1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdot 5)} = \\frac{42}{2} = 21$  \n",
    "То есть в матрице измерений A будет 21 столбец.\n",
    "\n",
    "Конечно, вручную создавать полиномиальные столбцы в матрице наблюдений мы не будем. В модуле «ML-2. Обучение с учителем: регрессия» мы с вами уже знакомились с полиномиальными признаками, генерация которых реализована в классе PolynomialFeatures из модуля preprocessing. \n",
    "\n",
    "Потренируемся на следующем **примере** ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 3**\n",
    "\n",
    "Строится полиномиальная регрессия второй степени, задано три фактора:\n",
    "\n",
    "$\\vec{x}_{1}=\\left(\\begin{array}{c} 1 \\\\ 3 \\\\ -2 \\\\ 1 \\\\ 5 \\\\ 13 \\\\ 1 \\end{array}\\right) \\vec{x}_{2}=\\left(\\begin{array}{c} 3 \\\\ 4 \\\\ 5 \\\\ -2 \\\\ 4 \\\\ 11 \\\\ 3 \\end{array}\\right) \\text { и } \\vec{x}_{3}=\\left(\\begin{array}{c} 4 \\\\ 5 \\\\ 2 \\\\ 2 \\\\ 6 \\\\ 8 \\\\ -1 \\end{array}\\right)$\n",
    "Создайте матрицу наблюдений $A_{poly}$ со сгенерированными полиномиальными признаками.\n",
    "\n",
    "Для начала составим обычную матрицу наблюдений $A$, расположив векторы в столбцах. Обратите внимание, что вектор из 1 мы не будем добавлять в матрицу (за нас это сделает генератор полиномиальных признаков):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  4]\n",
      " [ 3  4  5]\n",
      " [-2  5  2]\n",
      " [ 1 -2  2]\n",
      " [ 5  4  6]\n",
      " [13 11  8]\n",
      " [ 1  3 -1]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 3, -2, 1, 5, 13, 1],\n",
    "    [3, 4, 5, -2, 4, 11, 3],\n",
    "    [4, 5, 2, 2, 6, 8, -1],\n",
    "]).T\n",
    "print(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем импортируем класс PolynomialFeatures из библиотеки sklearn. Создадим объект этого класса, указав при инициализации степень полинома равной 2. Также укажем, что нам нужна генерация столбца из 1 (параметр include_bias=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2, include_bias=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось только вызвать метод fit_transform() от имени этого объекта и передать в него нашу матрицу наблюдений A. Для удобства выведем результат в виде DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3      4      5      6      7     8     9\n",
       "0  1.0   1.0   3.0  4.0    1.0    3.0    4.0    9.0  12.0  16.0\n",
       "1  1.0   3.0   4.0  5.0    9.0   12.0   15.0   16.0  20.0  25.0\n",
       "2  1.0  -2.0   5.0  2.0    4.0  -10.0   -4.0   25.0  10.0   4.0\n",
       "3  1.0   1.0  -2.0  2.0    1.0   -2.0    2.0    4.0  -4.0   4.0\n",
       "4  1.0   5.0   4.0  6.0   25.0   20.0   30.0   16.0  24.0  36.0\n",
       "5  1.0  13.0  11.0  8.0  169.0  143.0  104.0  121.0  88.0  64.0\n",
       "6  1.0   1.0   3.0 -1.0    1.0    3.0   -1.0    9.0  -3.0   1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_poly = poly.fit_transform(A)\n",
    "display(pd.DataFrame(A_poly))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили нашу матрицу A_{poly}. Давайте посмотрим на её столбцы:\n",
    "\n",
    "- столбец 0 — единичный, он отвечает за слагаемое с нулевой степенью полинома (любое число в степени 0 даёт единицу).\n",
    "- столбцы 1, 2 и 3 — это наши исходные признаки (векторы $\\vec{x}_1$, $\\vec{x}_2$ и $\\vec{x}_3$).\n",
    "- столбцы 4, 5 и 6 — произведения первого столбца со всеми столбцами: $\\vec{x}_1 \\vec{x}_1=\\vec{x}_{1}^{2}$, $\\vec{x}_1 \\vec{x}_2$ и $\\vec{x}_1 \\vec{x}_3$ соответственно.\n",
    "- столбцы 7 и 8 — произведения второго столбца со столбцами 2 и 3:  $\\vec{x}_2 \\vec{x}_2=\\vec{x}_{2}^{2}$ и $\\vec{x}_2 \\vec{x}_3$.\n",
    "- столбец 9 — произведение третьего столбца с самим собой: $\\vec{x}_3 \\vec{x}_3=\\vec{x}_{3}^{2}$.\n",
    "Таким образом, при генерации полиномиальных признаков объект PolynomialFeatures сначала создаёт исходные факторы, затем умножает каждый из них на все факторы и повторяет процедуру. При этом, если комбинация $\\vec{x}_i \\vec{x}_j$ уже была сгенерирована ранее, то комбинация $\\vec{x}_j \\vec{x}_i$ не рассматривается.\n",
    "\n",
    "А теперь построим модель полиномиальной регрессии на **реальных данных**.\n",
    "\n",
    "Возьмём все те же данные о стоимости жилья в районах Бостона. Будем использовать следующие четыре признака: LSTAT, CRIM, PTRATIO и RM. С их помощью мы построим полиномиальную регрессию от первой до пятой степени включительно, а затем сравним результаты по значению средней абсолютной процентной ошибки (MAPE).\n",
    "\n",
    "Чтобы не дублировать код, объявим функцию polynomial_regression(). Она будет принимать на вход матрицу наблюдений, вектор ответов и степень полинома, а возвращать матрицу с полиномиальными признаками, вектор предсказаний и коэффициенты регрессии, найденные по МНК:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(X, y, k):\n",
    "    poly = PolynomialFeatures(degree=k, include_bias=True)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    w_hat = np.linalg.inv(X_poly.T@X_poly)@X_poly.T@y\n",
    "    y_pred = X_poly @ w_hat\n",
    "    return X_poly, y_pred, w_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделяем интересующие нас признаки и строим полиномы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = boston_data[['LSTAT', 'PTRATIO', 'RM', 'CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    " \n",
    "A_poly, y_pred, w_hat = polynomial_regression(A, y, 1)\n",
    "A_poly2, y_pred2, w_hat2 = polynomial_regression(A, y, 2)\n",
    "A_poly3, y_pred3, w_hat3 = polynomial_regression(A, y, 3)\n",
    "A_poly4, y_pred4, w_hat4 = polynomial_regression(A, y, 4)\n",
    "A_poly5, y_pred5, w_hat5 = polynomial_regression(A, y, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на качество построенных регрессий, вычислив метрику:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE для полинома 1-й степени 18.20%\n",
      "MAPE для полинома 2-й степени  13.41%\n",
      "MAPE для полинома 3-й степени  12.93%\n",
      "MAPE для полинома 4-й степени  10.72%\n",
      "MAPE для полинома 5-й степени  470.54%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    " \n",
    "print('MAPE для полинома 1-й степени {:.2f}%'.format(mean_absolute_percentage_error(y, y_pred)*100))\n",
    "print('MAPE для полинома 2-й степени  {:.2f}%'.format(mean_absolute_percentage_error(y, y_pred2)*100))\n",
    "print('MAPE для полинома 3-й степени  {:.2f}%'.format(mean_absolute_percentage_error(y, y_pred3)*100))\n",
    "print('MAPE для полинома 4-й степени  {:.2f}%'.format(mean_absolute_percentage_error(y, y_pred4)*100))\n",
    "print('MAPE для полинома 5-й степени  {:.2f}%'.format(mean_absolute_percentage_error(y, y_pred5)*100))\n",
    "## MAPE для полинома 1-й степени 18.20%\n",
    "## MAPE для полинома 2-й степени  13.41%\n",
    "## MAPE для полинома 3-й степени  12.93%\n",
    "## MAPE для полинома 4-й степени  10.74%\n",
    "## MAPE для полинома 5-й степени  5328.16%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что видим? Полиномиальная регрессия первой степени (линейная регрессия) показывает наименьшее качество предсказания, так как зависимость между факторами и целевым признаком нелинейная. С повышением степени полинома процентная ошибка на обучающей выборке вроде бы падает, однако для полинома пятой степени она резко возрастает и начинает измеряться тысячами процентов. Это означает, что модель вообще не описывает зависимость в исходных данных — её прогноз не имеет никакого отношения к действительности.\n",
    "\n",
    "**Почему так происходит?**\n",
    "\n",
    "Проведём небольшое исследование. Для начала посмотрим на коэффициенты регрессии для полинома пятой степени. Смотреть на каждый из них неудобно, их слишком много (126, если быть точными), но можно взглянуть на минимум, максимум и среднее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>126.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1827.374637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>45541.271004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-213771.933237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.661564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.606294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>457926.187798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PRICE\n",
       "count     126.000000\n",
       "mean     1827.374637\n",
       "std     45541.271004\n",
       "min   -213771.933237\n",
       "25%        -0.661564\n",
       "50%         0.000009\n",
       "75%         2.606294\n",
       "max    457926.187798"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(w_hat5).describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что в степенях минимального и максимального коэффициентов явно что-то не так — коэффициенты слишком огромные (исчисляются миллионами).\n",
    "\n",
    "Теперь давайте взглянем на корреляционную матрицу для факторов, на которых мы строим полином пятой степени. Корреляцию со столбцом из единиц считать бессмысленно, поэтому мы не будем его рассматривать. Для удобства расчёта матрицы корреляций обернём матрицу  в DataFrame и воспользуемся методом corr():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ранг корреляционной матрицы: 110\n",
      "Количество факторов: 125\n"
     ]
    }
   ],
   "source": [
    "# считаем матрицу корреляций (без столбца из единиц)\n",
    "C = pd.DataFrame(A_poly5[:, 1:]).corr()\n",
    "# считаем ранг корреляционной матрицы\n",
    "print('Ранг корреляционной матрицы:', np.linalg.matrix_rank(C))\n",
    "# считаем количество факторов (не включая столбец из единиц)\n",
    "print('Количество факторов:', A_poly5[:, 1:].shape[1])\n",
    "# Ранг корреляционной матрицы: 110\n",
    "# Количество факторов: 125"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы нашли корень проблемы: ранг корреляционной матрицы — 110, в то время как общее количество факторов (не считая единичного столбца) — 125, то есть ранг корреляционной матрицы не максимален. Это значит, что в корреляционной матрице присутствуют единичные корреляции, а в исходной матрице — линейно зависимые столбцы.\n",
    "\n",
    "Как так вышло? На самом деле всё очень просто: в процессе перемножения каких-то из столбцов при создании полинома пятой степени получился такой полиномиальный фактор, который линейно выражается через другие факторы.\n",
    "\n",
    "В результате при вычислении обратной матрицы  у нас получилось деление на число, близкое к 0, а элементы обратной матрицы получились просто огромными. Отсюда и появились явно неверные степени коэффициентов, которые дают далёкий от действительности прогноз, что приводит к отрицательной метрике."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, заметим, что, например, для полинома четвёртой степени ранг матрицы корреляций максимален, то есть равен количеству факторов (не включая единичный столбец):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ранг корреляционной матрицы: 69\n",
      "Количество факторов: 69\n"
     ]
    }
   ],
   "source": [
    "# считаем матрицу корреляций (без столбца из единиц)\n",
    "C = pd.DataFrame(A_poly4[:, 1:]).corr()\n",
    "# считаем ранг корреляционной матрицы\n",
    "print('Ранг корреляционной матрицы:', np.linalg.matrix_rank(C))\n",
    "# считаем количество факторов (не включая столбец из единиц)\n",
    "print('Количество факторов:', A_poly4[:, 1:].shape[1])\n",
    "## Ранг корреляционной матрицы: 69\n",
    "## Количество факторов: 69"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому и коэффициенты регрессии полинома четвёртой степени находятся в адекватных пределах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-50.817470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>886.646328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-6919.292921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.187941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.322218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2304.985151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             PRICE\n",
       "count    70.000000\n",
       "mean    -50.817470\n",
       "std     886.646328\n",
       "min   -6919.292921\n",
       "25%      -0.187941\n",
       "50%      -0.000796\n",
       "75%       0.322218\n",
       "max    2304.985151"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(w_hat4).describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь посмотрим, что будет, если использовать для построения полиномиальной регрессии **реализацию из библиотеки sklearn**. Создадим функцию polynomial_regression_sk — она будет делать то же самое, что и прошлая функция, но средствами sklearn. Дополнительно будем смотреть также стандартное отклонение (разброс) по коэффициентам регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE для полинома степени 1 — 18.20%, СКО — 2\n",
      "MAPE для полинома степени 2 — 13.41%, СКО — 5\n",
      "MAPE для полинома степени 3 — 12.93%, СКО — 9\n",
      "MAPE для полинома степени 4 — 10.74%, СКО — 304\n",
      "MAPE для полинома степени 5 — 9.02%, СКО — 17055\n"
     ]
    }
   ],
   "source": [
    "def polynomial_regression_sk(X, y, k):\n",
    "    poly = PolynomialFeatures(degree=k, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    lr = LinearRegression().fit(X_poly, y)\n",
    "    y_pred = lr.predict(X_poly)\n",
    "    return X_poly, y_pred, lr.coef_\n",
    "\n",
    "A = boston_data[['LSTAT', 'PTRATIO', 'RM', 'CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "\n",
    "for k in range(1, 6):\n",
    "    A_poly, y_pred, w_hat = polynomial_regression_sk(A, y, k)\n",
    "    print(\n",
    "        \"MAPE для полинома степени {} — {:.2f}%, СКО — {:.0f}\".format(\n",
    "            k, mean_absolute_percentage_error(y, y_pred)*100, w_hat.std()\n",
    "        )\n",
    "\n",
    "    )\n",
    "## MAPE для полинома степени 1 — 18.20%, СКО — 2\n",
    "## MAPE для полинома степени 2 — 13.41%, СКО — 5\n",
    "## MAPE для полинома степени 3 — 12.93%, СКО — 9\n",
    "## MAPE для полинома степени 4 — 10.74%, СКО — 304\n",
    "## MAPE для полинома степени 5 — 9.02%, СКО — 17055"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очередная «магия» sklearn — построение полинома пятой степени прошло успешно.\n",
    "\n",
    "? Почему так получилось, если, строя полином «руками», мы получали противоположный результат?\n",
    "\n",
    "На самом деле с этим «заклинанием» из библиотеки sklearn мы уже знакомились в предыдущих юнитах. Секрет в том, что в sklearn для построения линейной регрессии используется не сама матрица наблюдений \\($A$\\), а её сингулярное разложение, которое гарантированно является невырожденным — из него исключаются линейно зависимые факторы. Таким образом, даже несмотря на немаксимальный ранг корреляционной матрицы, построить полином пятой степени всегда получится.\n",
    "\n",
    "Однако коэффициенты полинома пятой степени обладают значительно бόльшим разбросом, чем другие модели. Разброс будет всё больше расти при увеличении степени полинома. Коэффициенты не будут отражать реальной зависимости в данных и будут построены так, чтобы компенсировать линейную зависимость факторов, то есть будут неустойчивыми.\n",
    "\n",
    "К тому же, как мы уже знаем, чем выше степень полинома, тем выше шанс переобучения: модель может быть настолько сложной, что попросту попытается пройти через все точки в обучающем наборе данных, не уловив общей закономерности. Пример такой переобученной модели представлен ниже:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_6_24.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем ↓**\n",
    "\n",
    "- Модель полиномиальной регрессии — более общий случай линейной регрессии, в котором зависимость целевой переменной от факторов нелинейная.\n",
    "- Поиск коэффициентов полинома аналогичен линейной регрессии — решение неоднородной СЛАУ. \n",
    "- Возможна ситуация, когда какие-то сгенерированные полиномиальные факторы могут линейно выражаться через другие факторы. Тогда ранг корреляционной матрицы будет меньше числа факторов и поиск по классическому МНК-алгоритму не будет успешным.\n",
    "- В sklearn для решения последней проблемы предусмотрена защита — использование сингулярного разложения матрицы \\(A\\). Однако данная защита не решает проблемы неустойчивости коэффициентов регрессии.\n",
    "- Полиномиальная регрессия имеет сильную склонность к переобучению: чем выше степень полинома, тем сложнее модель и выше риск переобучения."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 6.1**\n",
    "\n",
    "Построена модель полиномиальной регрессии следующего вида:\n",
    "\n",
    "\\[$y=10.4+8 \\cdot x_1+0.5 \\cdot x_2+3 \\cdot x_{1}^{2}+0.4 \\cdot x_{2}^{2}+0 \\cdot x_1 x_2$\\]\n",
    "\n",
    "Поступило новое наблюдение, которое характеризуется вектором \\($x_{new}=(x_{1new}, x_{2new})^T=(1, 4)^T$\\).\n",
    "\n",
    "Сделайте прогноз целевой переменной с помощью полученной полиномиальной регрессии. Ответ округлите до первого знака после точки-разделителя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.8"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=10.4+8*1+0.5*4+3*1+0.4*4**2\n",
    "round(y, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 6.2**\n",
    "\n",
    "Строится полиномиальная регрессия второй степени от одного фактора \\($x$\\). Как будет выглядеть матрица наблюдений \\($A$\\), если:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_6_25.png\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "В качестве ответа введите элементы полученной матрицы \\($A$\\)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель полиномиальной регрессии от одного фактора:\n",
    "\\[$\\overrightarrow{y}=w_0\\ +w_1\\overrightarrow{x}+w_2{\\overrightarrow{x}}^2$\\]  \n",
    "Откуда матрица наблюдений \\($A$\\) будет иметь вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1]\n",
      " [ 1  3  9]\n",
      " [ 1 -2  4]\n",
      " [ 1  9 81]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 3, 9],\n",
    "    [1, -2, 4],\n",
    "    [1, 9, 81],\n",
    "])\n",
    "print(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 6.3**\n",
    "\n",
    "Какую размерность будет иметь матрица измерений \\(A\\), если в наборе данных пять факторов, а в качестве модели используется полином второй степени?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся формулой:\n",
    "\\[$c=\\frac{n!}{\\left(n-d\\right)!d!},$\\]  \n",
    "\\[$n=k+d,$\\]  \n",
    "где \\($k$\\) — количество факторов, \\($d$\\) — степень полинома, а \\($!$\\) — символ факториала.  \n",
    "Тогда:\n",
    "\\[$n=5+2=7$\\]  \n",
    "\\[$c=\\frac{5!}{\\left(7-2\\right)!2!}=\\frac{7!}{5!2!}=\\frac{5040}{240}=21$\\]  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 6.4**\n",
    "\n",
    "С помощью классического МНК найдите коэффициенты полиномиальной регрессии, если используется полином второй степени и задан фактор \\($\\vec{x}$\\) и целевая переменная \\($\\vec{y}$\\).\n",
    "\n",
    "\\[$\\vec{y} = w_0 + w_1 \\vec{x} + w_2 \\vec{x}^2$\\]\n",
    "\n",
    "\\[$\\vec{x}=\\left(\\begin{array}{c} 1 \\\\ 3 \\\\ -2 \\\\ 9 \\end{array}\\right) \\vec{y}=\\left(\\begin{array}{c} 3 \\\\ 7 \\\\ -5 \\\\ 21 \\end{array}\\right)$\\]\n",
    "\n",
    "В качестве ответа приведите координаты вектора коэффициентов \\($\\hat{w}_0$\\), \\($\\hat{w}_1$\\), \\($\\hat{w}_2$\\), округлив их до первого знака после точки-разделителя."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходная модель имеет вид:\n",
    "\\[$\\overrightarrow{y}=w_0\\ +w_1\\overrightarrow{x}+w_2{\\overrightarrow{x}}^2$\\]\n",
    "Тогда матрица наблюдений \\($A$\\) будет следующей:\n",
    "\n",
    "<img src=\"data\\pic-2.png\" alt=\"drawing\" width=\"350\"/>\n",
    "\n",
    "Оценки коэффициентов регрессионной модели находим по методу наименьших квадратов:\n",
    "\\[$\\widehat{\\overrightarrow{w}}={\\left(A^TA\\right)}^{-1}A^T\\overrightarrow{y}$\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1  2.5 -0. ]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "[1, 1, 1, 1],\n",
    "[1, 3, -2, 9],\n",
    "[1, 9, 4, 81]\n",
    "]).T\n",
    "y = np.array([3, 7, -5, 21])\n",
    "print(np.round(np.linalg.inv(A.T@A)@A.T@y, 1))\n",
    "## [ 0.1 2.5 -0. ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Регуляризация"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В предыдущем юните мы говорили о том, что полиномиальная регрессия склонна к переобучению. Это связано со сложностью модели и её способностью подстраиваться под очень сложные зависимости, из-за которых возникает высокий разброс.\n",
    "\n",
    "Рассмотрим **пример** ↓\n",
    "\n",
    "Обучим модель полиномиальной регрессии третьей степени. Будем использовать данные о жилье в Бостоне и возьмём следующие четыре признака: LSTAT, CRIM, PTRATIO и RM.\n",
    "\n",
    "Для оценки качества модели будем использовать кросс-валидацию и сравнивать среднее значение метрики на тренировочных и валидационных фолдах. Кросс-валидацию организуем с помощью функции cross_validate из модуля model_selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метрики используем среднюю абсолютную процентную ошибку — MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 12.64 %\n",
      "MAPE на валидационных фолдах: 24.16 %\n"
     ]
    }
   ],
   "source": [
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    " \n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    " \n",
    "# создаём модель линейной регрессии\n",
    "lr = LinearRegression()\n",
    " \n",
    "# оцениваем качество модели на кросс-валидации, метрика — MAPE\n",
    "cv_results = cross_validate(lr, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))\t\n",
    " \n",
    "## MAPE на тренировочных фолдах: 12.64 %\n",
    "## MAPE на валидационных фолдах: 24.16 %"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что мы видим? Даже при, казалось бы, небольшой, третьей степени полинома мы получили переобучение: на тренировочной выборке \\($MAPE=12.64 \\%$\\), а вот на тестовой — \\($MAPE=24.16 \\%$\\). Показатели качества отличаются практически в два раза, что говорит о высоком разбросе модели. Ещё более удручающий результат мы получим, если воспользуемся полиномом большей степени (при желании вы можете проверить это самостоятельно).\n",
    "\n",
    "Как с этим справиться, мы тоже уже знаем.\n",
    "\n",
    "- Можно попробовать понизить сложность модели (снизить степень полинома). Но до какой степени? Можно постепенно перебирать степень полинома до тех пор, пока не получим адекватные результаты, но, согласитесь, процедура не очень приятная.\n",
    "- Можно воспользоваться методами регуляризации.\n",
    "О втором способе как раз и поговорим подробнее с математической точки зрения.\n",
    "\n",
    "Для начала вспомним, что такое **регуляризация**.\n",
    "\n",
    "**Регуляризация** — это способ уменьшения переобучения моделей машинного обучения путём намеренного увеличения смещения модели для уменьшения её разброса.\n",
    "\n",
    "Регуляризация для линейной регрессии преследует сразу несколько целей. Однако далее мы увидим, что все эти цели на самом деле взаимосвязаны:\n",
    "\n",
    "- предотвратить переобучение модели;\n",
    "- включить в функцию потерь штраф за переобучение;\n",
    "- обеспечить существование обратной матрицы \\($(A^T A)^{-1}$\\);\n",
    "- не допустить огромных коэффициентов модели.\n",
    "→ Мы знаем, что большие значения весов — прямое свидетельство переобучения модели линейной регрессии и её нестабильности. Идея регуляризации состоит в наложении ограничения на вектор весов (часто говорят — наложение штрафа за высокие веса). В качестве штрафа принято использовать **норму вектора весов**.\n",
    "\n",
    "Давайте запишем это на языке линейной алгебры. Вот задача минимизации длины вектора ошибок, о которой мы говорили, когда выводили формулу МНК:\n",
    "\n",
    "\\[$\\left\\|\\vec{y} - A\\vec{w} \\right\\|^2 \\rightarrow min,$\\]\n",
    "\n",
    "где \\($\\vec{y}$\\) — вектор истинных ответов, \\($A$\\) — матрица наблюдений, \\($\\vec{w}$\\) — вектор весов линейной регрессии \\($\\vec{w}=(w_0, w_1, w_2, …, w_k)^T$\\).\n",
    "\n",
    "Вот её приближённое решение по МНК:\n",
    "\n",
    "\\[$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$\\]\n",
    "\n",
    "Теперь в исходную задачу оптимизации добавим ограничение на норму вектора весов — она не должна превышать некоторого заранее заданного \\($b$\\):\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_7_1.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $\\left\\|\\vec{w} \\right\\|_{L_{p}}$ — норма вектора порядка $p>1$, которая определяется как:\n",
    "\n",
    "$\\|\\vec{w}\\|_{L_{p}}=\\sqrt[p]{\\sum_{i=0}^{k}\\left|w_{i}\\right|^{p}}$  \n",
    "**Примечание**. Обратите внимание на сумму под знаком корня. У нас она начинается с $i=0$. Однако иногда в литературе, например здесь, можно встретить $i=1$, то есть свободный член $w_0$ рекомендуется не регуляризировать (не ограничивать). На самом деле это утверждение эвристическое и может как выполняться, так и нет, в зависимости от особенностей реализации. Мы будем придерживаться реализации в sklearn, в которой  $w_0$ всё-таки включается в регуляризацию. \n",
    "\n",
    "Порядок нормы $p$ в общем случае может быть любой — главное, чтобы она была больше 1. Однако на практике распространены только первая и вторая степени, так называемые **L_1- и L_2-регуляризации**. О них мы поговорим ниже, а пока доведём решение задачи до конца и получим общую формулу.\n",
    "\n",
    "Поставленная задача оптимизации называется **условной** — мы ищем минимум при некотором условии. Мы ещё не умеем решать такие задачи, но обязательно научимся, а пока на минутку заглянем в теорию оптимизации и поговорим о **методе множителей Лагранжа**. Прелесть данного метода в том, что он позволяет свести условную задачу оптимизации к безусловной, то есть благодаря Лагранжу мы можем перейти от системы к одному уравнению.\n",
    "\n",
    "Метод множителей Лагранжа говорит, что записанная система с ограничением эквивалентна следующей записи:\n",
    "\n",
    "$L(\\vec{w}, \\alpha)=\\|\\vec{y}-A \\vec{w}\\|^{2}+\\alpha\\left(\\|\\vec{w}\\|_{L_{p}}\\right)^{p} \\rightarrow \\min$,\n",
    "где $L(\\vec{w}, \\alpha)$ — функция Лагранжа, которая зависит не только от вектора весов модели $\\vec{w}$, но и от некоторой константы $\\alpha \\geq 0$ — множителя Лагранжа.\n",
    "\n",
    "Это и есть финальный результат, на котором мы пока что остановимся. По сути, ничего особо не изменилось по сравнению с изначальной задачей оптимизации. Добавилось только одно слагаемое — $\\alpha ( \\| \\vec{w} \\|_{L_{p}})^p$. Заметим, что если $\\alpha = 0$, то мы получаем исходную задачу $\\| \\vec{y} - A\\vec{w}  \\|^2  \\rightarrow min$. Далее мы увидим, что это маленькое слагаемое очень сильно поможет нам победить переобучение модели.\n",
    "\n",
    "В машинном обучении множитель Лагранжа  принято называть **коэффициентом регуляризации**. Он отвечает за «силу» регуляризации. Чем он больше, тем меньшие значения может принимать слагаемое $\\ {\\left({‖\\overrightarrow{w}‖}_{L_p}\\right)}^p$, то есть тем сильнее ограничения на норму весов. В этом и была наша цель — ограничить веса."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь разберёмся с **частными случаями**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_2-РЕГУЛЯРИЗАЦИЯ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём мы, как ни странно, с $L_2$-регуляризации, так как она очень наглядно показывает, как регуляризация обеспечивает невырожденность матрицы $A^T A$.\n",
    "\n",
    "**L_2-регуляризация (Ridge)**, или **регуляризация по Тихонову** — это регуляризация, в которой порядок нормы $p=2$. \n",
    "\n",
    "Тогда, если подставить $p=2$ в наши формулы, то оптимизационная задача в случае $L_2$-регуляризации будет иметь вид:\n",
    "\n",
    "${‖\\overrightarrow{w}‖}_{L_2}=\\sqrt[2]{\\sum^k_{i=0}{}{|w}_i{|}^2=}\\sqrt{\\sum^k_{i=0}{}{(w}_i)^2}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_7_2.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. Видно, что норма порядка $p=2$ на самом деле является знакомой нам длиной вектора. То есть в случае $L_2$-регуляризации мы накладываем ограничение на длину вектора весов $\\vec{w}$.\n",
    "\n",
    "В терминах функции Лагранжа задача будет выглядеть как:\n",
    "\n",
    "${‖\\overrightarrow{y}-A\\overrightarrow{w}‖}^2+α\\sum^k_{i=0}{}{(w}_i)^2→min$\n",
    "Как мы отметили ранее, у данной задачи даже есть аналитическое решение, полученное математиком Тихоновым, вот оно:\n",
    "\n",
    "${\\widehat{\\overrightarrow{w}}}_{ridge}={\\left(A^TA+\\alpha E\\right)}^{-1}A^T \\overrightarrow{y}$,\n",
    "где $E$ — единичная матрица размера $dim (I) =(k+1, k+1)$ вида:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_7_3.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. В других реализациях аналитического решения регуляризации Тихонова, отличных от sklearn, где коэффициент $w_0$ не участвует в регуляризации, единичная матрица $E$ заменяется на матрицу $I$, в которой первый столбец и первая строка — нулевые. Это делается для того, чтобы исключить коэффициент $w_0$ из регуляризации:\n",
    "\n",
    "$I=\\left(\\begin{array}{cccc} 0 & 0 & \\ldots & 0 \\\\ 0 & 1 & \\ldots & 0 \\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ 0 & 0 & \\cdots & 1 \\end{array}\\right)$\n",
    "\n",
    "Что мы в итоге получаем? Преимущество этой формулы в том, что, если $\\alpha >0$, то матрица $A^T A+\\alpha E$ гарантированно является невырожденной, даже если матрица $A^T A$ таковой не является. Так получается за счёт того, что по диагонали матрицы $A^T A$ мы добавляем поправки, которые создают линейную независимость между столбцами матрицы.\n",
    "\n",
    "Продемонстрируем это на **примере** ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? **Пример № 1**\n",
    "\n",
    "Построить линейную регрессию с $L_2$-регуляризацией, если:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_7_5.png\" alt=\"drawing\" width=\"350\"/>\n",
    "\n",
    "Коэффициент регуляризации $\\alpha = 5$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте составим матрицу наблюдений A для нашей задачи:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_7_6.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём матрицу Грама $A^T A$: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_7_7.png\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что матрица $A^T A$ вырождена: её второй и третий столбцы являются пропорциональными с коэффициентом 2. Значит, наша классическая формула МНК  (без сингулярного разложения) не сработает.\n",
    "\n",
    "$\\widehat{\\overrightarrow{w}}={\\left(A^TA\\right)}^{-1}A^T\\overrightarrow{y}$\n",
    "\n",
    "Проверим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # матрица наблюдений (включая столбец единиц)\n",
    "# A = np.array([\n",
    "#     [1, 1, 1, 1, 1],\n",
    "#     [1, 0, -3, 2, 4],\n",
    "#     [2, 0, -6, 4, 8]\n",
    "# ]).T\n",
    "# # вектор целевого признака\n",
    "# y = np.array([4, 3, -4, 2, 7])\n",
    "# # получаем оценку коэффициентов регрессии по МНК\n",
    "# w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "# print(w_hat) \n",
    "# ## LinAlgError: Singular matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы ожидаемо получили ошибку, говорящую о том, что матрица A вырождена. \n",
    "\n",
    "Теперь попробуем воспользоваться регуляризацией Тихонова. Для этого составляем матрицу E. Она будет размером 3x3 (количество параметров — 3):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_7_8.png\" alt=\"drawing\" width=\"350\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим регуляризационное слагаемое к матрице $A^T A$:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_7_9.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что матрица $A^T A+\\alpha E$ уже не будет вырожденной: её столбцы уже не являются линейно зависимыми, а значит решение будет существовать.\n",
    "\n",
    "Попробуем найти вектор оценок весов ${\\widehat{\\overrightarrow{w}}}_{ridge}$ по формуле:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6122449  0.29387755 0.5877551 ]\n"
     ]
    }
   ],
   "source": [
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# единичная матрица\n",
    "E = np.eye(3)\n",
    "# коэффициент регуляризации \n",
    "alpha = 5\n",
    "# получаем оценку коэффициентов регрессии по МНК с регуляризацией Тихонова\n",
    "w_hat_ridge = np.linalg.inv(A.T@A+alpha*E)@A.T@y\n",
    "print(w_hat_ridge) \n",
    "## [0.6122449  0.29387755 0.5877551 ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает! Мы получили вектор весов:\n",
    "\n",
    "$\\widehat{\\overrightarrow{w}}={\\left(0.61,\\ 0.29,\\ 0.59\\right)}^T$\n",
    "Итак, мы посмотрели, как работает аналитическое решение $L_2$-регуляризации. Однако в реализации sklearn для решения этой задачи поддерживается сразу несколько методов — как численных (координатный спуск, градиентный спуск или LBFGS), так и аналитических (классическая регуляризация Тихонова или она же через SVD-разложение). По умолчанию метод выбирается автоматически. На простых данных все методы будут показывать примерно одинаковые результаты при одном и том же значении коэффициента регуляризации, однако на реальных данных, когда данные не стандартизированы и присутствует сильная мультиколлинеарность между факторами, результат работы каждого из методов решения задачи оптимизации может значительно отличаться. Имейте это в виду при построении модели. Подробнее о методах вы можете прочитать в документации.\n",
    "\n",
    "Напомним, что за реализацию линейной регрессии в sklearn отвечает класс Ridge. Основной параметр модели, на который стоит обратить внимание — $alpha$, коэффициент регуляризации из формулы Тихонова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте обучим модель для решения нашей последней задачи, а затем проверим коэффициенты регрессии. Так как мы заранее заложили в матрицу A столбец из единиц, то, чтобы получить корректное решение, параметр fit_intercept следует установить в значение False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6122449  0.29387755 0.5877551 ]\n"
     ]
    }
   ],
   "source": [
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# получаем оценку коэффициентов регрессии по МНК с регуляризацией Тихонова\n",
    "ridge = Ridge(alpha=5, fit_intercept=False)\n",
    "ridge.fit(A, y)\n",
    "print(ridge.coef_) \n",
    "## [0.6122449  0.29387755 0.5877551 ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили тот же самый результат, что и раньше.\n",
    "\n",
    "Наконец, посмотрим, как регуляризация поможет побороть переобучение модели полиномиальной регрессии на наборе данных о домах в Бостоне. Используем те же самые признаки: LSTAT, CRIM, PTRATIO и RM. \n",
    "\n",
    "→ Сразу отметим, что для успешной сходимости численных методов оптимизации, которые используются для решения задачи условной оптимизации, необходима стандартизация (нормализация) исходных данных, которая не требовалась для аналитического МНК в классической линейной регрессии (LinearRegression).\n",
    "\n",
    "**Примечание**. Здесь под **стандартизацией** мы понимаем именно приведение распределения признака к нулевому среднему и единичному стандартному отклонению (StandartScaler), а не стандартизацию векторов, о которой мы говорили в этом модуле. Последнюю также можно использовать в качестве способа масштабирования данных, однако её реализации нет в sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся моделью полиномиальной регрессии третьей степени с регуляризацией Тихонова (коэффициент регуляризации возьмём равным 20) и проверим её качество на кросс-валидации по метрике MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 12.54 %\n",
      "MAPE на валидационных фолдах: 17.02 %\n"
     ]
    }
   ],
   "source": [
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# инициализируем стандартизатор StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "X = scaler.fit_transform(X)\n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    "# создаём модель линейной регрессии c L2-регуляризацией\n",
    "ridge = Ridge(alpha=20, solver='svd')\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(ridge, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))\n",
    "## MAPE на тренировочных фолдах: 12.54 %\n",
    "## MAPE на валидационных фолдах: 17.02 %"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам удалось уменьшить ошибку (MAPE) на валидационных фолдах кросс-валидации с 24.16% до 17.02% и сократить разницу в метриках, тем самым уменьшив разброс ответов модели.\n",
    "\n",
    "Теперь перейдём к $L_1$-регуляризации."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 7.4**\n",
    "\n",
    "Вычислите коэффициенты линейной регрессии с $L_2$-регуляризацией, используя аналитическую формулу Тихонова, если:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_7_10.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "Коэффициент регуляризации $\\alpha$=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08523045 -1.70784126  1.91141216  0.7293992 ]\n"
     ]
    }
   ],
   "source": [
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [5, 9, 4, 3, 5],\n",
    "    [15, 18, 18, 19, 19],\n",
    "    [7, 6, 7, 7, 7]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([24, 22, 35, 33, 36])\n",
    "# получаем оценку коэффициентов регрессии по МНК с регуляризацией Тихонова\n",
    "ridge = Ridge(alpha=1, fit_intercept=False)\n",
    "ridge.fit(A, y)\n",
    "print(ridge.coef_) \n",
    "## [0.6122449  0.29387755 0.5877551 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08523045 -1.70784126  1.91141216  0.7293992 ]\n"
     ]
    }
   ],
   "source": [
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [5, 9, 4, 3, 5],\n",
    "    [15, 18, 18, 19, 19],\n",
    "    [7, 6, 7, 7, 7]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([24, 22, 35, 33, 36])\n",
    "# единичная матрица\n",
    "E = np.eye(4)\n",
    "# коэффициент регуляризации \n",
    "alpha = 1\n",
    "# получаем оценку коэффициентов регрессии по МНК с регуляризацией Тихонова\n",
    "w_hat_ridge = np.linalg.inv(A.T@A+alpha*E)@A.T@y\n",
    "print(w_hat_ridge) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_1-РЕГУЛЯРИЗАЦИЯ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$L_1$-регуляризацией, Lasso (Least Absolute Shrinkage and Selection Operator)**, называется регуляризация, в которой порядок нормы $p=1$.\n",
    "\n",
    "Тогда оптимизационная задача в случае $L_1$-регуляризации будет иметь вид:\n",
    "\n",
    "${‖\\overrightarrow{w}‖}_{L_1}=\\sqrt[1]{\\sum^k_{i=0}{}{|w}_i{|}^1=}\\sum^k_{i=0}{}{|w}_i|$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_7_12.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. Таким образом, в случае L_1-регуляризации мы ограничиваем сумму модулей весов модели. Такая величина называется нормой Манхэттена (расстоянием городских кварталов) https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%B3%D0%BE%D1%80%D0%BE%D0%B4%D1%81%D0%BA%D0%B8%D1%85_%D0%BA%D0%B2%D0%B0%D1%80%D1%82%D0%B0%D0%BB%D0%BE%D0%B2.\n",
    "\n",
    "Запишем полученную систему в терминах метода Лагранжа:\n",
    "\n",
    "${‖\\overrightarrow{y}-A\\overrightarrow{w}‖}^2+α\\sum^k_{i=0}{}{|w}_i|→min$\n",
    "→ Можно показать, что данная задача имеет аналитическое решение, однако в реализации sklearn оно даже не заявлено как возможное для использования в связи с нестабильностью взятия производной от функции модуля, поэтому мы не будем его рассматривать. Ознакомиться с ним вы можете здесь. http://www.machinelearning.ru/wiki/images/7/7e/VetrovSem11_LARS.pdf\n",
    "\n",
    "В sklearn $L_1$-регуляризация реализована в классе Lasso https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html, а заданная выше оптимизационная задача решается **алгоритмом координатного спуска  (Coordinate Descent)**.\n",
    "\n",
    "Давайте посмотрим, как работает Lasso на «игрушечном» примере, а затем применим его для набора данных о домах в Бостоне."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 2**\n",
    "\n",
    "Построить линейную регрессию с L_1-регуляризацией, если:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_7_13.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэффициент регуляризации $\\alpha$ = 0.1.\n",
    "\n",
    "Составим матрицу наблюдений A для нашей задачи:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_7_14.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из примера №1 мы уже знаем, что матрица Грама $A^T A$ будет вырождена, а значит классического МНК-решения (не беря в расчёт сингулярное разложение) не получится.\n",
    "\n",
    "Попробуем найти коэффициенты регрессии с помощью $L_1$-регуляризации. Для этого подадим нашу матрицу наблюдений $A$ и вектор целевого признака $\\vec{y}$ в модель Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.14925373 0.         0.71921642]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# получаем оценку коэффициентов регрессии с помощью L1-регуляризации\n",
    "lasso = Lasso(alpha=0.1, fit_intercept=False)\n",
    "lasso.fit(A, y)\n",
    "print(lasso.coef_)\n",
    "## [1.14925373 0.         0.71921642]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот наша оценка вектора весов:\n",
    "\n",
    "$\\widehat{\\overrightarrow{w}}={\\left(1.15,\\ 0,\\ 0.72\\right)}^T$\n",
    "→ Сразу обращаем внимание, что, в отличие от регуляризации Тихонова, $L_1$-регуляризация «занулила» коэффициент, стоящий при факторе ${\\overrightarrow{x}}_1$. Это произошло не случайно, так как это особенность данного метода. Как говорится, «не баг, а фича», причём очень важная. Коэффициенты, стоящие при коллинеарных или высококоррелированных факторах, зануляются. Также чем выше коэффициент регуляризации, тем больше вероятность того, что коррелированные или малозначащие факторы будут исключены из модели. Чуть позже мы рассмотрим геометрическую интерпретацию и поймём, почему так происходит.\n",
    "\n",
    "А пока давайте применим $L_1$-регуляризацию к нашей полиномиальной модели третьей степени, прогнозирующей типичную цену на дома в районах Бостона.\n",
    "\n",
    "Так как метод координатного спуска, который применяется для поиска коэффициентов, является численным, то необходима стандартизация исходных данных, чтобы обеспечить ему сходимость. Возьмём в качестве коэффициента регуляризации $\\alpha=0.1$ и проверим качество полученной модели с помощью кросс-валидации по метрике MAPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 12.44 %\n",
      "MAPE на валидационных фолдах: 16.44 %\n"
     ]
    }
   ],
   "source": [
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "\n",
    "# инициализируем стандартизатор StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    "\n",
    "# создаём модель линейной регрессии c L1-регуляризацией\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000)\n",
    "\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(lasso, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))\n",
    "## MAPE на тренировочных фолдах: 12.44 %\n",
    "## MAPE на валидационных фолдах: 16.44 %"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что с помощью $L_1$-регуляризации удалось уменьшить ошибку модели (MAPE) на валидационных фолдах с 24.16% до 16.44% и сократить разницу в метриках на тренировочных и валидационных фолдах даже лучше, чем с этим справилась $L_2$-регуляризация. Однако на самом деле мы просто удачно выбрали коэффициент регуляризации — при других значениях могли получиться совершенно другие результаты."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELASTIC-NET"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последний вид регуляризации (хотя их на самом деле больше), который мы рассмотрим, называется Elastic-Net (эластичная сетка). Это комбинация $L_1$- и $L_2$-регуляризации.\n",
    "\n",
    "Идея Elastic-Net состоит в том, что мы вводим ограничение как на норму весов порядка $p=1$, так и на норму порядка $p=2$. Тогда оптимизационная задача будет иметь вид:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md2_7_15.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного модифицировав формулу функции Лагранжа, которая получается в результате такой задачи условной оптимизации, можно получить финальный результат:\n",
    "\n",
    "${\\|\\overrightarrow{y}-A\\overrightarrow{w}\\|}^2+\\alpha \\cdot \\lambda \\sum^k_{i=0}{}{|w}_i|+\\frac{\\alpha \\cdot (1-\\lambda )}{2}\\sum^k_{i=0}{}{(w}_i)^2\\to min$\n",
    "Здесь коэффициенты $\\alpha$ и $\\lambda$ отвечают за вклад слагаемых регуляризации.\n",
    "\n",
    "- Если $\\alpha=0$,  получаем классическую МНК-задачу оптимизации.\n",
    "- Если $\\lambda=1$, получаем Lasso-регрессию.\n",
    "- Если  $\\alpha \\neq 0, \\lambda=0$, получаем Ridge-регрессию с коэффициентом $\\frac{\\alpha}{2}$.\n",
    "Попробуйте самостоятельно подставить эти значения и убедиться в этом.\n",
    "\n",
    "Аналитического решения у этой задачи нет, поэтому для её решения в sklearn, как и для модели Lasso, используется координатный спуск.\n",
    "\n",
    "В sklearn эластичная сетка реализована в классе ElasticNet https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html из пакета с линейными моделями — linear_model. За коэффициент $\\alpha$ отвечает параметр $alpha$, за коэффициент $\\lambda$ — l1_ratio.\n",
    "\n",
    "Некоторые рекомендации от разработчиков ElasticNet:\n",
    "\n",
    "- Использование параметра l1_ratio <0.01 приводит к нестабильным результатам.\n",
    "- Вместо использования ElasticNet с $alpha=0$ лучше используйте LinearRegression, так как там применяется аналитическое решение, которое позволяет получать более точные решения, чем численный **координатный спуск**.\n",
    "\n",
    "По традиции рассмотрим «игрушечный» пример работы с Elastic-Net, а затем применим эту модель к нашей задаче о домах в Бостоне."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 3**\n",
    "\n",
    "Построить линейную регрессию с Elastic-Net-регуляризацией, если:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_7_16.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Решить задачу с тремя комбинациями коэффициентов регуляризации:\n",
    "\n",
    "1. $\\alpha=0.1$, $\\lambda=0.2$.  \n",
    "2. $\\alpha=0.1$, $\\lambda=0.7$.  \n",
    "3. $\\alpha=0.1$, $\\lambda=1$.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составим матрицу наблюдений A для нашей задачи:\n",
    "\n",
    "<img src=\"data\\MATHML_md2_7_17.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже знаем, что матрица Грама $A^T A$ будет вырождена, а значит классического МНК-решения (не беря в расчёт сингулярное разложение) не получится.\n",
    "\n",
    "Сразу переходим к построению регрессии с помощью ElasticNet.\n",
    "\n",
    "**→ Случай 1: $\\alpha=0.1$, $\\lambda=0.2$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.13492457 0.19525842 0.6237965 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# получаем оценку коэффициентов регрессии \n",
    "lasso = ElasticNet(alpha=0.1, l1_ratio=0.2, fit_intercept=False)\n",
    "lasso.fit(A, y)\n",
    "print(lasso.coef_)\n",
    "## [1.13492457 0.19525842 0.6237965 ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили оценку вектора коэффициентов:\n",
    "\n",
    "${\\widehat{\\overrightarrow{w}}}_{en_1}={\\left(1.13,\\ 0.2,\\ 0.62\\right)}^T$\n",
    "Обратим внимание, что зануления коэффициентов коллинеарных факторов ${\\overrightarrow{x}}_1$ и ${\\overrightarrow{x}}_2$ не произошло. Каждый из них вошёл в уравнение регрессии с ненулевым коэффициентом.\n",
    "\n",
    "**→ Случай 2: $\\alpha=0.1$, $\\lambda=0.7$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.14379753 0.         0.71993025]\n"
     ]
    }
   ],
   "source": [
    "# получаем оценку коэффициентов регрессии\n",
    "lasso = ElasticNet(alpha=0.1, l1_ratio=0.7, fit_intercept=False)\n",
    "lasso.fit(A, y)\n",
    "print(lasso.coef_)\n",
    "## [1.14379753 0.         0.71993025]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили оценку вектора коэффициентов:\n",
    "\n",
    "${\\widehat{\\overrightarrow{w}}}_{en_2}={\\left(1.14,\\ 0,\\ 0.72\\right)}^T$\n",
    "Обратим внимание, что произошло зануление коэффициентов. Это неспроста, так как мы понизили влияние $L_2$-регуляризации и одновременно повысили влияние $L_1$-регуляризации, которая, как мы уже знаем, приводит к исключению линейно зависимых факторов.\n",
    "\n",
    "**→ Случай 3: $\\alpha=0.1$, $\\lambda=1$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.14925373 0.         0.71921642]\n"
     ]
    }
   ],
   "source": [
    "# получаем оценку коэффициентов регрессии\n",
    "lasso = ElasticNet(alpha=0.1, l1_ratio=1, fit_intercept=False)\n",
    "lasso.fit(A, y)\n",
    "print(lasso.coef_)\n",
    "## [1.14925373 0.         0.71921642]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили оценку вектора коэффициентов:\n",
    "\n",
    "${\\widehat{\\overrightarrow{w}}}_{en_3}={\\left(1.14,\\ 0,\\ 0.72\\right)}^T$\n",
    "В округлениях значения не заметно, однако если присмотреться к коэффициентам более внимательно, можно увидеть, что мы получили в точности те же значения, которые получали для модели Lasso в примере № 2. Неудивительно, ведь мы обнулили влияние $L_2$-регуляризации, выставив l1_ratio=1. По сути, мы использовали чистую модель Lasso.\n",
    "\n",
    "? Возникает вопрос: какой набор коэффициентов линейной регрессии всё-таки подходит лучше?\n",
    "\n",
    "Ответить на него можно, только вычислив метрику качества и сравнив ошибки прогнозов каждой из полученных моделей. Мы уверены, вы можете сделать это самостоятельно.\n",
    "\n",
    "Нам осталось только попробовать применить Elastic-Net к данным о недвижимости в Бостоне.\n",
    "\n",
    "Как и для других моделей с регуляризацией, для Elastic-Net также лучше заранее позаботиться о стандартизации данных. В качестве коэффициентов регуляризации возьмём $\\alpha=0.1$,  $\\lambda=0.5$. Качество модели проверим с помощью кросс-валидации на пяти фолдах, метрика — MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 12.65 %\n",
      "MAPE на валидационных фолдах: 15.70 %\n"
     ]
    }
   ],
   "source": [
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# инициализируем стандартизатор StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "X = scaler.fit_transform(X)\n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    "# создаём модель линейной регрессии c L1- и L2-регуляризациями\n",
    "lasso = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(lasso, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100)) \n",
    "## MAPE на тренировочных фолдах: 12.65 %\n",
    "## MAPE на валидационных фолдах: 15.70 %"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, Elastic-Net позволил нам уменьшить значение MAPE на валидационных фолдах с 24.16% до 15.7%. Отличный результат! Он получился лучше, чем у моделей Ridge и Lasso, но опять же скажем, что так бывает не всегда.\n",
    "\n",
    "→ На практике при использовании моделей с регуляризацией стоит подбирать значения коэффициентов регуляризации с помощью методов подбора гиперпараметров, которые мы изучали в модуле «ML-7. Оптимизация гиперпараметров модели». Только после подбора гиперпараметров можно сделать вывод, какая из моделей показывает наилучшие результаты для решения конкретной задачи. Надеемся, вы помните, как подбираются гиперпараметры (если нет, освежите знания в модуле ML-7)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4604fddfbe3ec3d9d385c207fa0ddb9a05dfef66380401ad05b04bbd42d45367"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
