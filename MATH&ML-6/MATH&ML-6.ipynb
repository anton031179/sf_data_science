{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Введение"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Мы подошли к заключительной части раздела, посвящённого математическому анализу и оптимизации. В предыдущих модулях мы уже познакомились с функциями и их исследованием, научились находить минимальные и максимальные значения для функций, а также применять эти значения для решения реальных задач. В данном модуле мы продолжим углубляться в методы оптимизации, сравним их и обсудим, для каких случаев какие алгоритмы подходят лучше всего.\n",
    "\n",
    "За два прошлых модуля мы узнали достаточно много важных понятий и инструментов — давайте повторим их ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.1**\n",
    "\n",
    "Пусть прибыль в вашей компании выражается следующей функцией, которая зависит от параметра x — количества производимых товаров:\n",
    "\n",
    "$f(x)=-x^{4}+6 x^{3}-4 x^{2}+80$\n",
    "\n",
    "Найдите максимально возможную прибыль, которую вы можете получить, варьируя количество произведённых товаров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1/2, 4]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sympy\n",
    "from sympy import Symbol, S\n",
    "from sympy import diff\n",
    "from sympy import solveset, Eq\n",
    "from sympy import *\n",
    " \n",
    "x= symbols('x')\n",
    "f = -x**4 + 6*x**3 - 4*x**2 + 80\n",
    "fa = f.diff(x) \n",
    "sol = solve(fa, x)\n",
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 4 x^{3} + 18 x^{2} - 8 x$"
      ],
      "text/plain": [
       "-4*x**3 + 18*x**2 - 8*x"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 4\n",
    "eval('-x**4 + 6*x**3 - 4*x**2 + 80')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.4**\n",
    "\n",
    "Допустим, вы хотите произвести некоторое количество товара, которое зависит от часов работы двух ключевых сотрудников следующим образом:\n",
    "\n",
    "$f(x, y)=x^{2}+2 y^{2}$\n",
    "\n",
    "Однако вы можете оплатить этим сотрудникам не более 20 часов работы.\n",
    "\n",
    "Какое наибольшее количество товаров вы сможете произвести в таком случае?\n",
    "\n",
    "Помните, что количество часов работы должно быть целым, поэтому, прежде чем вычислять итоговый результат, округлите часы работы до целого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Целевая функция для аргументов a и b :\n",
      " f =  gamma\n",
      "Функция ограничений:  x + y - 20 = 0\n",
      "Функция Лагранжа :\n",
      "  w*(x + y - 20) + x**2 + 2*y**2\n",
      "df/dx = w + 2*x = 0\n",
      "df/dy = w + 4*y = 0\n",
      "df/dw = x + y - 20 = 0\n",
      "Стационарная точка M(x,y):\n",
      " 13.333333333333334 , 6.666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "267.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "x,y,w = symbols(' x y w' )\n",
    "g = x**2 +  2*y**2\n",
    "print('Целевая функция для аргументов a и b :\\n f = ', gamma)\n",
    "q = x + y - 20\n",
    "print('Функция ограничений: ', q,'= 0')\n",
    "f = x**2 +  2*y**2 + w*(x + y - 20)\n",
    "print('Функция Лагранжа :\\n ',f)\n",
    "fx = f.diff(x)\n",
    "print('df/dx =',fx,'= 0')\n",
    "fy = f.diff(y)\n",
    "print('df/dy =',fy,'= 0')\n",
    "fw = f.diff(w)\n",
    "print('df/dw =',fw,'= 0')\n",
    "sols = solve([fx,fy,fw],x,y,w)\n",
    "print('Стационарная точка M(x,y):\\n',float(sols[x]),',',float(sols[y]))\n",
    "\n",
    "x = round(float(sols[x]), 0)\n",
    "y = round(float(sols[y]), 0)\n",
    "eval('x**2 +  2*y**2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ЭТОТ МОДУЛЬ МЫ ПОСВЯТИМ ИЗУЧЕНИЮ НОВЫХ МЕТОДОВ ОПТИМИЗАЦИИ:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- рассмотрим, какие вариации существуют у уже известного вам алгоритма градиентного спуска, и узнаем, в чём суть **обратного распространения ошибки**;\n",
    "- познакомимся с **методом Ньютона** и **квазиньютоновскими методами BFGS и L-BFGS**;\n",
    "- разберём область применения задач линейного программирования и попрактикуемся в их решении;\n",
    "- узнаем, что такое **метод отжига** и **метод координатного спуска**.\n",
    "\n",
    "Из предыдущего модуля вы помните, что в каких-то методах оптимизации мы использовали лишь значение функции, где-то — считали градиент, а где-то — находили матрицы производных. Эти особенности дают возможность поделить все алгоритмы на три группы:\n",
    "\n",
    "- методы нулевого порядка (их работа основана на оценке значений самой целевой функции в разных точках);\n",
    "- методы первого порядка (при работе они используют первые производные в дополнение к информации о значении функции);\n",
    "- методы второго порядка (для них необходимо оценивать и значение функции, и значение градиента, и гессиан (матрицу Гессе).\n",
    "\n",
    "**Примечание**. Иногда в литературе можно встретить термины «оракул первого порядка» или «оракул нулевого порядка». Так обозначают компоненты алгоритма, которые находят информацию на каждом шаге для метода соответствующего порядка.\n",
    "\n",
    "В данном модуле мы будем рассматривать методы разных порядков и практически в каждом юните будем практиковаться в их применении. В конце мы обобщим полученные знания и сравним, какие методы и в каких случаях показывают наилучшие результаты.\n",
    "\n",
    "→ Важно отметить, что некоторые алгоритмы оптимизации мы пока не сможем использовать по прямому назначению: мы опробуем их на известных вам моделях линейной или логистической регрессии или на функциях одной или нескольких переменных, но основная сфера их применения — это искусственные нейронные сети. Тем не менее, понимание всего пула алгоритмов поможет вам в будущем без проблем подбирать необходимый метод вне зависимости от того, задачу какой сложности вы решаете: минимизируете простейшую функцию или обучаете нейронную сеть со сложной архитектурой."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Градиентный спуск: применение и модификации"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В предыдущем модуле мы познакомились с алгоритмом градиентного спуска, а также с его очень популярной вариацией — градиентным спуском с momentum. Но на самом деле у градиентного спуска очень много модификаций, и, поскольку это действительно самый известный алгоритм, он является основой множества методов оптимизации. В этом юните мы рассмотрим и сравним три вариации градиентного спуска, которые используются наиболее часто.\n",
    "\n",
    "Чтобы лучше понимать, какую роль играют методы оптимизации в DS и почему их так много, важно понимать основные **принципы работы нейронных сетей**. Также в современных статьях и обзорах разных методов оптимизации они практически всегда рассматриваются в контексте применения в нейронных сетях.\n",
    "\n",
    "Процесс обучения человеческого мозга очень сложен, и наука пока не может дать достаточно подробный ответ на вопрос о том, как мы получаем и усваиваем знания, как принимаем решения. Однако той информации, которую уже удалось получить, оказалось достаточно, чтобы по аналогии создать модели искусственных нейронных сетей.\n",
    "\n",
    "Люди учатся через пробы и ошибки, через процесс **синаптической пластичности**. Точно так же, как связи в мозге укрепляются и формируются по мере того, как мы переживаем новые события, мы обучаем искусственные нейронные сети, вычисляя ошибки нейросетевых предсказаний и усиливая или ослабляя внутренние связи между нейронами на основе этих ошибок.\n",
    "\n",
    "Для лучшего понимания удобно воспринимать нейронную сеть как функцию, которая принимает входные данные для получения итогового прогноза. Переменными этой функции являются параметры, или веса, нейрона.\n",
    "\n",
    "Следовательно, ключевым моментом для решения задачи, которую мы ставим для нейронной сети, будет корректировка значений весов таким образом, чтобы они аппроксимировали или наилучшим образом представляли набор данных.\n",
    "\n",
    "На изображении ниже показана простая нейронная сеть, которая получает входные данные $(X_1, \\ X_2, \\  X_3, \\ X_n)$. Эти входные данные передаются нейронам в слое, содержащем веса $(W_1, \\ W_2, \\ W_3, \\ W_n)$. Входные данные и веса подвергаются операции умножения, и результат суммируется с помощью специальной функции, а функция активации регулирует конечный результат модели."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_2_1.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы оценить, насколько эффективно работает наша нейронная сеть, необходим показатель оценки разницы между предсказанием нейронной сети и фактическим значением целевой функции, позволяющий корректировать параметры сети так, чтобы разница между прогнозом и реальностью была как можно меньше. В Data Science эту разницу часто называют **функцией стоимости**.\n",
    "\n",
    "Ниже представлена модель работы простейшей нейронной сети:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_2_2.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом изображении мы можем видеть модель простой нейронной сети из плотно связанных нейронов, которая классифицирует рукописные представления цифр 0, 1, 2, 3. Каждый нейрон в выходном слое соответствует цифре. Чем выше активация соединения с нейроном, тем выше вероятность, выдаваемая нейроном. Вероятность соответствует вероятности того, что цифра, переданная вперёд по сети, связана с активированным нейроном.\n",
    "\n",
    "Когда цифра 3 передаётся через сеть, мы ожидаем, что соединения (представленные на диаграмме стрелками), ответственные за классификацию этой цифры, будут иметь более высокую активацию, что приводит к более высокой вероятности для выходного нейрона, связанного с цифрой 3.\n",
    "\n",
    "Если объяснить происходящее более простыми словами, то механизм работы примерно такой:\n",
    "\n",
    "1. На первом слое мы выделяем из цифры какие-то первичные признаки (округлости, палочки).\n",
    "2. На втором слое мы выделяем уже какие-то паттерны (фрагменты цифры).\n",
    "3. На третьем слое паттерны складываются в целую цифру и мы можем предсказать результат.\n",
    "   \n",
    "Каждый раз элементы, более похожие на элементы цифры 3, дают более высокую активацию.\n",
    "\n",
    "За активацию нейрона отвечает несколько компонентов, и мы должны менять их в процессе обучения нашей нейронной сети, повышая качество её предсказания.\n",
    "\n",
    "Допустим, мы используем уже хорошо известную вам среднеквадратичную ошибку как меру для оценки качества модели. В этом случае мы каждый раз вычисляем её, получаем результат и отправляем его обратно для коррекции весов в сети. Здесь как раз и появляется понятие **обратного распространения ошибки**.\n",
    "\n",
    "**Обратное распространение (backpropagation)** — это механизм, с помощью которого компоненты, влияющие на итоговый результат, итеративно корректируются для уменьшения функции стоимости.\n",
    "\n",
    "Важнейшим математическим процессом, связанным с обратным распространением, является вычисление производных. Операции обратного распространения вычисляют частную производную функции стоимости по отношению к весам и активациям предыдущего слоя, чтобы определить, какие значения влияют на градиент функции стоимости.\n",
    "\n",
    "→ В процессе минимизации  функции ошибки мы постоянно вычисляем значение градиентов и таким образом приходим к локальному минимуму. На каждом этапе обучения нейронной сети её веса пересчитываются с помощью найденного градиента, причём скорость обучения (или, мы уже называли её ранее, темп обучения или шаг градиентного спуска) определяет коэффициент, с которым вносятся изменения в значения весов. Это повторяется на каждом шаге обучения нейронной сети. Нашей целью является постоянное приближение к локальному минимуму.\n",
    "\n",
    "Принцип обратного распространения ошибки заключается в том, что сначала мы устанавливаем какие-то случайные веса (параметры) для модели, находим итоговую ошибку и движемся по сети обратно, корректируя веса (для этого вычисляем частные производные, т. е. градиент).\n",
    "\n",
    "Данный процесс выглядит следующим образом:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_2_3.gif\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распространение ошибок и использование частных производных для корректировки весов происходит до тех пор, пока не будут скорректированы все параметры в сети вплоть до первого слоя.\n",
    "\n",
    "Если вам хочется больше узнать про механизм обратного распространения ошибки, рекомендуем обратиться к этой статье https://brilliant.org/wiki/backpropagation/.\n",
    "\n",
    "Как вы можете видеть, даже в самом простом примере нейронной сети много переменных и действий, которые с ними происходят. Из-за этого ландшафт функции потерь для нейронных сетей становится очень сложным. К примеру, ландшафт для нейронной сети с 56 слоями может выглядеть так:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_2_4.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущем модуле мы без проблем решили задачу с помощью классического градиентного спуска, и результат совпал с итогом, полученным с помощью реализации алгоритма в стандартной библиотеке Python. Но функция потерь там была совершенно другой. Когда дело доходит до искусственных нейронных сетей, алгоритмы оптимизации сталкиваются с множеством проблем.\n",
    "\n",
    "**Основные проблемы при реализации градиентного спуска**:\n",
    "\n",
    "- Классический градиентный спуск склонен застревать в точках локального минимума и даже в седловых точках, словом — везде, где градиент равен нулю. Это мешает найти глобальный минимум.\n",
    "- Обычно у оптимизируемой функции очень сложный ландшафт: где-то она совсем пологая, где-то более крутой обрыв. В таких ситуациях градиентный спуск показывает не лучшие результаты. Так происходит потому, что в алгоритме градиентного спуска фиксированный шаг, а нам в идеале хотелось бы его изменять в зависимости от формы функции прямо в процессе обучения.\n",
    "- Много проблем возникает из-за темпа обучения: при низком алгоритм сходится невероятно медленно, при быстром — «пролетает» мимо минимумов.\n",
    "- При обучении градиентного спуска координаты в некоторых измерениях могут редко изменяться, что приводит к плохой обобщающей способности алгоритма. Можно попытаться придать каждого признаку бόльшую важность, но в таком случае есть серьёзный риск переобучить модель.\n",
    "  \n",
    "Тем не менее, нельзя отрицать, что градиентный спуск — невероятно эффективный и популярный алгоритм. Допустим, он прекрасно подходит для минимизации среднеквадратичной ошибки в случае решения задачи регрессии. Однако в силу его несовершенств, которые очень явно проявляются в сложных моделях (например, в нейронных сетях), были созданы некоторые его модификации, которые позволяют решать задачи с большей результативностью.\n",
    "\n",
    "Обычно выделяют **три основных вариации градиентного спуска**:\n",
    "\n",
    "- Batch Gradient Descent;\n",
    "- Stochastic Gradient Descent;\n",
    "- Mini-batch Gradient Descent.\n",
    "  \n",
    "Далее мы рассмотрим все эти модификации и обсудим их различия. Важно понимать, что, несмотря на то что про их применение часто говорят именно в контексте нейронных сетей, они прекрасно подходят и для использования с обычными методами машинного обучения. К примеру, для стандартной линейной регрессии также подходит и обычный градиентный спуск."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BATCH GRADIENT DESCENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая вариация — это **Batch Gradient Descent**. По-русски её называют **пакетным градиентным спуском**, или **ванильным градиентным спуском** (хотя англоязычную вариацию Vanilla Gradient Descent чаще не переводят). По сути, это и есть классический градиентный спуск, который мы с вами рассматривали в предыдущем модуле."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_2_5.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто в различных источниках шаг Batch Gradient Descent записывается в следующих обозначениях:\n",
    "\n",
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "**Примечание**. На самом деле совершенно не важно, какими буквами выражать те или иные объекты, но мы будем использовать в этом модуле обозначения, которые часто встречаются в научных статьях и различной литературе.\n",
    "\n",
    "Здесь $\\theta$ — вектор с параметрами функции, $\\eta$ — шаг градиента, $\\nabla_{\\theta} J(\\theta)$ — градиент функции, найденный по её параметрам.\n",
    "\n",
    "Таким образом, на каждом шаге градиентный спуск находит направление наискорейшего убывания функции и движется чётко по нему. Поэтому для несложных функций он достаточно быстро сходится. Его обучение можно изобразить следующим образом:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_2_6.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой градиентный спуск достаточно хорошо работает, если мы рассматриваем выпуклые или относительно гладкие функции ошибки. Такие, к примеру, мы можем наблюдать у линейной или логистической регрессии. Мы обсуждали, что градиентный спуск не умеет находить глобальный минимум среди прочих, но, например, для выпуклой функции он может это сделать. Поэтому с выпуклыми функциями (допустим, со среднеквадратичной ошибкой для линейной регрессии) нам удобнее всего использовать именно его.\n",
    "\n",
    "Но всё усложняется, когда мы хотим применить его при обучении нейронных сетей. Как уже говорилось, у таких моделей функция потерь имеет много локальных экстремумов, в каждом из которых градиентный спуск может застрять. Когда данных очень много, а оптимизируемая функция очень сложная, данный алгоритм применять затруднительно, так как поиск градиента по всем наблюдениям делает задачу очень затратной в плане вычислительных ресурсов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOCHASTIC GRADIENT DESCENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что мы реализуем градиентный спуск для набора данных объёмом 10 000 наблюдений и у нас десять переменных. Среднеквадратичную ошибку считаем по всем точкам, то есть для 10 000 наблюдений. Производную необходимо посчитать по каждому параметру, поэтому фактически за каждую итерацию мы будем выполнять не менее 100 000 вычислений. И если, допустим, у нас 1000 итераций, то нам нужно 100000*1000=100000000 вычислений. Это довольно много, поэтому градиентный спуск на сложных моделях и при использовании больших наборов данных работает крайне долго.\n",
    "\n",
    "Чтобы преодолеть эту проблему, придумали **стохастический градиентный спуск**. Слово «стохастический» можно воспринимать как синоним слова «случайный». Где же при использовании градиентного спуска может возникнуть случайность? При выборе данных. При реализации стохастического спуска вычисляются градиенты не для всей выборки, а только для случайно выбранной единственной точки."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_2_7.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это значительно сокращает вычислительные затраты.\n",
    "\n",
    "В виде формулы это можно записать следующим образом:\n",
    "\n",
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J\\left(\\theta ; x^{(i)} ; y^{(i)}\\right)$\n",
    "\n",
    "На визуальном представлении ниже можно увидеть, что стохастический спуск создаёт много колебаний при сходимости. Это происходит как раз за счёт того, что берётся не вся выборка, а только один объект, и между объектами может быть достаточно большая разница. Чем меньше выборка, тем меньше стабильности при реализации."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_2_8.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стохастический градиентный спуск очень часто используется в нейронных сетях и сокращает время машинных вычислений, одновременно повышая сложность и производительность крупномасштабных задач."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MINI-BATCH GRADIENT DESCENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Третья вариация градиентного спуска — **Mini-batch Gradient Descent**. Также можно называть его м**ини-пакетным градиентным спуском**. По сути, эта модификация сочетает в себе лучшее от классической реализации и стохастического варианта. На данный момент это наиболее популярная реализация градиентного спуска, которая используется в глубоком обучении (т. е. в обучении нейронных сетей).\n",
    "\n",
    "В ходе обучения модели с помощью мини-пакетного градиентного спуска обучающая выборка разбивается на пакеты (батчи), для которых рассчитывается ошибка модели и пересчитываются веса."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_2_9.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть, с одной стороны, мы используем все преимущества обычного градиентного спуска, а с другой — уменьшаем сложность вычислений и повышаем их скорость по аналогии со стохастическим спуском. Кроме того, алгоритм работает ещё быстрее за счёт возможности применения векторизованных вычислений.\n",
    "\n",
    "Формализовать это можно следующим образом:\n",
    "\n",
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J\\left(\\theta ; x^{(i: i+n)} ; y^{(i: i+n)}\\right)$\n",
    "\n",
    "Как показывает визуализация ниже, амплитуда колебаний при сходимости алгоритма больше, чем в классическом градиентном спуске, но меньше, чем в стохастическом:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_2_10.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем сравнение трёх вариаций градиентного спуска**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\pic-1.png\" alt=\"drawing\" width=\"1100\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Итак, мы рассмотрели алгоритмы градиентного спуска и сравнили их параметры. Теперь, если вам необходимо будет выбрать метод градиентного спуска, вы сможете понять, какой из них подходит лучше всего."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.7**\n",
    "\n",
    "Давайте потренируемся применять стохастический градиентный спуск для решения задачи линейной регрессии. Мы уже рассмотрели его реализацию «с нуля», однако для решения практических задач можно использовать готовые библиотеки.\n",
    "\n",
    "Загрузите стандартный датасет об алмазах из библиотеки Seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('diamonds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалите часть признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['depth', 'table', 'x', 'y', 'z'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодируйте категориальные признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логарифмируйте признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['carat'] = np.log(1+df['carat'])\n",
    "df['price'] = np.log(1+df['price'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определите целевую переменную и предикторы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"price\")\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите выборку на обучающую и тестовую (объём тестовой возьмите равным 0.33), значение random_state должно быть равно 42.\n",
    "\n",
    "Теперь реализуйте алгоритм линейной регрессии со стохастическим градиентным спуском (класс SGDRegressor). Отберите с помощью GridSearchCV оптимальные параметры по следующей сетке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "parameters = {\n",
    "\"loss\": [\"squared_error\", \"epsilon_insensitive\"],\n",
    "\"penalty\": [\"elasticnet\"],\n",
    "\"alpha\": np.logspace(-3, 3, 15),\n",
    "\"l1_ratio\": np.linspace(0, 1, 11),\n",
    "\"max_iter\": np.logspace(0, 3, 10).astype(int),\n",
    "\"random_state\": [42],\n",
    "\"learning_rate\": [\"constant\"],\n",
    "\"eta0\": np.logspace(-4, -1, 4)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите регрессию с оптимальными параметрами. В качестве ответа введите получившееся значение MSE, предварительно округлив его до третьего знака после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0026826957952797246, 'eta0': 0.001, 'l1_ratio': 0.0, 'learning_rate': 'constant', 'loss': 'epsilon_insensitive', 'max_iter': 21, 'penalty': 'elasticnet', 'random_state': 42}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.043"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDRegressor(random_state=42)\n",
    "sgd_cv = GridSearchCV(estimator=sgd, param_grid=parameters, n_jobs=-1)\n",
    "sgd_cv.fit(X_train, y_train)\n",
    "print(sgd_cv.best_params_)\n",
    "sgd = SGDRegressor(**sgd_cv.best_params_)\n",
    "sgd.fit(X_train, y_train)\n",
    "sgd.score(X_train, y_train) # r2\n",
    "ls = sgd.predict(X_test)\n",
    "round(mean_squared_error(y_test, ls), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *БОНУС: АЛГОРИТМЫ, ОСНОВАННЫЕ НА ГРАДИЕНТНОМ СПУСКЕ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск настолько популярен и хорошо применим для решения различных задач, что послужил основой множества дополнительных методов. Поговорим про некоторые из них.\n",
    "\n",
    "Иногда в данных присутствуют очень редко встречающиеся входные параметры.\n",
    "\n",
    "Например, если мы классифицируем письма на «Спам» и «Не спам», таким параметром может быть очень специфическое слово, которое встречается в спаме намного реже других слов-индикаторов. Или, если мы говорим о распознавании изображений, это может быть какая-то очень редкая характеристика объекта.\n",
    "\n",
    "В таком случае нам хотелось бы иметь для каждого параметра свою скорость обучения: чтобы для часто встречающихся она была низкой (для более точной настройки), а для совсем редких — высокой (это повысит скорость сходимости). То есть нам очень важно уметь обновлять параметры модели, учитывая то, насколько типичные и значимые признаки они кодируют.\n",
    "\n",
    "Решение этой задачи предложено в рамках алгоритма **AdaGrad** (его название обозначает, что это адаптированный градиентный спуск). В нём обновления происходят по следующему принципу:\n",
    "\n",
    "$G_t = G_t + g^2_t$\n",
    "$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} g_t$\n",
    "\n",
    "Здесь мы храним сумму квадратов градиентов для каждого параметра. Таким образом, параметры, которые сильно обновляются каждый раз, начинают обновляться слабее. Скорость обучения в таком алгоритме будет постоянно затухать. Мы будем начинать с больших шагов, и с приближением к точке минимума шаги будут уменьшаться — это улучшит скорость сходимости.\n",
    "\n",
    "Данный алгоритм достаточно популярен и работает лучше стохастического градиентного спуска. Его использует и компания Google в своих алгоритмах классификации изображений.\n",
    "\n",
    "Однако снижение скорости обучения в AdaGrad иногда происходит слишком радикально, и она практически обнуляется. Чтобы решить эту проблему, были созданы алгоритмы RMSProp, AdaDelta, Adam и некоторые другие.\n",
    "\n",
    "Если вам интересно подробнее узнать о перечисленных алгоритмах и окунуться в процесс оптимизации нейронных сетей, рекомендуем обратиться к следующим статьям:\n",
    "\n",
    "- \"An overview of gradient descent optimization algorithms\" https://ruder.io/optimizing-gradient-descent/index.html#adagrad  \n",
    "- «Методы оптимизации нейронных сетей» https://habr.com/ru/post/318970/  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Метод Ньютона"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В предыдущем юните мы познакомились с различными вариациями градиентного спуска. Но для того чтобы наш арсенал методов был более полным и позволял решать самые разные задачи, нам необходимо разобраться и с рядом других алгоритмов. В этом юните мы будем изучать метод Ньютона.\n",
    "\n",
    "Метод Ньютона используется во многих алгоритмах машинного обучения. Часто в литературе его сравнивают с градиентным спуском, так как два этих алгоритма очень популярны. Вы уже сталкивались с методом Ньютона, но не знали об этом.\n",
    "\n",
    "В документации https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html для функции LogisticRegression из библиотеки scikit-learn представлено пять вариантов алгоритмов оптимизации, которые можно использовать при обучении модели:\n",
    "\n",
    "- 'newton-cg';\n",
    "- 'lbfgs';\n",
    "- 'liblinear';\n",
    "- 'sag';\n",
    "- 'saga'.\n",
    "\n",
    "Последние два являются вариациями стохастического градиентного спуска (а значит вам уже понятен принцип их работы), а с первыми тремя нам только предстоит познакомиться. В этом юните мы рассмотрим алгоритм 'newton-cg', в следующем — 'lbfgs', а в седьмом юните — 'liblinear'. Вы будете понимать суть всех методов, представленных в самой популярной библиотеке для машинного обучения, и выбирать подходящий, исходя из особенностей поставленной задачи.\n",
    "\n",
    "Начнём с метода Ньютона. Этот алгоритм работает быстрее, чем градиентный спуск, и тратит меньше времени для достижения минимума, однако у него есть и определённые недостатки, о которых мы поговорим позже.\n",
    "\n",
    "Метод Ньютона изначально появился как метод решения уравнений вида $f(x) = 0$.\n",
    "\n",
    "Проиллюстрируем принцип его работы геометрически. Пусть у нас есть график некоторой функции. Проведём к нему касательную в точке $x_n$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_3_1.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда эта касательная имеет наклон, равный $f'(x_n)$, и проходит через точку $x_n$, $f(x_n)$. В таком случае мы можем сказать, что уравнение этой касательной: $y = f'(x_n) (x - x_n) + f(x_n)$.\n",
    "\n",
    "Так как нам необходимо решить уравнение, то нужно попасть в такую точку $x_{n+1}$, чтобы в ней значение координаты по оси ординат было нулевым, то есть в точку с координатами $x =  x_{n+1}$ и $y = 0$.\n",
    "\n",
    "Подставим это в наше уравнение касательной:\n",
    "\n",
    "$y=f^{\\prime}\\left(x_{n}\\right)\\left(x-x_{n}\\right)+f\\left(x_{n}\\right)$  \n",
    "$x=x_{n+1}, y=0$  \n",
    "$0=f^{\\prime}\\left(x_{n}\\right) \\cdot\\left(x_{n+1}-x_{n}\\right)+f\\left(x_{n}\\right)$  \n",
    "$f^{\\prime}\\left(x_{n}\\right) \\cdot\\left(x_{n+1}-x_{n}\\right)=-f\\left(x_{n}\\right)$  \n",
    "$x_{n+1}-x_{n}=-\\frac{f\\left(x_{n}\\right)}{f^{\\prime}\\left(x_{n}\\right)}$  \n",
    "$x_{n+1}=x_{n}{-\\frac{f\\left(x_{n}\\right)}{f^{\\prime}\\left(x_{n}\\right)}}$  \n",
    "\n",
    "Можно посмотреть на это и в анимации:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_3_2.gif\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, как для $x$ вычисляется $f(x)$, строится касательная, и в точке пересечения касательной с осью $Ox$ строится новая точка, к которой также строится касательная, и так далее. Математически доказано, что таким образом $x_i$ приближается к значению, где $f(x) = 0$. \n",
    "\n",
    "Формально первый  шаг этого алгоритма мы можем записать следующим образом:\n",
    "\n",
    "$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$   \n",
    "\n",
    "Все остальные шаги можно обобщить с помощью следующей зависимости:\n",
    "\n",
    "$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$  \n",
    "\n",
    "Шаги могут повторяться сколько угодно раз до достижения необходимой точности.\n",
    "\n",
    "Давайте посмотрим, как с использованием этого метода можно решить уравнение ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 1**\n",
    "\n",
    "Найти корень уравнения $x^2 - 4x - 7 = 0$, который находится рядом с точкой  $x = 5$, с точностью до тысячных.\n",
    "\n",
    "Функция: $f(x) = x^2 - 4x - 7 = 0$  \n",
    "Начальная точка: $x_0 = 5$  \n",
    "Производная для функции: $f'(x) = 2x - 4$ \n",
    "\n",
    "Начнём поочерёдно совершать шаги и переходить в следующие точки, используя формулу, которую мы рассмотрели ранее:\n",
    "\n",
    "$x_1 = 5 - \\frac{5^2 - 4 \\times 5 - 7}{2 \\times 5 - 4} = 5 - \\frac{(-2)}{6}  = \\frac{16}{3} \\approx 5.33333$  \n",
    "$x_2 = \\frac{16}{3} - \\frac{(\\frac{16}{3})^2 - 4 (\\frac{16}{3}) - 7}{2 (\\frac{16}{3}) - 4} = \\frac{16}{3} - \\frac{\\frac{1}{9}}{\\frac{20}{3}} = \\frac{16}{3} - \\frac{1}{60} = \\frac{319}{60} \\approx 5.31667$  \n",
    "$x_3 = \\frac{319}{60} - \\frac{(\\frac{319}{60})^2 - 4 (\\frac{319}{60}) - 7}{2 (\\frac{319}{60}) - 4} = \\frac{319}{60} - \\frac{\\frac{1}{3600}}{\\frac{398}{60}} \\approx 5.31662$  \n",
    "\n",
    "Мы видим, что в последних двух точках мы уже находимся примерно в одном и том же месте. На всякий случай проверим ещё одну:\n",
    "\n",
    "$x_4 = 5.31662 - \\frac{(5.31662)^2 - 4 (5.31662) - 7}{2 (5.31662) - 4} = 5.31662$\n",
    "\n",
    "Действительно, мы нашли точку с точностью до тысячных, в которой останемся — это и будет нашим ответом."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 2**\n",
    "\n",
    "Найти корни сложного полинома $f(x) = 6x^5 - 5x^4 - 4x^3 + 3x^2$.\n",
    "\n",
    "Как мы знаем, к сожалению, для полинома пятой степени нет формулы поиска корней, поэтому будем использовать численные методы. В этих случаях приходится прибегать к числовому линейному приближению.\n",
    "\n",
    "Ниже представлен график нашего полинома. У него три корня: в точках 0, 1 и где-то между ними. Как найти третий корень?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_3_3.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В методе Ньютона мы берём случайную точку $x_0$, а затем проводим касательную в ней. Точка $x_1$, где эта касательная пересекает ось абсцисс, станет нашим следующим предположением. Так что теперь мы  строим уже касательную в этой точке, и так далее . Мы продолжаем до тех пор, пока не достигнем необходимой точности. В целом, мы можем сделать приближение настолько близким к нулю, насколько хотим."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.1**\n",
    "\n",
    "Найдите третий корень полинома $f(x) = 6x^5 - 5x^4 - 4x^3 + 3x^2$, взяв за точку старта 0.7. Введите получившееся значение с точностью до трёх знаков после точки-разделителя.\n",
    "\n",
    "Попробуйте реализовать алгоритм с использованием Python на основе алгоритма градиентного спуска, изученного в предыдущем модуле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6286669787764609\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def func1(x):\n",
    "    return 6*x**5-5*x**4-4*x**3+3*x**2\n",
    "\n",
    "def func2(x):\n",
    "    return 30*x**4-20*x**3-12*x**2+6*x\n",
    "\n",
    "init_value = 0.7\n",
    "iter_count = 0\n",
    "x_curr = init_value\n",
    "epsilon = 0.000001\n",
    "f = func1(x_curr)\n",
    "\n",
    "while (abs(f) > epsilon):\n",
    "    f = func1(x_curr)\n",
    "    f_prime = func2(x_curr)\n",
    "    x_curr = x_curr - (f)/(f_prime)\n",
    "    iter_count += 1\n",
    "print(x_curr)\n",
    "print(iter_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Отлично, мы научились искать приближённые значения для корней уравнения. Но как же это поможет нам найти минимум или максимум для функции?\n",
    "\n",
    "Дело в том, что в задаче оптимизации можно решать не $f(x) = 0$, а $f'(x) = 0$ — тогда мы найдём потенциальные точки экстремума.\n",
    "\n",
    "В многомерном случае по аналогичным рассуждениям производная превращается в градиент, а вторая производная превращается в гессиан (матрица вторых производных или, как мы её называли в предыдущем модуле, матрица Гессе). Поэтому в формуле появится обратная матрица.\n",
    "\n",
    "Для многомерного случая формула выглядит следующим образом:\n",
    "\n",
    "$x^{(n+1)} = x^{(n)} - \\left [Hf(x^{(n)})  \\right ]^{-1} \\nabla f(x^{(n)})$  \n",
    "\n",
    "Можно заметить, что эта формула совпадает с формулой для градиентного спуска, но вместо умножения на learning rate (темп обучения) используется умножение на обратную матрицу к гессиану. Благодаря этому функция может сходиться за меньшее количество итераций, так как мы учитываем информацию о выпуклости функции через гессиан. Можно увидеть это на иллюстрации работы двух методов, где метод Ньютона явно сходится быстрее:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_3_4.png\" alt=\"drawing\" width=\"900\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод Ньютона, если считать в количестве итераций, в многомерном случае (с гессианом) работает быстрее градиентного спуска."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше мы уже разобрали применение метода Ньютона для поиска корней уравнения. Теперь давайте снова используем его, но уже для оптимизации функции ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Оптимизировать функцию $f(x) = x^3 - 3x^2 -45x  + 40$.\n",
    "\n",
    "Находим производную функции:\n",
    "\n",
    "$f^{\\prime}=3 x^{2} - 6x - 45$\n",
    "\n",
    "Находим вторую производную:\n",
    "\n",
    "$f^{\\prime \\prime}=6x - 6$\n",
    "\n",
    "Сразу определим их в Python, чтобы можно было параллельно решить задачу и с помощью программирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x):\n",
    "    return 3*x**2 - 6*x -45\n",
    "def func2(x):\n",
    "    return 6*x - 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь необходимо взять какую-нибудь изначальную точку. Например, пусть это будет точка x = 42. Также нам необходима точность — её возьмем равной 0.0001. На каждом шаге будем переходить в следующую точку по уже упомянутой выше формуле:\n",
    "\n",
    "$x^{(n+1)}=x^{(n)}-\\frac{f^{\\prime}\\left(x^{(n)}\\right)}{f^{\\prime \\prime}\\left(x^{(n)}\\right)}$\n",
    "\n",
    "Например, в нашем случае следующая после 42 точка будет рассчитываться следующим образом:\n",
    "\n",
    "$x_{2}=42-\\frac{f^{\\prime}\\left(x_{1}\\right)}{f^{\\prime \\prime}\\left(x_{2}\\right)}$  \n",
    "$f^{\\prime}\\left(x_{1}\\right)=3 x^{2}-6 x-45 \\mid _{x_{1}=42}\\;= 3 \\cdot 42^{2}-6 \\cdot 42-45=4995$  \n",
    "$f^{\\prime \\prime}\\left(x_{1}\\right)=6 x-6 \\mid _{x_{1}=42} \\; = \\;6.42-6=246$  \n",
    "$x_{2}=42-\\frac{4995}{246} \\approx 42-20.305=21.695$\n",
    "\n",
    "Третья точка будет вычисляться по аналогичному принципу:\n",
    "\n",
    "$x_3 = 21.695 - \\frac{f'(21.695)}{f''(21.695)}$\n",
    "\n",
    "Но, к счастью, нам совсем не обязательно высчитывать всё вручную — воспользуемся Python и распишем наш алгоритм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.695121951219512\n",
      "11.734125501243229\n",
      "7.1123493600499685\n",
      "5.365000391507974\n",
      "5.015260627016227\n",
      "5.000029000201801\n",
      "5.000000000105126\n",
      "5.000000000000001\n"
     ]
    }
   ],
   "source": [
    "initial_value = 42\n",
    "iter_count = 0\n",
    "x_curr = initial_value\n",
    "epsilon = 0.0001\n",
    "f = func1(x_curr)\n",
    "\n",
    "while (abs(f) > epsilon):\n",
    "    f = func1(x_curr)\n",
    "    f_prime = func2(x_curr)\n",
    "    x_curr = x_curr - (f)/(f_prime)\n",
    "    iter_count += 1\n",
    "    print(x_curr)\n",
    "\n",
    "#21.695121951219512\n",
    "#11.734125501243229\n",
    "#7.1123493600499685\n",
    "#5.365000391507974\n",
    "#5.015260627016227\n",
    "#5.000029000201801\n",
    "#5.000000000105126\n",
    "#5.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно объединить всё в одну функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def newtons_method(f, der, eps, init):\n",
    "    iter_count = 0\n",
    "    x_curr = init\n",
    "    f = f(x_curr)\n",
    "    while (abs(f) > eps):\n",
    "        f = f(x_curr)\n",
    "        f_der = der(x_curr)\n",
    "        x_curr = x_curr - (f)/(f_prime)\n",
    "        iter_count += 1\n",
    "    return x_curr\n",
    " \n",
    "from scipy.optimize import newton\n",
    "newton(func=func1,x0=50,fprime=func2, tol=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У метода Ньютона есть **ряд достоинств и недостатков**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\happy-icon.png\" alt=\"drawing\" width=\"70\"/>\n",
    "\n",
    "- Если мы минимизируем квадратичную функцию, то с помощью метода Ньютона можно попасть в минимум целевой функции за один шаг.\n",
    "\n",
    "- Также этот алгоритм сходится за один шаг, если в качестве минимизируемой функции выступает функция из класса поверхностей вращения (т. е. такая, у которой есть симметрия).\n",
    "\n",
    "- Для несимметричной функции метод не может обеспечить сходимость, однако скорость сходимости  всё равно превышает скорость методов, основанных на градиентном спуске."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\sad-icon.png\" alt=\"drawing\" width=\"70\"/>\n",
    "\n",
    "- Этот метод очень чувствителен к изначальным условиям.\n",
    "\n",
    "При использовании градиентного спуска мы всегда гарантированно движемся по антиградиенту в сторону минимума. В методе Ньютона происходит подгонка параболоида к локальной кривизне, и затем алгоритм движется к неподвижной точке данного параболоида. Из-за этого мы можем попасть в максимум или седловую точку. Особенно ярко это видно на невыпуклых функциях с большим количеством переменных, так как у таких функций седловые точки встречаются намного чаще экстремумов.\n",
    "\n",
    "Поэтому здесь необходимо обозначить ограничение: метод Ньютона стоит применять только для задач, в которых целевая функция выпуклая.\n",
    "\n",
    "Впрочем, это не является проблемой. В линейной регрессии или при решении задачи классификации с помощью метода опорных векторов или логистической регрессии мы как раз ищем минимум у выпуклой целевой функции, то есть данный алгоритм подходит нам во многих случаях.\n",
    "\n",
    "\n",
    "- Также метод Ньютона может быть затратным с точки зрения вычислительной сложности, так как требует вычисления не только градиента, но и гессиана и обратного гессиана (при делении на матрицу необходимо искать обратную матрицу).\n",
    "\n",
    "Если у задачи много параметров, то расходы на память и время вычислений становятся астрономическими. Например, при наличии 50 параметров нужно вычислять более 1000 значений на каждом шаге, а затем предстоит ещё более 500 операций нахождения обратной матрицы. Однако метод всё равно используют, так как выгода от быстрой сходимости перевешивает затраты на вычисления."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на его ограниченное практическое применение, метод Ньютона по-прежнему представляет большую ценность. Он имеет большое преимущество перед градиентным спуском в силу своей быстроты и отсутствия необходимости в настройке гиперпараметра шага (мы помним, что в градиентном спуске выбор шага — довольно непростая задача, а здесь можно обойтись без этого). Причём преимущество в быстроте очень ощутимое: в сравнении на реальных данных метод Ньютона находит решение задачи за 3 итерации, а градиентный спуск — за 489. То есть мы сильно выигрываем в скорости сходимости, а для анализа данных это очень важно, ведь экономия времени и вычислительных ресурсов позволяет решать задачи быстрее.\n",
    "\n",
    "? Мы увидели, какой эффективной может быть оптимизация второго порядка при правильном использовании. Но что, если бы мы могли каким-то образом использовать эффективность, полученную при использовании производных второго порядка, но при этом избежать вычислительных затрат на вычисление обратного гессиана? Другими словами — можем ли мы создать алгоритм, который будет своего рода гибридом между градиентным спуском и методом Ньютона, где мы сможем получать более быструю сходимость, чем градиентный спуск, но меньшие вычислительные затраты на каждую итерацию, чем в методе Ньютона?\n",
    "\n",
    "Оказывается, такой алгоритм существует. Точнее, целый класс таких методов оптимизации, называемых **квазиньютоновскими методами**. Мы познакомимся с ними уже в следующем юните, но для начала давайте закрепим пройденный материал ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.6**\n",
    "\n",
    "Дана функция $f(x) = x^3 - 72x - 220$. Найдите решение уравнения $f(x) = 0$ для поиска корня в окрестностях точки $x_0 = 12$. Ответ округлите до трёх знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.727134419408875\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "def func1(x):\n",
    "    return x**3-72*x - 220\n",
    "\n",
    "def func2(x):\n",
    "    return 3*x**2-72\n",
    "\n",
    "init_value = 12\n",
    "iter_count = 0\n",
    "x_curr = init_value\n",
    "epsilon = 0.000001\n",
    "f = func1(x_curr)\n",
    "\n",
    "while (abs(f) > epsilon):\n",
    "    f = func1(x_curr)\n",
    "    f_prime = func2(x_curr)\n",
    "    x_curr = x_curr - (f)/(f_prime)\n",
    "    iter_count += 1\n",
    "print(x_curr)\n",
    "print(iter_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} =\n",
    "= 12 - \\frac{(12)^3 - 72 \\times 12 - 220}{3 \\times (12)^2 - 72} = 12 - \\frac{644}{390} = \\frac{919}{90} \\approx 10.211$\n",
    "$\\cdots$\n",
    "$x_3 = x_2 - \\frac{f(x_2)}{f'(x_2)} \\approx 9.727$\n",
    "$x_4 \\approx 9.727$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.7**\n",
    "\n",
    "Найдите положительный корень для уравнения $x^2 + 9x - 5 = 0$.\n",
    "\n",
    "В качестве стартовой точки возьмите $x_0 = 2.2$.\n",
    "\n",
    "Расчёт произведите поэтапно или с помощью Python.\n",
    "\n",
    "Ответ округлите до двух знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5249378105604451\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "def func1(x):\n",
    "    return x**2+9*x-5\n",
    "\n",
    "def func2(x):\n",
    "    return 2*x+9\n",
    "\n",
    "init_value = 2.2\n",
    "iter_count = 0\n",
    "x_curr = init_value\n",
    "epsilon = 0.000001\n",
    "f = func1(x_curr)\n",
    "\n",
    "while (abs(f) > epsilon):\n",
    "    f = func1(x_curr)\n",
    "    f_prime = func2(x_curr)\n",
    "    x_curr = x_curr - (f)/(f_prime)\n",
    "    iter_count += 1\n",
    "print(x_curr)\n",
    "print(iter_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} =$  \n",
    "$= x_n - \\frac{x^2_n + 9x_n - 5}{2x_n + 9} = \\frac{x^2_n + 5}{2x_n + 9}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.9**\n",
    "\n",
    "С помощью метода Ньютона найдите точку минимума для функции $f(x) = 8x^3 - 2x^2 - 450$.\n",
    "\n",
    "Для расчётов используйте Python.\n",
    "\n",
    "В качестве стартовой точки возьмите 42, точность примите за 0.0001.\n",
    "\n",
    "Ответ округлите до трёх знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16666666666666785\n",
      "0.167\n",
      "-450.019\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return 8*x**3-2*x**2-450\n",
    "def func1(x):\n",
    "    return 24*x**2 - 4*x\n",
    "def func2(x):\n",
    "    return 48*x -4\n",
    "init_value = 42\n",
    "iter_count = 0\n",
    "x_curr = init_value\n",
    "epsilon = 0.0001\n",
    "f = func1(x_curr)\n",
    "while (abs(f) > epsilon):\n",
    "    f = func1(x_curr)\n",
    "    f_prime = func2(x_curr)\n",
    "    x_curr = x_curr - (f)/(f_prime)\n",
    "    iter_count += 1\n",
    "print(x_curr)\n",
    "print(round(x_curr, 3))\n",
    "print(round(func(x_curr),3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Квазиньютоновские методы"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В предыдущем юните мы рассмотрели метод Ньютона. В отличие от градиентного спуска, метод Ньютона использует на каждой итерации не только градиент, но и матрицу Гессе. Это обеспечивает более быструю сходимость к минимуму, но в то же время это приводит к слишком большим вычислительным затратам. В данном юните мы рассмотрим класс методов, в которых решается  эта проблема, — **класс квазиньютоновских методов**.\n",
    "\n",
    "Напомним, что в методе Ньютона мы обновляем точку на каждой итерации в соответствии со следующим правилом:\n",
    "\n",
    "$\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\left[H\\left(\\mathbf{x}_{k}\\right)\\right]^{-1} \\nabla f\\left(\\mathbf{x}_{k}\\right)$\n",
    "\n",
    "На каждом шаге здесь вычисляется гессиан, а также его обратная матрица.\n",
    "\n",
    "В квазиньютоновских методах вместо вычисления гессиана мы просто аппроксимируем его матрицей, которая обновляется от итерации к итерации с использованием информации, вычисленной на предыдущих шагах. Так как вместо вычисления большого количества новых величин мы использует найденные ранее значения, квазиньютоновский алгоритм тратит гораздо меньше времени и вычислительных ресурсов.\n",
    "\n",
    "Формально это описывается следующим образом:\n",
    "\n",
    "$x_{k+1} = x_k - H_k \\nabla f(x_k)$\n",
    "\n",
    "В данном случае вместо обратного гессиана появляется матрица $H_k$, которая строится таким образом, чтобы максимально точно аппроксимировать настоящий обратный гессиан.\n",
    "\n",
    "Математически это записывается так:\n",
    "\n",
    "$H_k - \\left [\\nabla^2 f(x_k) \\right ]^{-1} \\to 0 \\ при \\ k \\to \\infty$\n",
    "\n",
    "Здесь имеется в виду, что разница между матрицей в квазиньютоновском методе и обратным гессианом стремится к нулю.\n",
    "\n",
    "Эта матрица обновляется на каждом шаге, и для этого существуют разные способы. Для каждого из способов есть своя модификация квазиньютоновского метода. Эти способы объединены **ограничением**: процесс обновления матрицы должен быть достаточно эффективным и не должен требовать вычислений гессиана. То есть, по сути, на каждом шаге мы должны получать информацию о гессиане, не находя непосредственно сам гессиан.\n",
    "\n",
    "Если вас интересует математическая сторона обновления и аппроксимации матрицы, прочитайте эту статью. В силу того, что понимание этой части метода требует очень серьёзной математической подготовки, мы опустим её. Однако можем заверить вас, что для успешного использования алгоритма и его понимания знание всех математических выводов не требуется.\n",
    "\n",
    "**Три самые популярные схемы аппроксимации:**\n",
    "\n",
    "- симметричная коррекция ранга 1 (SR1);\n",
    "- схема Дэвидона — Флетчера — Пауэлла (DFP);\n",
    "- схема Бройдена — Флетчера — Гольдфарба — Шанно (BFGS).\n",
    "\n",
    "Последняя схема (**BFGS**) самая известная, стабильная и считается наиболее эффективной. На ней мы и остановимся. Своё название она получила из первых букв фамилий создателей и исследователей данной схемы: Чарли Джорджа Бройдена, Роджера Флетчера, Дональда Гольдфарба и Дэвида Шанно.\n",
    "\n",
    "У этой схемы есть две известных вариации:\n",
    "\n",
    "- L-BFGS;\n",
    "- L-BFGS-B.\n",
    "\n",
    "Обе этих вариации необходимы в случае большого количества переменных для экономии памяти (так как во время их реализации хранится ограниченное количество информации). По сути, они работают одинаково, и L-BFGS-B является лишь улучшенной версией L-BFGS для работы с ограничениями.\n",
    "\n",
    "Метод BFGS очень устойчив и на данный момент считается одним из наиболее эффективных. Поэтому, если, например, применить функцию optimize без указания метода в библиотеке SciPy, то по умолчанию будет использоваться именно BFGS либо одна из его модификаций, указанных выше. Также данный метод используется в библиотеке sklearn при решении задачи логистической регрессии."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим **алгоритм применения этого метода**. Постарайтесь понять последовательность действий и основной принцип.\n",
    "\n",
    "1. Реализация алгоритма начинается с того, что мы задаём начальную точку $x_0$, выбираем точность алгоритма, а также изначальную аппроксимацию для обратного гессиана функции. \n",
    "\n",
    "Здесь как раз таится главная проблема: нет никакого универсального рецепта для выбора этого приближения. Можно поступить по-разному: использовать гессиан, вычисленный в изначальной точке, взять единичную матрицу (такая настройка стоит по умолчанию в некоторых функциях библиотек Python) или другую матрицу, если она невырождена и хорошо обусловлена.\n",
    "\n",
    "2. Когда мы определили, откуда будем начинать, необходимо понять, как попасть в следующую точку. Поэтому на втором шаге мы вычисляем направление для поиска следующей точки:\n",
    "\n",
    "$p_k = -H_k * \\nabla f_k$\n",
    "\n",
    "3. Далее находим следующую точку, используя соотношение:\n",
    "\n",
    "$x_{k+1} = x_k + k * p_k$\n",
    "\n",
    "Здесь важным вопросом является нахождение коэффициента $k$, который регулирует шаг. Его подбирают линейным поиском в соответствии с условиями, о которых можно подробно прочитать здесь.\n",
    "\n",
    "Нам важно лишь понимать суть: мы находим такое значение k, при котором получим минимальное значение функции $f(x_k + k * p_k)$ (так как мы хотим попасть в минимум). Также важно отметить, что в расчёте коэффициента $k$ участвуют две константы $0 \\leq c_1 \\leq c_2 \\leq 1$. Обычно в качестве их значений берут 0.001 и 0.9 (это считается хорошей эвристикой, показывающей высокие результаты).\n",
    "\n",
    "4. На следующем шаге необходимо определить два следующих вектора:\n",
    "\n",
    "$s_{k}=x_{k+1}-x_{k}$  \n",
    "$y_{k}=\\nabla f_{k+1}-\\nabla f_{k}$  \n",
    "\n",
    "Здесь $s_k$ — шаг алгоритма, a $y_k$ — изменение градиента на данной итерации. Они не требовались для перехода в следующую точку и нужны строго для того, чтобы найти следующее приближение обратной матрицы гессиана.\n",
    "\n",
    "5. После того как мы их нашли, обновляем гессиан, руководствуясь следующей формулой:\n",
    "\n",
    "$H_{k+1}=\\left(I-k * s_{k} * y_{k}^{T}\\right) H_{k}\\left(I-k * y_{k} * s_{k}^{T}\\right)+* s_{k} * s_{k}^{T}$\n",
    "\n",
    "Не стоит её пугаться. Как уже было сказано выше, эта формула —  результат очень серьёзных и длительных математических исследований. Её не требуется знать наизусть или уметь реализовывать вручную. Однако для полноты повествования мы не можем не привести её.\n",
    "\n",
    "В данной формуле за I обозначена единичная матрица, $s_k$ и $y_k$ мы вычислили на предыдущем шаге, а $k$ вычисляется следующим образом:\n",
    "\n",
    "$k=\\frac{1}{y_{k}^{T} s_{k}}$\n",
    "\n",
    "Алгоритм довольно сложный, поэтому давайте рассмотрим **пример** ↓\n",
    "\n",
    "Сразу оговоримся, что несколько шагов в этом алгоритме мы приведём без ручных расчётов (например, нахождение следующей аппроксимации гессиана) в силу их высокой сложности. Постарайтесь сильнее всего сконцентрироваться на шагах решения и понять логику работы алгоритма и последовательность действий. Мы начнём реализовывать алгоритм «вручную», а затем вы сможете самостоятельно завершить решение задачи с использованием Python (разумеется, далее будет указано, как это сделать). К сожалению, серьёзные и эффективные методы настолько сложны, что решать с их помощью задачи, используя только лист бумаги, ручку и калькулятор (как мы могли это делать, например, с методом Лагранжа), уже невозможно.\n",
    "\n",
    "Будем искать экстремум для функции следующего вида:\n",
    "\n",
    "$f(x, y)=x^{2}-x y+y^{2}+9 x-6 y+20$\n",
    "\n",
    "В качестве начальной точки выберем следующую:\n",
    "\n",
    "$x_0 = (1,1)$\n",
    "\n",
    "Находим градиент для нашей функции:\n",
    "\n",
    "$\\begin{aligned} f_{x}^{\\prime} &=\\left(x^{2}-x y+y^{2}+9 x-6 y+20\\right)^{\\prime}_{ x}=\\\\ &=2 x-y+0+9-0+0=2 x-y+9 \\\\ f^{\\prime}_{ y} &=\\left(x^{2}-x y+y^{2}+9 x-6 y+20\\right)^{\\prime} _{y}=\\\\ &=0-x+2 y+0-6+0=-x+2 y-6 \\end{aligned}\n",
    "\\nabla f=\\left(\\begin{array}{c}2 x-y+9 \\\\ -x+2 y-6\\end{array}\\right)$\n",
    "\n",
    "Начинаем первую итерацию с точки x_0 = (1,1). Вычисляем для неё градиент:\n",
    "\n",
    "$\\nabla f=\\left(\\begin{array}{c}2 x-y+9 \\\\ -x+2 y-6\\end{array}\\right) \\quad=\\left(\\begin{array}{c}2 \\cdot 1-1+9 \\\\ -1 \\times 2 \\cdot 1-6\\end{array}\\right)=\\left(\\begin{array}{c}10 \\\\ -5\\end{array}\\right)$\n",
    "\n",
    "Теперь необходимо выяснить, стоит ли заканчивать поиск (ведь, возможно, мы уже в минимуме). Для этого находим длину вектора градиента:\n",
    "\n",
    "$\\left|\\nabla f(x_0) \\right| = \\sqrt{10^2 + (-5)^2} = 11.18$\n",
    "\n",
    "Сравниваем полученный результат с точностью, которая нам необходима. Допустим, мы хотим достигнуть точности 0.001:\n",
    "\n",
    "$\\left|\\nabla f(x_0) \\right| = 11.18 > 0.001$\n",
    "\n",
    "Итак, точность не достигнута, так как градиент в экстремуме должен быть равен или очень близок к нулю. Это значит, что надо искать дальше.\n",
    "\n",
    "Теперь необходимо определить, в каком направлении искать нужную точку. Для этого выполняем следующие вычисления, согласно нашему алгоритму:\n",
    "\n",
    "$p_0 = -H_0 * \\nabla f(x_0) = - \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ \\end{pmatrix} \\begin{pmatrix} 10 \\\\ -5 \\end{pmatrix} = \\begin{pmatrix} -10 \\\\ 5 \\end{pmatrix}$  \n",
    "$x_0 + \\alpha_0 * p_0 = (1,1) + \\alpha_0 (10,-5) = (1 - 10 \\alpha_0, 1 + 5 \\alpha_0)$  \n",
    "$f(\\alpha_0) = (1 - 10 \\alpha_0)^2 - (1 - 10 \\alpha_0) (1 + 5 \\alpha_0) + (1 + 5 \\alpha_0)^2 + 9 (1 - 10 \\alpha_0) - 6 (1 + 5 \\alpha_0) + 20$\n",
    "\n",
    "Упрощаем выражение:\n",
    "\n",
    "$1-20 a_{0}+100 a_{0}^{2}-1+10 a_{0}-5 a_{0}+50 a_{0}^{2}+1+10 a_{0}+25 a_{0}^{2}+9-90 a_{0}-6-30 a_{0}+20= 175 a_{0}^{2}-125 a_{0}+24$\n",
    "\n",
    "Находим производную от результата:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial a_0}=350a_{0}-125=0 \\Rightarrow a_{0}=0.357$\n",
    "\n",
    "Теперь мы можем найти следующую точку:\n",
    "\n",
    "$x_{1}=x_{0}+{a }_{0} * p_{0}=(-2.571,2.786)$  \n",
    "$s_{0}=x_{1}-x_{0}=(-2.571,2.786)-(1,1)=(-3.571,1.786)$\n",
    "\n",
    "Вычисляем значение градиента в следующей найденной точке, т. е. в $x_1$:\n",
    "\n",
    "$\\nabla f\\left(x_{1}\\right)=\\left(\\begin{array}{l}1.071 \\\\ 2.143\\end{array}\\right)$  \n",
    "$y_{0}=\\nabla f\\left(x_{1}\\right)-\\nabla f\\left(x_{0}\\right)=(1.071,2.143)-(10,-5)=(-8.929,7.143)$  \n",
    "\n",
    "Находим приближение гессиана:\n",
    "\n",
    "$H_{1}=\\left(\\begin{array}{ll}0.694 & 0.367 \\\\ 0.367 & 0.709\\end{array}\\right)$\n",
    "\n",
    "Проверяем, стоит ли остановиться в этой точке:\n",
    "\n",
    "$\\left|\\nabla f(x_1) \\right| = 2.396 > 0.001$\n",
    "\n",
    "К сожалению, необходимая точность всё ещё не достигнута.\n",
    "\n",
    "Перечисленные выше шаги стоит повторять до тех пор, пока не будет достигнуто значение, близкое к нулю (меньшее, чем изначально заданная точность).\n",
    "\n",
    "Разумеется, при решении прикладных задач не понадобится делать ничего подобного, ведь мы умеем программировать и знаем, что в библиотеках Python есть все необходимые методы.\n",
    "\n",
    "Давайте рассмотрим, как с помощью функций Python мы сможем применить квазиньютоновские методы для оптимизации функции $f(x,y) = x^2 + y^2$.\n",
    "\n",
    "Подгрузим необходимые библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, которую будем оптимизировать. Вместо отдельных x и y можно взять координаты единого вектора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x[0]**2.0 + x[1]**2.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь определим градиент для функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_func(x):\n",
    "    return np.array([x[0] * 2, x[1] * 2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим начальную точку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = [1.0, 1.0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим алгоритм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 3\n",
      "Решение: f([0. 0.]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))\n",
    " \n",
    "#Статус оптимизации Optimization terminated successfully.\n",
    "#Количество оценок: 3\n",
    "#Решение: f([0. 0.]) = 0.00000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили, что минимум функции достигается в точке (0,0). Значение функции в этой точке также равно нулю.\n",
    "\n",
    "Можно повторить то же самое с вариацией  L-BFGS-B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "Количество оценок: 3\n",
      "Решение: f([0. 0.]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**2.0 + x[1]**2.0\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([x[0] * 2, x[1] * 2])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [1, 1]\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Иногда количество итераций у двух модификаций различается, но ответ совпадает. Бывает также, что одна из вариаций может не сойтись, а другая — достичь экстремума, поэтому советуем не воспринимать их как взаимозаменяемые алгоритмы. На практике лучше пробовать разные варианты: если у вас не сошёлся алгоритм BFGS, можно попробовать L-BFGS-B, и наоборот. Также можно экспериментировать одновременно с обоими алгоритмами, чтобы выбрать тот, который будет сходиться для функции за меньшее число итераций и тем самым экономить время."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Важно понимать, что для некоторых функций не из всех стартовых точек получается достичь сходимости метода. Тогда их можно перебирать, к примеру, с помощью цикла.\n",
    "\n",
    "✍ Итак, мы обсудили один из самых эффективных на сегодняшний день алгоритмов — вариацию BFGS квазиньютоновских методов. Вы будете регулярно сталкиваться с этим алгоритмом при решении различных задач и при использовании библиотек для оптимизации. Так что давайте попрактикуемся: в этом юните мы посмотрели фрагмент поэтапного разбора метода BFGS для функции $f(x,y) = x^2 - xy + y^2 + 9x - 6y + 20$ — давайте завершим начатое и найдём точку минимума ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.1**\n",
    "\n",
    "Найдите точку минимума для функции $f(x,y) = x^2 - xy + y^2 + 9x - 6y + 20$.\n",
    "\n",
    "В качестве стартовой возьмите точку (-400, -400).\n",
    "\n",
    "Значения координат округлите до целого числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.99999852  1.00000155]\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return x[0] ** 4.0 - x[0] * x[1] + x[1] ** 2 + 9 * x[0] - 6 * x[1] + 20\n",
    "def grad_func(x):\n",
    "    return np.array([2 * x[0] - x[1] + 9, -x[0] + 2 * x[1] - 6])\n",
    "\n",
    "x_0 = [-400, -400]\n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "solution = result['x']\n",
    "print (solution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.4**\n",
    "\n",
    "Найдите минимум функции $f(x) = x^2 - 3x + 45$ с помощью квазиньютоновского метода BFGS.\n",
    "\n",
    "В качестве стартовой точки возьмите $x = 10$.\n",
    "\n",
    "В качестве ответа введите минимальное значение функции в достигнутой точке.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 5\n",
      "Решение: f([1.5]) = 42.75000\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "def func(x):\n",
    "    return x[0]**2.0 - 3*x[0] + 45\n",
    "def grad_func(x):\n",
    "    return 2*x[0]-3\n",
    "x_0 = 10\n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.5**\n",
    "\n",
    "Решите предыдущую задачу, применяя модификацию L-BFGS-B.\n",
    "\n",
    "В каком случае получилось меньше итераций?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "Количество оценок: 3\n",
      "Решение: f([1.5]) = 42.75000\n"
     ]
    }
   ],
   "source": [
    "x_0 = 10\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.7**\n",
    "\n",
    "Найдите минимум функции $f(x,y) = x^4 + 6*y^2 + 10$, взяв за стартовую точку (100,100).\n",
    "\n",
    "Какой алгоритм сошелся быстрее?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 37\n",
      "Решение: f([1.31617159e-02 6.65344582e-14]) = 10.00000\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return x[0]**4.0 + 6*x[1]**2.0 + 10\n",
    "def grad_func(x):\n",
    "    return np.array([4* x[0] ** 3, 12* x[1]])\n",
    "x_0 = [100.0, 100.0]\n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "Количество оценок: 40\n",
      "Решение: f([-9.52718297e-03 -2.32170510e-06]) = 10.00000\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return x[0]**4.0 + 6*x[1]**2.0 + 10\n",
    "def grad_func(x):\n",
    "    return np.array([4* x[0] ** 3, 12* x[1]])\n",
    "x_0 = [100.0, 100.0]\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Линейное программирование"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В предыдущем модуле мы уже немного говорили о задачах условной оптимизации. Теперь пришло время вернуться к ним и рассмотреть их более основательно. Мы разберём один из наиболее часто встречающихся случаев задачи условной оптимизации — **задачу линейного программирования**.\n",
    "\n",
    "**Линейное программирование** — это метод оптимизации для системы линейных ограничений и линейной целевой функции. Целевая функция определяет оптимизируемую величину, и цель линейного программирования состоит в том, чтобы найти значения переменных, которые максимизируют или минимизируют целевую функцию.\n",
    "\n",
    "Линейное программирование полезно применять для многих задач, требующих оптимизации ресурсов:\n",
    "\n",
    "- В производстве — чтобы рассчитать человеческие и технические ресурсы и минимизировать стоимость итоговой продукции.\n",
    "- При составлении бизнес-планов — чтобы решить, какие продукты продавать и в каком количестве, чтобы максимизировать прибыль.\n",
    "- В логистике — чтобы определить, как использовать транспортные ресурсы для выполнения заказов за минимальное время.\n",
    "- В сфере общепита — чтобы составить расписание для официантов.\n",
    "\n",
    "Задача линейного программирования — это задача оптимизации, в которой целевая функция и функции-ограничения линейны, а все переменные неотрицательны.\n",
    "\n",
    "Есть и разновидность задачи линейного программирования, в которой мы знаем, что все переменные могут принимать только целочисленные значения. Например, если переменные в нашей задаче — это количество людей или произведённых на заводе изделий, то они, разумеется, не могут быть дробными.\n",
    "\n",
    "**Целочисленным линейным программированием (ЦЛП)** называется вариация задачи линейного программирования, когда все переменные — целые числа.\n",
    "\n",
    "Давайте рассмотрим алгоритм решения задачи линейного программирования на конкретном **примере** ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 1**\n",
    "\n",
    "Фабрика игрушек производит игрушки-антистресс и игрушки-вертушки.\n",
    "\n",
    "Для изготовления игрушки-антистресс необходимо потратить 2 доллара  и 3 часа, для изготовления игрушки-вертушки — 4 доллара и 2 часа.\n",
    "\n",
    "На этой неделе в бюджете у фабрики есть 220 долларов, и оплачено 150 трудочасов для производства указанных игрушек.\n",
    "\n",
    "Если одну игрушку-антистресс можно продать за 6 долларов, а игрушку-вертушку — за 7, то сколько экземпляров каждого товара необходимо произвести на этой неделе, чтобы максимизировать прибыль?\n",
    "\n",
    "Такая задача идеально подходит для использования методов линейного программирования по следующим причинам:\n",
    "\n",
    "- все условия являются линейными;\n",
    "- значения переменных каким-то образом ограничены;\n",
    "- цель состоит в том, чтобы найти значения переменных, которые максимизируют некоторую величину.\n",
    "\n",
    "Обратите внимание, что производство каждой детали связано с затратами, как временными, так и финансовыми. Изготовление каждой игрушки-антистресс стоит 2 доллара, а изготовление каждой вертушки — 4 доллара. У фабрики есть только 220 долларов, чтобы потратить их на производство изделий. Отсюда возникает ограничение на возможное количество изготовленных товаров.\n",
    "\n",
    "Обозначим за $x$ количество произведённых игрушек-антистресс, за $y$ — количество произведённых игрушек-вертушек Тогда это ограничение можно записать в виде следующего неравенства:\n",
    "\n",
    "$2 x+4 y \\leq 220$\n",
    "\n",
    "Также существует ограничение на то, сколько времени мы можем потратить на производство игрушек. На изготовление каждой игрушки-антистресс уходит 3 часа, а на изготовление каждой игрушки-вертушки — 2 часа. На этой неделе у фабрики есть только 150 рабочих часов, так что производство ограничено по времени. Это ограничение можно записать в виде неравенства:\n",
    "\n",
    "$3 x+2 y \\leq 150$\n",
    "\n",
    "К этим ограничениям мы также можем добавить ограничения из соображений здравого смысла. Невозможно произвести отрицательное количество игрушек, поэтому необходимо обозначить следующие ограничения:\n",
    "\n",
    "$\\begin{aligned} &x \\geq 0 \\\\ &y \\geq 0 \\end{aligned}$\n",
    "\n",
    "Итак, мы записали все необходимые ограничения. Они образуют систему неравенств:\n",
    "\n",
    "$\\left\\{\\begin{aligned} 2 x+4 y & \\leq 220 \\\\ 3 x+2 y & \\leq 150 \\\\ x & \\geq 0 \\\\ y & \\geq 0  \\end{aligned}\\right.$\n",
    "\n",
    "Если представить систему этих неравенств в графическом виде, получим многоугольник:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_5_1.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закрашенная область является областью допустимых решений этой задачи. Наша цель — найти внутри этого многоугольника такую точку, которая даст наилучшее решение нашей задачи (максимизацию или минимизацию целевой функции).\n",
    "\n",
    "Итак, мы поняли, что нам нужно найти максимальное значение прибыли (целевой функции) для точек внутри области допустимых значений, однако самой целевой функции у нас пока нет. Давайте составим её. На изготовление каждой игрушки-антистресс требуется 2 доллара, а продать её можно за 6. Получается, что чистая прибыль от продажи составляет 4 доллара. Чтобы изготовить игрушку-вертушку, мы потратим 4 доллара, а продадим её за 7. Значит, чистая прибыль для вертушки составляет 3 доллара. Исходя из этого, получаем целевую функцию для суммарной прибыли:\n",
    "\n",
    "$p(x, y)=4 x+3 y$\n",
    "\n",
    "Нам необходимо найти максимально возможное значение этой функции с учётом того, что точка, в которой оно будет достигаться, должна удовлетворять условиям системы, которую мы написали ранее.\n",
    "\n",
    "Для того чтобы решить задачу, выразим одну переменную через другую (так удобнее строить графики линейной функции в стандартной системе координат):\n",
    "\n",
    "$y=-\\frac{4}{3} x+\\frac{P}{3}$\n",
    "\n",
    "На графике ниже наша линия изображена красным цветом. Она может двигаться вверх и вниз в зависимости от значения P (вы можете видеть три прямых — это три разных положения одной и той же прямой).\n",
    "\n",
    "Попробуем найти такую точку, для которой значение y будет наибольшим. Нас это интересует, так как мы хотим максимизировать значение $P$, а при максимально возможном $P$ прямая поднимется настолько высоко, насколько это возможно, и пересечение с точкой ординат тоже будет находиться максимально высоко."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_5_2.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линия, которая максимизирует точку пересечения c осью ординат, проходит через точку (20,45) — это точка пересечения первых двух ограничений. Все остальные прямые, которые проходят выше, не проходят через область допустимых решений. Все остальные «нижние» прямые проходят более чем через одну точку в допустимой области и не максимизируют пересечение прямой с осью ординат, так как находятся ниже.\n",
    "\n",
    "Таким образом, получаем, что завод должен произвести 20 игрушек-антистресс и 45 игрушек-вертушек. Это даст прибыль в размере 215 долларов:\n",
    "\n",
    "$20*4 + 45* 3 = 215$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 2**\n",
    "\n",
    "Фермер кормит своих коров специальной смесью. Для её изготовления он использует два вида корма: на 1 кг кукурузного корма приходится 100 г белка и 750 г крахмала, на 1 кг пшеничного — 150 г белка и 700 г крахмала.\n",
    "\n",
    "Каждой корове необходимо давать не более 7 кг корма в сутки, иначе у неё заболит живот и придётся тратить деньги на ветеринара. При этом, чтобы давать оптимально полезное и вкусное молоко, каждая корова должна ежедневно потреблять как минимум 650 г белка и 4000 г крахмала.\n",
    "\n",
    "Известно, что кукурузный корм стоит 0.4 доллара за 1 кг, а пшеничный — 0.45 долларов за 1 кг.\n",
    "\n",
    "Какая кормовая смесь будет минимизировать затраты и в то же время позволит получать качественное молоко?\n",
    "\n",
    "Обозначим за $c$ количество кукурузного корма в смеси, а за $w$ — количество пшеничного. Тогда ограничения можно выразить следующим образом:\n",
    "\n",
    "$\\left\\{\\begin{aligned} 0.1 c+0.15 w & \\geq 0.65 \\\\ 0.75 c+0.7 w & \\geq 4 \\\\ c+w & \\leq 7 \\\\ c & \\geq 0 \\\\ w & \\geq 0 \\end{aligned}\\right.$\n",
    "\n",
    "Минимизировать мы будем целевую функцию, отражающую затраты на корм и имеющую следующий вид:\n",
    "\n",
    "$f(c, w)=0.40 c+0.45 w$\n",
    "\n",
    "Снова изобразим условие задачи графически. Начнём с многоугольника области допустимых значений:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_5_3.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что нам необходимо найти точку, в которой пересекаются две прямые, обозначающие условия. Можно записать это следующим образом (точка пересечения — решение системы):\n",
    "\n",
    "$\\left\\{\\begin{array}{l} 0.1 c+0.15 w=0.65 \\\\ 0.75 c+0.7 w=4 \\end{array}\\right.$\n",
    "\n",
    "Найдя эту точку (для этого можно воспользоваться методом Гаусса или программированием), можно найти и итоговую стоимость смеси:\n",
    "\n",
    "$f(3.411,2.059)=\\$ 2.29$\n",
    "\n",
    "Отлично, мы научились решать задачи линейного программирования. Теперь давайте закрепим полученные знания ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 5.5**\n",
    "\n",
    "Магазин спортивных товаров получает прибыль в размере 6 долларов с каждого проданного футбольного мяча и 5.5 долларов — с бейсбольного.\n",
    "\n",
    "Каждый месяц магазин продаёт от 35 до 45 футбольных мячей и от 40 до 55 бейсбольных.\n",
    "\n",
    "Известно, что в этом месяце у магазина есть в общей сложности 80 мячей.\n",
    "\n",
    "Какую максимальную прибыль в этом месяце может получить магазин от продажи мячей?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\left\\{\\begin{aligned} 35 \\leq x & \\leq 45 \\\\ 40 \\leq y & \\leq 55 \\\\ x+y &=80 \\\\ \\end{aligned}\\right.$\n",
    "\n",
    "Максимизируем следующую функцию:\n",
    "\n",
    "$6 \\cdot x+5.5 \\cdot y$\n",
    "\n",
    "Получаем, что оптимальная точка: x =40 и y = 40.\n",
    "\n",
    "$6\\cdot 40 + 5.5 \\cdot 40 = 460$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 5.6**\n",
    "\n",
    "На текстильной фабрике есть 750 метров хлопка и 1000 метров полиэстера.\n",
    "\n",
    "Для изготовления свитшота требуется 1 метр хлопка и 2 метра полиэстера, для изготовления рубашки — 1.5 метра хлопка и 1 метр полиэстера.\n",
    "\n",
    "Свитшот можно продать за 30 евро, а рубашку — за 24 евро.\n",
    "\n",
    "Какое суммарное количество свитшотов и рубашек максимизирует возможную прибыль?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим за S количество произведённых свитшотов, а за C — количество произведённых рубашек. Запишем\n",
    "ограничения:\n",
    "\n",
    "$\\begin{aligned} &S \\geq 0 \\\\ &C \\geq 0 \\\\ &S+1.5 C \\leq 750 \\\\ &2 S+C \\leq 1000 \\end{aligned}$\n",
    "\n",
    "Максимизируем следующую функцию:\n",
    "$V(S, C)=30 S+24 C$\n",
    "\n",
    "<img src=\"data\\pic-2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "\n",
    "$V(0,0) = 0 \\ EUR$  \n",
    "$V(500,0) = 500 \\cdot 30 = 15000 \\ EUR$\n",
    "$V(0,500) = 500 \\cdot 24 = 12000 \\ EUR$  \n",
    "$V(375,250) = 375 \\cdot 30 + 250 \\cdot 24 = 17250 \\ EUR$  \n",
    "$375 + 250 = 625$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следующем юните мы научимся решать задачи линейного программирования с использованием библиотек Python. Однако для того чтобы грамотно передавать условия в функции, необходимо научиться формулировать задачи линейного программирования в матричном виде. Давайте рассмотрим несколько примеров в качестве подготовки ↓\n",
    "\n",
    "**? Пример № 1**\n",
    "\n",
    "Вы отвечаете за рекламу в компании.\n",
    "\n",
    "Затраты на рекламу в месяц не должны превышать 10 000 руб. Один показ рекламы в интернете стоит 1 рубль, а на телевидении — 90 рублей. При этом на телевидении нельзя купить больше 20 показов.\n",
    "\n",
    "Практика показывает, что 1 показ телерекламы приводит в среднем 300 клиентов, а 1 показ в интернете — 0.5 клиента.\n",
    "\n",
    "Вам необходимо привести как можно больше клиентов.\n",
    "\n",
    "Обозначим за $x_1$ количество показов в интернете, за $x_2$ — количество показов на телевидении. Будем максимизировать число приведённых клиентов $0.5 x_1 + 300 x_2$.\n",
    "\n",
    "Составим задачу минимизации с ограничением на количество показов и затраты. Наша целевая функция примет следующий вид:\n",
    "\n",
    "$\\min \\left(-0.5 x_{1}-300 x_{2}\\right)$\n",
    "\n",
    "Мы заменили максимизацию на минимизацию, так как готовим задачу для решения стандартным алгоритмом, а по умолчанию задача оптимизации сводится к минимизации. Поэтому перед применением функций Python формулируйте задачу именно в формате поиска минимума.\n",
    "\n",
    "Ограничения можно выразить так:\n",
    "\n",
    "$\\left\\{\\begin{array}{l}x_{2} \\leq 20 \\\\ x_{1}+90 x_{2} \\leq 10000\\end{array}\\right.$\n",
    "\n",
    "Разумеется, будем помнить, что количество показов рекламы всегда неотрицательное.\n",
    "\n",
    "Представим наши данные в векторном виде. Пусть $c$ — это вектор приведённых клиентов:\n",
    "\n",
    "$c=\\left(\\begin{array}{c}-0.5 \\\\ -300\\end{array}\\right)$\n",
    "\n",
    "За $A$ и $b$ мы возьмём такие матрицы, чтобы с их помощью можно было представить систему ограничений:\n",
    "\n",
    "$A=\\left(\\begin{array}{cc}0 & 1 \\\\ 1 & 90\\end{array}\\right), \\ b=\\left(\\begin{array}{c}20 \\\\ 10000\\end{array}\\right)$\n",
    "\n",
    "Тогда наша задача формулируется следующим образом:\n",
    "\n",
    "$\\operatorname{min} c^{T} x$  \n",
    "$A x \\leq b$\n",
    "\n",
    "Итак, у нас получилось перевести задачу на язык линейной алгебры. Решать такие задачи мы будем в следующем юните, а пока рассмотрим ещё один пример посложнее ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 2**\n",
    "\n",
    "Есть $n$ задач и $n$ человек, которые могут их выполнить.\n",
    "\n",
    "Каждая задача должна быть сделана одним человеком, и каждый должен сделать ровно одну задачу.\n",
    "\n",
    "Выполнение задачи $j$ человеком $i$ будет стоить $c_{ij}$.\n",
    "\n",
    "Вам необходимо сделать все задачи как можно дешевле.\n",
    "\n",
    "Так как очевидно, что люди и задачи могут быть выражены только целыми числами, будем формулировать задачу как задачу целочисленного программирования.\n",
    "\n",
    "Пусть $x_{ij} = 1$, если задачу $j$ выполнил человек $i$. Если работы выполнил кто-то другой, то $x_{ij} = 0$.\n",
    "\n",
    "Распределение работ по людям мы можем представить в виде таблицы-матрицы, например, следующим образом:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_5_7_new.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь на пересечениях столбцов-работ и строк-людей указана стоимость работы, если её выполнит выбранный человек. Например, второй человек выполнит первую работу за 1 денежную единицу, вторую — за 5 денежных единиц, а третью — за 2 денежных единицы.\n",
    "\n",
    "Если мы выберем первого человека для выполнения первой работы, второго — для третьей и третьего — для второй (как и закрашено на рисунке выше), тогда в матрице распределения работ мы обозначим соответствующие элементы за 1, а остальные — за 0.\n",
    "\n",
    "Теперь сформулируем задачу. Минимизируем суммарную стоимость $c_{ij} \\cdot x_{ij}$:\n",
    "\n",
    "$\\min \\sum_{i, j} c_{i j} x_{i j}$\n",
    "\n",
    "Теперь запишем условия:\n",
    "\n",
    "$x$ равен либо 0, либо 1: $x_{i j} \\leq 1$.  \n",
    "Каждый человек должен взять ровно одну задачу: $\\forall i: \\sum_{j} x_{i j}=1$.  \n",
    "Каждую задачу должен взять ровно один человек: $\\forall j: \\sum_{i} x_{i j}=1$.  \n",
    "Мы сформулировали задачи линейного программирования в чётком математическом виде. Самое время переходить к следующему юниту и учиться решать задачи с использованием программирования →\n",
    "\n",
    "**ДОПОЛНИТЕЛЬНО**\n",
    "\n",
    "Если вам интересно изучить более сложные, но очень известные задачи оптимизации, к которым применимо линейное программирование, рекомендуем прочитать следующие статьи:\n",
    "\n",
    "- Задача коммивояжёра https://habr.com/ru/post/560468/  \n",
    "- Транспортная задача https://habr.com/ru/post/573224/  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Практика: линейное программирование"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Мы познакомились с задачами линейного программирования, и теперь пришло время научиться решать их с использованием функций Python.\n",
    "\n",
    "В языке Python есть множество библиотек, с помощью которых можно решить задачу линейного программирования. Вот основные, которые мы рассмотрим в данном юните:\n",
    "\n",
    "- **SciPy** (scipy.optimize.linprog);  \n",
    "- **CVXPY**;   \n",
    "- **PuLP**.  \n",
    "\n",
    "У каждой библиотеки есть свои особенности использования, и большинство задач можно решить с помощью любой из них. Давайте посмотрим все варианты, чтобы у вас всегда был выбор — решим по одной задаче для каждой библиотеки."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 1. SciPy** (scipy.optimize.linprog)\n",
    "\n",
    "У нас есть 6 товаров с заданными ценами на них и заданной массой.\n",
    "\n",
    "Вместимость сумки, в которую мы можем положить товары, заранее известна и равна 15 кг.\n",
    "\n",
    "Какой товар и в каком объёме необходимо взять, чтобы сумма всех цен товаров была максимальной?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим переменные на основе предложенных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "values = [4, 2, 1, 7, 3, 6] #стоимости товаров\n",
    "weights = [5, 9, 8, 2, 6, 5] #вес товаров\n",
    "C = 15 #вместимость сумки\n",
    "n = 6 #количество товаров"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформулируем задачу линейного программирования. Максимизируем произведение стоимости на количество, учитывая, что произведение веса на искомое количество товаров должно укладываться во вместимость сумки:\n",
    "\n",
    "$\\max \\sum v_{i} x_{i}$  \n",
    "$\\sum w_{i} x_{i} \\leq C$  \n",
    "\n",
    "Из предыдущего юнита мы уже знаем, что в векторно-матричной форме наша задача должна формулироваться в следующем виде:\n",
    "\n",
    "$\\operatorname{min} c^{T} x$  \n",
    "$A x \\leq b$  \n",
    "\n",
    "Получается, что в наших обозначениях мы имеем следующее:\n",
    "\n",
    "$c=-v, \\ A=w^{T}, \\ b=(C)$\n",
    "\n",
    "Здесь нам необходимо вспомнить линейную алгебру, так как очень важно, чтобы векторы были в нужных нам размерностях, иначе мы не сможем использовать матричное умножение. Вектор $A$ размера 6 мы превращаем в матрицу размера (1, 6) с помощью функции expand_dims(). Создаём все необходимые переменные:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = - np.array(values) #изменяем знак, чтобы перейти от задачи максимизации к задаче минимизации\n",
    "A = np.array(weights)  #конвертируем список с весами в массив\n",
    "A = np.expand_dims(A, 0) #преобразуем размерность массива\n",
    "b = np.array([C]) #конвертируем вместимость в массив"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передаём подготовленные переменные в оптимизатор SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     con: array([], dtype=float64)\n",
       "     fun: -52.50000000003075\n",
       " message: 'Optimization terminated successfully.'\n",
       "     nit: 5\n",
       "   slack: array([-2.24904539e-11])\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([6.18738537e-14, 1.05853307e-12, 1.21475944e-13, 7.50000000e+00,\n",
       "       4.00246695e-13, 4.71394166e-13])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "linprog(c=c, A_ub=A, b_ub=b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем искомое значение функции — 52.5 (в выводе значение с минусом, но мы меняем знак, возвращаясь к задаче максимизации). x = (0, 0, 0, 7.5, 0, 0). Таким образом, мы взяли только самую дорогую, четвёртую вещь. Она одна весит 2 кг, а если взять её 7.5 раз, то получится как раз 15 кг. Отлично, задача решена."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 2. CVXPY**\n",
    "\n",
    "Снова решим задачу из примера № 1, но уже предположим, что товары нельзя дробить, и будем решать задачу целочисленного линейного программирования.\n",
    "\n",
    "SciPy не умеет решать такие задачи, поэтому будем использовать новую библиотеку CVXPY.\n",
    "\n",
    "Важно! С установкой этот библиотеки порой возникают проблемы. Если вы столкнулись с трудностями, посоветуйтесь с ментором или воспользуйтесь Google Colaboratory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью CVXPY создадим переменную-массив. Укажем его размерность, а также условие, что все числа в массиве должны быть целыми:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"pip\" �� ���� ����७��� ��� ���譥�\n",
      "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n",
      "\"pip\" �� ���� ����७��� ��� ���譥�\n",
      "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n",
      "\"pip\" �� ���� ����७��� ��� ���譥�\n",
      "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n",
      "\"pip\" �� ���� ����७��� ��� ���譥�\n",
      "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n"
     ]
    }
   ],
   "source": [
    "!pip install cvxpy\n",
    "!pip install pyscipopt\n",
    "!pip install cvxopt\n",
    "!pip install glpk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.solvers import solve\n",
    "import cvxpy\n",
    "#from cvxopt import glpk\n",
    "x = cvxpy.Variable(shape=n, integer = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее зададим ограничения, используя матричное умножение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A.flatten() # Преобразуем размерность массива\n",
    "constraint = cvxpy.sum(cvxpy.multiply(A, x)) <= C\n",
    "total_value = cvxpy.sum(cvxpy.multiply(x, c))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переходим непосредственно к решению задачи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = cvxpy.Problem(cvxpy.Minimize(total_value), constraints=[constraint])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызываем получившееся решение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem.solve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получаем бесконечность. Это совершенно нереалистично.\n",
    "\n",
    "В таком случае будем рассматривать только положительные значения $x$:\n",
    "\n",
    "$x \\geq 0$\n",
    "\n",
    "В переформулированном виде задача будет решаться следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 7., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = cvxpy.Variable(shape=n, integer=True)\n",
    "constraint = cvxpy.sum(cvxpy.multiply(A, x)) <= C\n",
    "x_positive = x >= 0\n",
    "total_value = cvxpy.sum(cvxpy.multiply(x, c))\n",
    "\n",
    "problem = cvxpy.Problem(\n",
    "    cvxpy.Minimize(total_value), constraints=[constraint, x_positive]\n",
    ")\n",
    "\n",
    "problem.solve()\n",
    "x.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы уже получаем 49, и берём только четвёртый товар в количестве семи штук. Можно увидеть, что результат, в целом, очень близок к первому, когда мы использовали библиотеку SciPy — различие лишь в добавлении целочисленности. Значит, у нас получилось решить задачу, когда мы добавили недостающее условие.\n",
    "\n",
    "А что если мы можем брать не любое количество товаров, а только один или не брать их вовсе? Задаём x типа boolean.\n",
    "\n",
    "$x= 0 \\ или \\ x = 1$\n",
    "\n",
    "Программное решение такой задачи имеет следующий вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = cvxpy.Variable(shape=n, boolean=True)\n",
    "constraint = cvxpy.sum(cvxpy.multiply(A, x)) <= C\n",
    "x_positive = x >= 0\n",
    "total_value = cvxpy.sum(cvxpy.multiply(x, c))\n",
    "\n",
    "problem = cvxpy.Problem(\n",
    "    cvxpy.Minimize(total_value), constraints=[constraint, x_positive]\n",
    ")\n",
    "\n",
    "problem.solve()\n",
    "x.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим стоимость, равную 17, взяв первый, четвёртый и шестой товары.\n",
    "\n",
    "Обратите внимание, что, используя SciPy, мы могли не указывать явно, что x только положительные, так как в линейном программировании считаются только неотрицательные x.\n",
    "\n",
    "А вот CVXPY универсальна. Мы просто задали функцию, не указывая, что это линейное программирование. CVXPY «поняла», что это задача оптимизации, и использовала нужные алгоритмы. Поэтому здесь ограничение на положительные x мы указывали явно."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**? Пример № 3. PuLP**\n",
    "\n",
    "В нашей каршеринговой компании две модели автомобилей: модель A и модель B. Автомобиль A даёт прибыль в размере 20 тысяч в месяц, а автомобиль B — 45 тысяч в месяц. Мы хотим заказать на заводе новые автомобили и максимизировать прибыль. Однако на производство и ввод в эксплуатацию автомобилей понадобится время:\n",
    "\n",
    "Проектировщику требуется 4 дня, чтобы подготовить документы для производства каждого автомобиля типа A, и 5 дней — для каждого автомобиля типа B.\n",
    "Заводу требуется 3 дня, чтобы изготовить модель A, и 6 дней, чтобы изготовить модель B.\n",
    "Менеджеру требуется 2 дня, чтобы ввести в эксплуатацию в компании автомобиль A, и 7 дней —  автомобиль B.\n",
    "Каждый специалист может работать суммарно 30 дней."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pulp\n",
      "  Downloading PuLP-2.7.0-py3-none-any.whl (14.3 MB)\n",
      "     ---------------------------------------- 14.3/14.3 MB 1.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pulp\n",
      "Successfully installed pulp-2.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pulptest.exe is installed in 'c:\\Users\\Work\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pulp\n",
    "from pulp import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Целевая функция будет выглядеть следующим образом:\n",
    "\n",
    "$20000 A + 45000 B$\n",
    "\n",
    "Также запишем ограничения:\n",
    "\n",
    "$A, \\ B \\geq 0$  \n",
    "$4A + 5B \\leq 30$   \n",
    "$3A +6B \\leq 30$  \n",
    "$2A + 7B \\leq 30$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметьте, что здесь мы снова пишем обычные неравенства, а не условия в матричном виде. Дело в том, что для данной библиотеки так «удобнее», так как она принимает все условия в «первичном» виде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество автомобилей модели А:  1.0\n",
      "Количество автомобилей модели В:  4.0\n",
      "Суммарный доход:  200000.0\n"
     ]
    }
   ],
   "source": [
    "problem = LpProblem('Производство машин', LpMaximize)\n",
    "A = LpVariable('Автомобиль A', lowBound=0 , cat=LpInteger)\n",
    "B = LpVariable('Автомобиль B', lowBound=0 , cat=LpInteger)\n",
    "#Целевая функция\n",
    "problem += 20000*A + 45000*B \n",
    "#Ограничения\n",
    "problem += 4*A + 5*B <= 30 \n",
    "problem += 3*A + 6*B <=30\n",
    "problem += 2*A + 7*B <=30\n",
    "problem.solve()\n",
    "print(\"Количество автомобилей модели А: \", A.varValue)\n",
    "print(\"Количество автомобилей модели В: \", B.varValue)\n",
    "print(\"Суммарный доход: \", value(problem.objective))\n",
    "#Количество автомобилей модели А:  1.0\n",
    "#Количество автомобилей модели В:  4.0\n",
    "#Суммарный доход:  200000.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выходит, что необходимо произвести 1 автомобиль типа A и 4 автомобиля типа B. Тогда суммарный чистый доход будет равен 200 тысячам."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 6.1**\n",
    "\n",
    "Составьте оптимальный план перевозок со склада № 1 и склада № 2 в три торговых центра с учётом тарифов, запасов на складах и потребностей торговых центров, которые указаны в таблице:\n",
    "\n",
    "<img src=\"data\\MATHML_md6_6_1_1.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Сформулируйте предложенную задачу как задачу линейного программирования и решите её любым способом (желательно программным).\n",
    "\n",
    "В качестве ответа введите минимальную суммарную стоимость поставки. Ответ округлите до целого числа.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "\n",
    "cost = np.array([\n",
    "    [2, 5, 3],\n",
    "    [7, 7, 6]\n",
    "])\n",
    "stock = np.array([180, 220])\n",
    "demand = np.array([110, 150, 140])\n",
    "num_warehouse = 2\n",
    "num_clients = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{ij}$ - сколько забирается со i склада клиенту j  \n",
    "$$f = \\sum_{i,j} cost_{ij} * x_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5 3 7 7 6]\n"
     ]
    }
   ],
   "source": [
    "c = cost.flatten()\n",
    "print(c) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого склада количество взятых предметов должно быть меньше, чем на складе:\n",
    "\n",
    "$$\\forall i: \\sum_j x_{ij} \\leq stock_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0 0]\n",
      " [0 0 0 1 1 1]]\n",
      "[180 220]\n"
     ]
    }
   ],
   "source": [
    "A = []\n",
    "b = []\n",
    "for i in range(0, num_warehouse):\n",
    "    A.append([0] * (num_clients * i) + [1] * num_clients + [0] * (num_clients * (num_warehouse - i - 1)))\n",
    "    b.append(stock[i])\n",
    "A = np.asarray(A)\n",
    "b = np.asarray(b)\n",
    "print(A)\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого клиента количество приобретаемых товаров должно быть больше на единицу, чем спрос:\n",
    "\n",
    "$$\\forall j: \\sum_i x_{ij} \\geq demand_j$$\n",
    "\n",
    "Который также:\n",
    "\n",
    "$$\\forall j: - \\sum_i x_{ij} \\leq -demand_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  0  0  0]\n",
      " [ 0  0  0  1  1  1]\n",
      " [-1  0  0 -1  0  0]\n",
      " [ 0 -1  0  0 -1  0]\n",
      " [ 0  0 -1  0  0 -1]]\n",
      "[ 180  220 -110 -150 -140]\n"
     ]
    }
   ],
   "source": [
    "A = A.tolist()\n",
    "b = b.tolist()\n",
    "for j in range(0, num_clients):\n",
    "    A.append(([0] * j + [-1] + [0] * (num_clients - j - 1)) * num_warehouse)\n",
    "    b.append(-demand[j])\n",
    "A = np.asarray(A)\n",
    "b = np.asarray(b)\n",
    "print(A)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     con: array([], dtype=float64)\n",
       "     fun: 1899.9999725510097\n",
       " message: 'Optimization terminated successfully.'\n",
       "     nit: 5\n",
       "   slack: array([ 2.61579072e-06,  3.20307387e-06, -1.59710822e-06, -2.18403449e-06,\n",
       "       -2.03772191e-06])\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([1.09999998e+02, 1.64001118e-08, 6.99999991e+01, 9.36756152e-08,\n",
       "       1.49999998e+02, 6.99999989e+01])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linprog(c=c, A_ub=A, b_ub=b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответ: 110 единиц со склада 1 клиенту 1, 70 единиц со склада 1 клиенту 3,\n",
    "150 наименований со склада 2 клиенту 2, 70 наименований со склада 2 клиенту 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 6.2**\n",
    "\n",
    "В прошлом юните мы обсуждали задачу о назначениях исполнителей задач - теперь пришло время решить её.\n",
    "\n",
    "Напомним суть: необходимо распределить пять задач между пятью исполнителями таким образом, чтобы суммарные затраты на работы были наименьшими.\n",
    "\n",
    "<img src=\"data\\MATHML_md6_6_2.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "В качестве ответа введите минимальную стоимость работ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "import cvxpy as cvx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Матрица стоимостей $C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([[1000, 12, 10, 19, 8],\n",
    "    [12, 1000, 3, 7, 2], \n",
    "    [10, 3, 1000, 6, 20], \n",
    "    [19, 7, 6, 1000, 4], \n",
    "    [8, 2, 20, 4, 1000]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Матрица переменных $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cvx.Variable(shape=(5,5), boolean=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ограничения (сумма $X$ по строкам и столбцам равна 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = [\n",
    "    cvx.sum(x, axis=0) == np.ones(5),\n",
    "    cvx.sum(x, axis=1) == np.ones(5)\n",
    "]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Целевая функция (сумма $CX$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = cvx.sum(cvx.multiply(x, c))\n",
    "problem = cvx.Problem(cvx.Minimize(func), constraints=constraints)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem.solve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выбранные ячейки матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(x.value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 6.3**\n",
    "\n",
    "Найдите кратчайший маршрут из точки A, который проходит через все другие точки и возвращается в A.\n",
    "\n",
    "<img src=\"data\\MATHML_md6_6_3.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "Сформулируйте эту задачу как задачу ЦЛП и решите её. В качестве ответа укажите длину кратчайшего пути."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть X будет массивом булевых значений 5x5. Если X[A, B] = True, то маршрут проходит по ребру (A, B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cvx.Variable(shape=(5, 5), boolean=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть C будет массивом целочисленных значений, в котором хранятся расстояния между вершинами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array(\n",
    "    [\n",
    "        [0, 12, 10, 19, 8],\n",
    "        [12, 0, 3, 7, 2],\n",
    "        [10, 3, 0, 6, 20],\n",
    "        [19, 7, 6, 0, 4],\n",
    "        [8, 2, 20, 4, 0],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим ограничения. Нетрудно заметить, что кратчайший маршрут будет соединять ровно 5 пар точек – то есть, сумма x по обеим осям будет равна 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = [\n",
    "    cvx.sum(x, axis=0) == np.ones(5),\n",
    "    cvx.sum(x, axis=1) == np.ones(5),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также заметим, что \"нулевое ребро\" между каждой точкой и ней самой же не может входить в наш маршрут – то есть, на главной диагонали X располагаются нули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    constraints.append(x[i, i] == 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, считающую длину маршрута."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = cvx.sum(cvx.multiply(x, c))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решим задачу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = cvx.Problem(cvx.Minimize(func), constraints=constraints)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem.solve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длина наикратчайшего пути – 32."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Найденный путь\n",
    "Посмотрим на найденный наикратчайший путь. Для этого взглянем на значения матрицы X, при которых достигается минимальная длина пути."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(x.value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это означает, что вершина A - первая в маршруте, B - третья, C - вторая, D - четвёртая. Эту же информацию можно достать из матрицы $X$.  \n",
    "Искомое решение.\n",
    "\n",
    "![alt text](https://i.ibb.co/qJzt4LW/image.png)\n",
    "\n",
    "Обратите внимание, что в заданиях 6.2 и 6.3 для записи двух разных задач используется почти одна и та же система уравнений!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Дополнительные методы оптимизации. Практика"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В этом и предыдущем модулях мы рассмотрели большой спектр методов оптимизации, но, разумеется, их существует гораздо больше — мы же разобрали только основные и самые эффективные.\n",
    "\n",
    "Давайте кратко рассмотрим ещё два метода, которые не включаются в классы с большим количеством алгоритмов (как, например, класс методов, основанных на градиентном спуске, или класс квазиньютоновских методов), но, тем не менее, иногда встречаются в практических заданиях."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### МЕТОД ИМИТАЦИИ ОТЖИГА"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первую очередь познакомимся с **методом имитации отжига** (simulated annealing).\n",
    "\n",
    "Данный метод можно использовать, когда нужно оптимизировать достаточно сложную и «неудобную» функцию, то есть, такую, для которой не получится применить градиентные или другие оптимизационные методы.\n",
    "\n",
    "Метод имитации отжига удобен для применения, так как это метод нулевого порядка, а значит у него намного меньше ограничений."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод отжига используется также в ускорении обучения нейронных сетей.\n",
    "\n",
    "Идея для алгоритма имитации отжига взята из реальной жизни. Во время отжига материал (обычно это металл) нагревают до определённой температуры, а затем медленно охлаждают. Когда материал горячий, у него снижается твёрдость и его проще обрабатывать. Когда он остывает, то становится более твёрдым и менее восприимчивым к изменениям.\n",
    "\n",
    "Наша задача — получить максимально холодный твёрдый материал. Для этого нужна крепкая кристаллическая решётка, в которой все атомы находятся на местах с минимальной энергией. Смысл в том, что, когда металл сильно нагревается, атомы его кристаллической решётки вынужденно покидают свои положения. Когда начинается охлаждение, они стремятся вернуться в состояние, где будет минимальный уровень определённой энергии. То есть нам было нужно, чтобы металл стал холодным и твёрдым, но мы сначала нагрели его (то есть «ухудшили» ситуацию относительно необходимой), чтобы в итоге (после охлаждения) получить намного более крепкую кристаллическую решётку.\n",
    "\n",
    "Метод имитации отжига относится к **эвристическим методам**. Это алгоритмы, которые основаны на некоторой математической и логической интуиции и возникают без фундаментальных теоретических предпосылок.\n",
    "\n",
    "Для таких методов обычно нет исследований и доказательств эффективности для различных случаев, однако есть практические наблюдения, демонстрирующие высокую эффективность. Случается и такое, что эвристический метод доказанно ложный с точки зрения математики, но его всё равно используют в отдельных случаях, где он внезапно показывает высокое качество.\n",
    "\n",
    "Обобщая, можно сказать, что **эвристический подход** — это подход, не имеющий под собой математического обоснования (а возможно, даже имеющий опровержение для каких-то случаев), но при этом являющийся эффективным и полезным.\n",
    "\n",
    "Метод отжига является именно таким: с одной стороны, он может быстро находить решения экспоненциально сложных задач, а с другой — не всегда сходится к решению, а также может приводить к поиску только локальных минимумов. Однако метод отжига — действенное средство для решения многих сложных оптимизационных задач. Давайте познакомимся с ним ↓\n",
    "\n",
    "**Идея метода** заключается в том, чтобы иногда позволять значению функции в точке «ухудшаться» (то есть удаляться от минимума) по аналогии с отжигом, где мы «ухудшали» состояние металла (т. е. нагревали его), чтобы в итоге достичь наилучшего результата."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Зачем это нужно?\n",
    "\n",
    "Представим, что мы ищем минимум для такой функции:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_7_1.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы будем минимизировать такую функцию с помощью градиентного спуска, то неизбежно застрянем в локальном минимуме. Метод отжига может позволить функции удалиться от минимума, и тогда мы сможем «перепрыгнуть» препятствие и достичь глобального минимума.\n",
    "\n",
    "Метод отжига реализуется по следующему алгоритму:\n",
    "\n",
    "1. Выбор изначальной точки. Обычно начальная точка выбирается случайным образом. Также нужно выбрать начальную и конечную температуру.\n",
    "2. Основной алгоритм:\n",
    "    1. Выбираем случайную точку рядом с нашей точкой.\n",
    "    2. Оцениваем, переходим ли мы в эту новую точку:\n",
    "        - Если значение функции в новой точке «лучше» значения функции в нашей точке, то переходим.\n",
    "        - Если значение функции в новой точке «хуже» значения функции в нашей точке, то всё равно с некоторой вероятностью переходим. Чем ниже температура, тем ниже эта вероятность.\n",
    "    3. Уменьшаем температуру. Если температура стала ниже конечной, алгоритм прекращает работу.\n",
    "\n",
    "**Математически** и в более строгом виде это можно **записать следующим образом**:\n",
    "\n",
    "1. Выбираем начальную точку $x_0$ и определяем счётчик итераций $k$. Также определяем начальную температуру $t_0$, конечную температуру $t_{min}$ и функцию температуры $T(k)$, по которой температура будет уменьшаться с количеством итераций.\n",
    "2. На каждой итерации $k$ выбираем случайную точку $x^*$ из окрестности точки $x_{k}$.\n",
    "3. Если мы видим, что значение функции в точке $x^*$ меньше значения функции в точке $x_{k}$, то переходим в новую точку:\n",
    "$f\\left(x^{*}\\right)<f\\left(x_{k}\\right)\\to x_{k+1}=x^{*}$  \n",
    "Если же значение функции в точке $x^*$ больше значения функции в точке $x_{k}$, то с вероятностью $e^{\\frac{f(x_{k})-f(x^*)}{t_{k}}}$ мы всё равно переходим в новую точку:\n",
    "$f(x^*)>f(x_k)\\;\\; \\& \\;\\;p<e^{\\frac{f(x_{k})-f(x^*)}{t_{k}}}\\;\\to\\; x_{k+1}=x^{*}$  \n",
    "4. Понижаем температуру:\n",
    "$t_{k+1}=T(k)$  \n",
    "Если температура $t_{k+1}$ меньше конечной температуры $t_{min}$, то алгоритм прекращает работу.\n",
    "5. Увеличиваем счётчик итераций на 1:\n",
    "$k=k+1$  \n",
    "Если мы применяем этот метод, то нам предстоит ответить на два вопроса:\n",
    "\n",
    "Как выбирать начальную точку $x_0$?\n",
    "Как выбирать функцию температуры $T(k)$?\n",
    "Первое значение, как правило, определяется случайно. Второе — в зависимости от того, насколько долго мы хотим, чтобы наш алгоритм работал. **Главное правило**: функция температуры $T(k)$ обязательно должна быть убывающей.\n",
    "\n",
    "У метода отжига есть **ряд плюсов и минусов**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\happy-icon.png\" alt=\"drawing\" width=\"50\"/>  \n",
    "\n",
    "- Не использует градиент. Это значит, что его можно использовать для функций, которые не являются непрерывно дифференцируемыми.  \n",
    "- Можно использовать даже для дискретных функций. Причём именно для дискретных функций метод подходит очень хорошо.  \n",
    "- Прост в реализации и использовании."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\sad-icon.png\" alt=\"drawing\" width=\"50\"/>  \n",
    "\n",
    "- Сложно настраивать под задачу из-за множества параметров.\n",
    "- Нет гарантий сходимости.\n",
    "- Выполнение может занимать много времени."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод отжига используется не очень часто, однако он довольно известен, и следует иметь о нём общее представление.\n",
    "\n",
    "Если вам интересно подробнее узнать про метод отжига и посмотреть на его имплементацию в Python, рекомендуем ознакомиться со следующей статьёй.\n",
    "\n",
    "https://dev.to/cesarwbr/how-to-implement-simulated-annealing-algorithm-in-python-4gid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### МЕТОД КООРДИНАТНОГО СПУСКА"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё один метод, про который необходимо узнать и с которым вы сталкивались ранее, — это **метод координатного спуска**.\n",
    "\n",
    "Это тип алгоритма оптимизации, который обычно используется для «сильно выпуклого» случая регрессии — регрессионной функции Lasso и для регрессионной модели Elastic Net.\n",
    "\n",
    "Метод координатного спуска можно считать одним из простейших методов оптимизации , который  в целом достаточно эффективно ищет локальные минимумы для гладких функций.\n",
    "\n",
    "При поиске минимума с помощью этого метода мы всегда изменяем положение точки в направлении осей координат, т. е. всегда изменяется только одна координата, и благодаря этому задача становится одномерной.\n",
    "\n",
    "Если визуализировать работу алгоритма, то мы увидим, что за счёт того, что каждый шаг происходит параллельно одной или другой координатной оси, ход алгоритма становится похож на «лесенку»:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\MATHML_md6_7_2.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути, мы просто выбираем некоторую точку и правило изменения координаты и движемся в соответствии с ним до локального минимума.\n",
    "\n",
    "Посмотрим на **основные отличия координатного и градиентного спусков**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data\\PIC-3.png\" alt=\"drawing\" width=\"1100\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПРАКТИКА**\n",
    "\n",
    "Предыдущий модуль мы завершили тем, что посмотрели на прописанный без готовых функций алгоритм классического градиентного спуска, чтобы построить прогностическую модель для продаж некоторого продукта на основе количества денег, потраченных на рекламу.\n",
    "\n",
    "Пришло время попрактиковаться и попробовать самостоятельно реализовать алгоритмы, с которыми вы познакомились для решения той же задачи.\n",
    "\n",
    "**Вам необходимо**:\n",
    "\n",
    "1. Скачать файл с данными. https://lms.skillfactory.ru/assets/courseware/v1/be9ea3bcfb9e5ebc744c1f2af98fed61/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/Advertising.zip  \n",
    "2. Открыть ноутбук-шаблон с заданием. https://lms.skillfactory.ru/assets/courseware/v1/7b2e6cbfd22f5452704aa9c75cff644b/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B0_%D0%9E%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.ipynb  \n",
    "3. Загрузить данные и определить целевую переменную и предикторы.\n",
    "4. Нормализовать признаки, с помощью которых вы будете осуществлять предсказание.\n",
    "5. Реализовать координатный спуск, следуя подсказкам в файле-шаблоне.\n",
    "6. Реализовать стохастический градиентный спуск, следуя подсказкам в файле-шаблоне.\n",
    "7. Оценить качество обеих полученных моделей с помощью MSE и MAE.\n",
    "8. Сделать содержательные выводы.\n",
    "\n",
    "**Важно**:\n",
    "\n",
    "- Вариант реализации алгоритмов, который будет предложен в подсказках, не единственно верный. Будет здорово, если вы сможете придумать полностью самостоятельную реализацию. Однако если будет сложно, можете воспользоваться нашим вариантом.\n",
    "- Пожалуйста, оформите ваше решение в соответствии со следующими требованиями:\n",
    "    - Решение оформляется только в Jupyter Notebook.\n",
    "    - Решение оформляется в соответствии с ноутбуком-шаблоном.\n",
    "    - Каждое задание выполняется в отдельной ячейке, выделенной под задание (в шаблоне они помечены как «ваш код здесь»). Не создавайте дополнительные ячейки.\n",
    "    - Решение не должно содержать функции из библиотек для машинного обучения и оптимизации, кроме тех ячеек, где это указано явным образом.\n",
    "    - Код должен быть читаемым и понятным: имена переменных и функций отражают их сущность, отсутствуют многострочные конструкции и условия.\n",
    "        - Пользуйтесь руководством PEP-8.\n",
    "\n",
    "**КРИТЕРИИ ОЦЕНИВАНИЯ**\n",
    "\n",
    "Вам предстоит выполнить **три задания**, каждое из которых состоит из более мелких подзадач.\n",
    "\n",
    "Для каждой подзадачи в ноутбуке-шаблоне указано количество баллов (если она оценивается отдельно). Один бонусный балл начисляется за аккуратное и полное выполнение всего задания.\n",
    "\n",
    "**ЗАДАНИЕ\tБАЛЛЫ**  \n",
    "**1. Загрузка и подготовка данных**  \n",
    "Аккуратное и полное выполнение задания\t1  \n",
    "**2. Координатный спуск**  \n",
    "Реализация алгоритма координатного спуска\t3  \n",
    "Аккуратное и полное выполнение задания\t1  \n",
    "**3. Стохастический градиентный спуск**  \n",
    "Масштабирование столбцов исходной матрицы\t1  \n",
    "Создание функции для вычисления среднеквадратичной ошибки\t1  \n",
    "Составление наивного прогноза (предсказание продаж средним значением)\t1  \n",
    "Создание функции для получения вектора прогнозов\t1  \n",
    "Создание функции для реализации шага стохастического градиентного спуска\t2  \n",
    "Создание функции для реализации стохастического градиентного спуска\t3  \n",
    "Аккуратное и полное выполнение задания\t1  \n",
    "Максимальное количество баллов — 15.  \n",
    "\n",
    "Отправьте ноутбук (в формате .IPYNB) с выполненными заданиями на проверку через форму ниже ↓\n",
    "\n",
    "ЗАДАНИЕ 7.4. МОДУЛЬ MATH&ML-6 (HW-02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4604fddfbe3ec3d9d385c207fa0ddb9a05dfef66380401ad05b04bbd42d45367"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
